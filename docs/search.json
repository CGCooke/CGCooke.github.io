[
  {
    "objectID": "posts/2019-03-23-Kalman-Filters.html",
    "href": "posts/2019-03-23-Kalman-Filters.html",
    "title": "Kalman filters",
    "section": "",
    "text": "While this sounds abstract, Kalman Filters provide a concrete mathematical formulation for fusing data from different sources and physical models to provide (potentially) optimum estimates of the state of a system.\nFor less philosophy and more maths, I strongly recommend stopping at this point and giving this excellent post a read. Afterwards, let’s concretely talk about Kalman filters,\n\nCreating a Kalman Filter in 7 easy steps:\nOne of the challenges with Kalman filters is that it’s easy to be initially overwhelmed by the mathematical background and lose sight of their implementation in practice. In reality, it’s possible to break the implementation down into a series of discrete steps, which come together to describe the filter fully.\nFilterPy is a great Python library for creating Kalman filters and has an accompanying book that takes a deep dive into the mathematical theory of Kalman filters. Let’s initially discuss the general process for defining a Kalman filter before applying it to practical application.\n\nx: Our filter state estimate, IE, what we want to estimate. If we’re going to track an object moving in a video, this could be its pixel coordinates and its velocity in pixels per second. [x,y,vx,vy]\nP: The covariance matrix. It encodes how sure the filter is about its estimates and evolves over time. In the object tracking example, how “confident” the filter is about the position of an object and its velocity. As the filter receives more measurements, the values in the covariance matrix are “washed out”, so the filter tends to be insensitive to the values used.\nQ: The process uncertainty. How significant is the error associated with the system doing something unexpected between measurements? This is the hardest to set, as it requires careful thought about the process. For example, if we are tracking the position and velocity of an object once a second, we would have more uncertainty if we were tracking the position of a fruit fly than an oil tanker.\nR: How uncertain each of our measurements is. This can be determined either through reading sensor datasheets or educated guesses.\nH: How each measurement is related to the internal state of our system, in addition to scaling measurements. If we have a GPS receiver, it tells us about our position, while an accelerometer tells us about our acceleration.\nF: The state transition matrix. How the system evolves over time. IE, if we know the position and velocity of an object, then in the absence of any error or external influence, we can predict its next position from its current position and velocity.\nB: The control matrix. This matrix allows us to tell the filter how we expect any inputs we provide (u) to update the system’s state. In many cases, the control matrix is not required, especially when we take measurements of a system we don’t control.\n\nAt this point, it’s worthwhile considering how these matrices are related. Tim Babb of Bzarg has a fantastic diagram that sets out how information flows through the filters mentioned above. If you haven’t already, I strongly recommend you read his post on how Kalman filters work \nLooking at the relationships between all of the matrices,\n\nx and P are the filter outputs; they tell what the filter believes the system’s state to be.\nH, F and B are matrices that control the filter’s operation.\nQand R are closely related because they denote uncertainty in the process and the measurements.\nz and u denote inputs to the filter. If we don’t control the system, then u is not required.\n\n\n\nA real-world example:\nIn computer vision, object tracking associates different detections of an object from different images/frames into a single “track”. Many algorithms have been developed for this task Simple Online and Realtime Tracking is particularly elegant. Let’s look at a real-world example. In summary, SORT creates a Kalman filter for each object it wants to track and then predicts the location and size of each object in each frame using the filter.\nAlex Bewley, one of the creators of SORT, has developed a fantastic implementation of SORT, which uses Filterpy.\nLet’s take a look at his implementation through the lens of what I’ve discussed above:\nQuickly defining some nomenclature,\n\nu and v are the x and y pixel coordinates of the centre of the bounding box around an object being tracked.\ns and r are the scale and aspect ratio of the bounding box surrounding the object.\n\\(\\dot u, \\dot v\\) are the x and y velocity of the bounding box.\n\\(\\dot s\\) is the rate at which the bounding box’s scale changes.\n\nkf = KalmanFilter(dim_x=7, dim_z=4)\nOur internal state is seven-dimensional: \\([u, v, s, r, \\dot u, \\dot v , \\dot s]\\)\nWhile our input vector is four-dimensional: \\([u, v, s, r]\\)\nkf.F = np.array([[1,0,0,0,1,0,0],\n                 [0,1,0,0,0,1,0],\n                 [0,0,1,0,0,0,1],\n                 [0,0,0,1,0,0,0], \n                 [0,0,0,0,1,0,0],\n                 [0,0,0,0,0,1,0],\n                 [0,0,0,0,0,0,1]])\nThe state transition matrix tells us that at each timestep, we update our state as follows:\n\\[u = u + \\dot u\\]\n\\[v = v + \\dot v\\]\n\\[s = s + \\dot s\\]\nkf.H = np.array([[1,0,0,0,0,0,0],\n                 [0,1,0,0,0,0,0],\n                 [0,0,1,0,0,0,0],\n                 [0,0,0,1,0,0,0]])\nThe sensor matrix tells us that we directly measure \\([u, v, s, r]\\).\nkf.R = np.array([[ 1,  0,  0,  0],\n                 [ 0,  1,  0,  0,],\n                 [ 0,  0, 10,  0,],\n                 [ 0,  0,  0, 10,]])\nThe sensor noise matrix tells us that we can measure \\(u\\) and \\(v\\) with a much higher certainty than \\(s\\) and \\(r\\).\nkf.P = np.array([[   10,    0,     0,     0,     0,     0,     0],\n                 [    0,   10,     0,     0,     0,     0,     0],\n                 [    0,    0,    10,     0,     0,     0,     0],\n                 [    0,    0,     0,    10,     0,     0,     0],\n                 [    0,    0,     0,     0, 10000,     0,     0],\n                 [    0,    0,     0,     0,     0, 10000,     0],\n                 [    0,    0,     0,     0,     0,     0, 10000]])\nThe covariance matrix tells us that the filter should have a very high initial uncertainty for each velocity component.\nkf.Q = np.array([[1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00]\n                 [0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00]\n                 [0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00]\n                 [0.e+00, 0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00]\n                 [0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00, 0.e+00]\n                 [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00]\n                 [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-04]])\nThe Process uncertainty matrix tells us how much “uncertainty” is in each system’s behaviour component.\nFilterpy has a function that can be very useful for generating Q.\nfilterpy.common.Q_discrete_white_noise\n\n\nFurther reading\nControl theory is a broad and intellectually stimulating area with wide applications. Brian Douglas has an incredible YouTube channel which I strongly recommend."
  },
  {
    "objectID": "posts/2020-04-29-how-long-is-the-blacklist.html",
    "href": "posts/2020-04-29-how-long-is-the-blacklist.html",
    "title": "How long is the Blacklist?",
    "section": "",
    "text": "I’m learning French, and as part of my language-learning journey, I’m slowly working through The Blacklist on Netflix.\nThe show’s premise involves an antihero, played by James Spader, working through a list of nefarious characters, taking them down one at a time.\nOut of idle intellectual curiosity, I was curious about how many names are on the list based on the sample of names given. It’s an interesting problem we can solve using Bayesian analysis, more commonly known as the “German Tank Problem” or the “Locomotive Problem”.\nIt’s also an exciting intro to Bayesian analysis. We are trying to infer an unknown quantity (The number of people on the list) while only being able to observe samples from this distribution."
  },
  {
    "objectID": "posts/2020-04-29-how-long-is-the-blacklist.html#bayes-in-brief",
    "href": "posts/2020-04-29-how-long-is-the-blacklist.html#bayes-in-brief",
    "title": "How long is the Blacklist?",
    "section": "Bayes in Brief",
    "text": "Bayes in Brief\nMany other, far better blog posts cover this topic, particularly this one.\nWith that out of the way, let’s talk about the problem at hand and how we can solve it with Bayesian analysis.\nWe have some evidence, \\(E\\), which is what we observe. In this case, it’s the index in the blacklist that we find out about each episode. IE, in the first episode, it is entry number \\(52\\).\nThen we have what we want to know, an estimate of the length of the blacklist, given the evidence we have observed. Given the evidence \\(P(H|E)\\), this is the probability of a hypothesis.\nIt’s a probability because we don’t know how long the blacklist is, given what we have observed, but we can work out how confident we can be about each potential option.\nGiven that \\(P(H|E)\\) is what we want, we can use Baye’s Formula to find it.\n\\(P(H|E) = \\frac{P(E|H) P(H)}{P(E)}\\)\nWe need three more pieces of the puzzle: \\(P(E|H)\\), \\(P(H)\\) and \\(P(E)\\).\n\\(P(E|H)\\) is the probability of the evidence, given the hypothesis. This term is also known as the Likelihood and is the one I found most confusing while initially learning about Bayesian analysis. In this case, it could be more complex.\n\\(P(H)\\) is our prior belief about the possible hypothesis. In this case, it’s long we plausibly think the blacklist could be. In practice, we know it has to be as long as any of the entries we have observed.\nMuch of the criticism of Baysian analysis comes from the fact we can inject our own beliefs into the process. In this case, I don’t have a strong view about the length of the blacklist, but it’s reasonable to assume that it’s probably less than 200 entries long.\nLet’s assume that the blacklist is of length \\(N\\). Let’s also believe any given entry is as likely to be the subject of each episode as any other. Then the probability of drawing any given number is \\(\\frac{1}{N}\\). This is the likelihood. Also, note the subtle point that N has to be at least as large as the number we have drawn.\nFinally, we have \\(P(E)\\), which is the probability of the evidence. This is just a normalisation factor, which is often ignored.\nLet’s get down to business. Fortunately, Wikipedia has all the data we need.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo. overall\nNo. in season\nTitle\nBlacklist guide\nDirected by\nWritten by\nOriginal air date\nUS viewers(millions)\n\n\n\n\n1\n1\n“Pilot”\nNo. 52\nJoe Carnahan\nJon Bokenkamp\nSeptember 23, 2013\n12.58[10]\n\n\n2\n2\n“The Freelancer”\nNo. 145\nJace Alexander\nJon Bokenkamp\nSeptember 30, 2013\n11.35[11]\n\n\n3\n3\n“Wujing”\nNo. 84\nMichael Watkins\nLukas Reiter\nOctober 7, 2013\n11.18[12]\n\n\n4\n4\n“The Stewmaker”\nNo. 161\nVince Misiano\nPatrick Massett & John Zinman\nOctober 14, 2013\n10.93[13]\n\n\n5\n5\n“The Courier”\nNo. 85\nNick Gomez\nJohn C. Kelley\nOctober 21, 2013\n10.44[14]\n\n\n6\n6\n“Gina Zanetakos”\nNo. 152\nAdam Arkin\nWendy West\nOctober 28, 2013\n10.51[15]\n\n\n7\n7\n“Frederick Barnes”\nNo. 47\nMichael Watkins\nJ. R. Orci\nNovember 4, 2013\n10.34[16]\n\n\n8\n8\n“General Ludd”\nNo. 109\nStephen Surjik\nAmanda Kate Shuman\nNovember 11, 2013\n10.69[17]\n\n\n9\n9\n“Anslo Garrick”\nNo. 16\nJoe Carnahan\nStory by : Joe Carnahan & Jason George Teleplay by : Joe Carnahan\nNovember 25, 2013\n10.96[18]\n\n\n10\n10\n“Anslo Garrick Conclusion”\nNo. 16\nMichael Watkins\nLukas Reiter & J. R. Orci\nDecember 2, 2013\n11.67[19]\n\n\n11\n11\n“The Good Samaritan”\nNo. 106\nDan Lerner\nBrandon Margolis & Brandon Sonnier\nJanuary 13, 2014\n9.35[20]\n\n\n12\n12\n“The Alchemist”\nNo. 101\nVince Misiano\nAnthony Sparks\nJanuary 20, 2014\n8.83[21]\n\n\n13\n13\n“The Cyprus Agency”\nNo. 64\nMichael Watkins\nLukas Reiter\nJanuary 27, 2014\n10.17[22]\n\n\n14\n14\n“Madeline Pratt”\nNo. 73\nMichael Zinberg\nJim Campolongo\nFebruary 24, 2014\n11.18[23]\n\n\n15\n15\n“The Judge”\nNo. 57\nPeter Werner\nJonathan Shapiro & Lukas Reiter\nMarch 3, 2014\n11.01[24]\n\n\n16\n16\n“Mako Tanida”\nNo. 83\nMichael Watkins\nStory by : Joe Carnahan Teleplay by : John Eisendrath & Jon Bokenkamp & Patrick Massett & John Zinman\nMarch 17, 2014\n10.97[25]\n\n\n17\n17\n“Ivan”\nNo. 88\nRandy Zisk\nJ.R. Orci & Amanda Kate Shuman\nMarch 24, 2014\n10.80[26]\n\n\n18\n18\n“Milton Bobbit”\nNo. 135\nSteven A. Adelson\nDaniel Voll\nMarch 31, 2014\n11.39[27]\n\n\n19\n19\n“The Pavlovich Brothers”\nNos. 119-122\nPaul Edwards\nElizabeth Benjamin\nApril 21, 2014\n11.24[28]\n\n\n20\n20\n“The Kingmaker”\nNo. 42\nKaren Gaviola\nJ. R. Orci & Lukas Reiter\nApril 28, 2014\n10.85[29]\n\n\n21\n21\n“Berlin”\nNo. 8\nMichael Zinberg\nJohn Eisendrath & Jon Bokenkamp\nMay 5, 2014\n10.47[30]\n\n\n22\n22\n“Berlin Conclusion”\nNo. 8\nMichael Watkins\nStory by : Richard D’Ovidio Teleplay by : John Eisendrath & Jon Bokenkamp & Lukas Reiter & J. R. Orci\nMay 12, 2014\n10.44[31]"
  },
  {
    "objectID": "posts/2020-04-29-how-long-is-the-blacklist.html#putting-it-into-practice",
    "href": "posts/2020-04-29-how-long-is-the-blacklist.html#putting-it-into-practice",
    "title": "How long is the Blacklist?",
    "section": "Putting it into practice",
    "text": "Putting it into practice\nNow, let’s look at all the episodes, from the seasons that have been aired:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 15]\n\n# E: the data\ny = np.array([52, 145, 84, 161, 85, 152,\n       47, 109, 16, 16, 106, 101,\n       64, 73, 57, 83, 88, 135,\n       119,120,121, 122, 42, 8, 8, 104, 112,\n       89, 82, 74, 114, 22, 12,\n       21, 21, 67, 71, 93, 94,\n       75, 7, 97, 117, 62, 87,\n       55, 11, 38, 80, 72, 43,\n       50, 86, 31, 108, 24, 24,\n       95, 132, 103, 77, 113, 78,\n       32, 32, 41, 18, 14, 14,\n       79, 66, 65, 81, 105, 53,\n       98, 98, 111, 163, 102,\n       34, 107, 59, 10, 61, 29,\n       46, 4, 4, 30, 37, 76, 44,\n       54, 90, 48, 13, 118, 100,\n       56, 63, 51, 68, 19, 25,\n       23, 13, 110, 26, 17, 33,\n       20, 124, 146, 147, 131, 91, 116,\n       58, 99, 160, 20, 20, 9,\n        6, 115, 69, 136, 92, 128,\n       60, 15, 27, 27, 151, 138,\n       130, 125, 162, 159, 3, 137,\n       155, 144, 126, 158, 149,\n       150])\n\nWe are now plotting the episodes in order. If we had to guess now, we would likely guess that the length of the Blacklist is 163 episodes or a few more. This is because we have seen entries on the list up to 163, and It looks like nearly every entry on the list has been crossed off.\n\nplt.stem(np.sort(y), use_line_collection=True)\nplt.show()\n\n\n\n\nLet’s say we have a sample of only the first ten episodes; what can we infer from this sample?\n\ny_sample = y[0:10]\nplt.stem(np.sort(y_sample), use_line_collection=True)\nplt.show()\n\n\n\n\nLet’s define our likelihood function, \\(P(E|H)\\).\n\ndef compute_likelyhood(i,observed):\n    if i < observed:\n        likelyhood = 0\n    else:\n        likelyhood = 1.0/i\n    return(likelyhood)\n\nNow let’s define the prior belief \\(P(H)\\) of what we think the length of the Blacklist could be.\n\nprior = np.ones(200)\nprior /= prior.sum()\n\nThis prior is quite broad, and it’s also uniform. We aren’t imputing our human judgement about how long the list is.\nNow, let’s start by computing the likelihood for each possible list length after watching the first episode and seeing person #52 crossed off the list.\n\nposterior = np.zeros(200)\nobserved = 52\nfor i in range(0,200):\n    posterior[i] = prior[i] * compute_likelyhood(i,observed)\n\nposterior /= posterior.sum()\nplt.stem(posterior)\nplt.xlabel('List length')\nplt.ylabel('Confidence')\n#plt.xlim(150,175)\nplt.show()\n\n/Users/cooke_c/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n  import sys\n\n\n\n\n\nNow we have this belief state, having seen the first episode, we can recursively use it as a prior and update it each time we see an episode:\n\nimport matplotlib.pylab as pl\n\n    \nposterior = np.zeros(200)\n\nepisode_number = 1\nfor observed in y_sample:\n    for i in range(1,200):\n        posterior[i] = prior[i] * compute_likelyhood(i,observed)\n\n    posterior /= posterior.sum()\n    \n    plt.plot(posterior,alpha=0.35,label='Episode: '+str(episode_number ), color=pl.cm.jet(episode_number/10.0))\n    episode_number+=1\n    prior=posterior\n    \nplt.legend()\nplt.xlabel('List length')\nplt.ylabel('Confidence')\nplt.xlim(155,175)\nplt.show()\n\n\n\n\nFrom the above, as we see more data, our confidence in the length of the list increases. Let’s look at the cumulative probability for different list lengths:\n\nplt.plot(np.cumsum(posterior))\nplt.grid()\nplt.xlim(155,185)\nplt.xlabel('List length')\nplt.ylabel('Cumulative Confidence')\nplt.show()\n\n\n\n\nFrom this, we can see that we can be approximately 50% confident that the length of Blacklist is between 160 and 170 entries long after we have seen the first ten episodes."
  },
  {
    "objectID": "posts/2020-04-29-how-long-is-the-blacklist.html#conclusion",
    "href": "posts/2020-04-29-how-long-is-the-blacklist.html#conclusion",
    "title": "How long is the Blacklist?",
    "section": "Conclusion",
    "text": "Conclusion\nWe have taken a lightning tour of how Bayesian analysis can allow us to infer from what we can observe a quantity we are interested in when a random process is involved.\nIn practice, Bayesian analysis isn’t conducted from first principles but instead uses a purpose-built library. Next time, I want to demonstrate how to do this analysis using PyMC3."
  },
  {
    "objectID": "posts/2021-02-15-the-gold-standard.html",
    "href": "posts/2021-02-15-the-gold-standard.html",
    "title": "The Gold Standard",
    "section": "",
    "text": "Australia has adopted an isolationist policy in response to the global coronavirus pandemic. Upon arriving in Australia, passengers are isolated in designated hotels for at least 14 days. While this process is highly effective, leaks can and do occur. Fortunately, these are regularly brought under control with effective contact tracing.\nThere is much discussion about the effectiveness of different state systems for hotel quarantine, and I thought it would be an interesting problem to model using Bayesian Analysis. Fortunately, there have been only a handful of leaks during the processing of more than 100,000 people. As part of this modelling, I want to find the underlying probability of a leak for each processed person. We can estimate this probability independently for each state and use it to understand if there are any fundamental differences."
  },
  {
    "objectID": "posts/2021-02-15-the-gold-standard.html#the-data",
    "href": "posts/2021-02-15-the-gold-standard.html#the-data",
    "title": "The Gold Standard",
    "section": "The Data",
    "text": "The Data\nTo keep things simple, I’m defining “leak” as COVID spreading unintendedly. IE, to a staff member in the system, or someone being separately isolated.\nBased on some quick perusing of ABC news, I’ve formed a rough list of 13 separate leaks from hotel quarantine.\nNSW: * Marriott Hotel at Circular Quay - August 2020 * Northern Beaches - December 2020 * Cleaner - December 2020 * Berala BWS - December 2020 * Bus Driver - December 2020 * Sofitel - February 2021 (Not yet confirmed to be a leak, but included for conservatism).\nVictoria: * Rydges on Swanston - May 2020 * Stamford Plaza - June 2020 * Park Royal hotel - January 2021 * Grand Hyatt - February 2021 * Holiday Inn - February 2021\nAnd one each in: * SA * QLD * WA\nI’ve formed an estimate for the number of people who passed through the hotel quarantine system from the ABS.\nI’m glad to re-run the numbers if you have more accurate figures for either leaks or arrivals.\n\n\n\nState\nArrivals\n\n\n\n\nNSW  \n91,400  \n\n\nVIC  \n26,710  \n\n\nQLD  \n23,830  \n\n\nSA    \n5,510    \n\n\nWA    \n16,740  \n\n\n\nOne argument is that there isn’t enough data to compare the states or that certain states have been “lucky” or “unlucky”. Using Bayesian Analysis, we can understand how confident we can be in our estimates of our underlying parameters (probability of a leak per person processed)."
  },
  {
    "objectID": "posts/2021-02-15-the-gold-standard.html#the-model",
    "href": "posts/2021-02-15-the-gold-standard.html#the-model",
    "title": "The Gold Standard",
    "section": "The Model",
    "text": "The Model\nLet’s get started modelling by first importing what we need:\n\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nWe can build a simple but effective model by treating the number of leaks as a binomial variable.\nWe can then infer the probability each processed person results in a leak based on the number of people processed and the number of leaks observed.\nWe need to provide a prior estimate for the probability of processing any one person resulting in a leak. We could guess that 1% of people returning have covid, and 1% of them will result in a leak, for a probability of leak of 0.0001. However, I will use a very broad, non-informative prior estimate that is uniform between 0 and 1.\n\nwith pm.Model() as model: \n    #Set up our model\n    states = ['NSW', 'VIC', 'QLD', 'SA', 'WA']\n    number_of_people_processed = [91400, 26710, 23830, 5510, 16740]\n    number_of_leaks_observed = [6, 5, 1, 1, 1]\n    prob_of_leak = pm.Uniform('prob_of_leak',lower=0, upper=1, shape=5)  \n    number_of_leaks = pm.Binomial('number_of_leaks', n = number_of_people_processed, p = prob_of_leak, observed = number_of_leaks_observed)\n\nNow we can use Markov Chain Monte Carlo (MCMC) to find the distribution of the underlying parameter (p). We are trying to find the distribution of values for p, where it “makes sense”, given the data we have observed and our prior estimate for it.\n\nnum_chains = 2\nnum_samples = 1_000\nwith model: \n    trace = pm.sample(draws=num_samples,chains = num_chains, tune=5_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [prob_of_leak]\n\n\n\n    \n        \n      \n      100.00% [12000/12000 00:05<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 5_000 tune and 1_000 draw iterations (10_000 + 2_000 draws total) took 11 seconds.\n\n\nThe beauty of MCMC is that instead of giving us a point estimate for each parameter, we get a probability distribution, representing how confident we can be in each value. This means we can be more confident in our comparisons between different parameters.\nLet’s visualise these distributions as boxplots using Seaborn.\n\n#Create a pandas dataframe.\nstate_col = []\nprob_of_leak_col = []\nfor i in range(0,5):\n    state = states[i]\n    prob_of_leak = trace.prob_of_leak[:,i]\n    state_col += num_chains*num_samples*[state]\n    prob_of_leak_col += list(prob_of_leak)\n    \ndata = {'State':state_col,'prob_of_leak':prob_of_leak_col}\ndf =  pd.DataFrame.from_dict(data)\n\n\n#Visualise the data.\nsns.set_theme(style=\"ticks\")\nf, ax = plt.subplots(figsize=(15, 10))\n\nax.set_xscale(\"log\")\n\nsns.boxplot(x=\"prob_of_leak\", y=\"State\", data=df, whis=[5, 95], width=0.7,\n            palette=\"vlag\", flierprops = dict(markerfacecolor = '0.50', markersize = 1))\n\n\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nax.set(xlabel=\"Probability of leak per person processed\")\nax.set_xlim(10**-5,10**-3)\nax.set(title=\"Comparative probablity of leak by state\")\n\nsns.despine(trim=True, left=True)"
  },
  {
    "objectID": "posts/2021-02-15-the-gold-standard.html#conclusion",
    "href": "posts/2021-02-15-the-gold-standard.html#conclusion",
    "title": "The Gold Standard",
    "section": "Conclusion",
    "text": "Conclusion\nWe can clearly see that there are two clusters of states.\n\nNSW, QLD and WA all have rates of leakage below 1 in 10,000\nVIC and SA, where the rate is 2-3 times higher.\n\nNSW, in particular, has a relative risk approximately 2.9 times lower than VIC.\nUsing Bayesian Analysis, we can be confident that there might be underlying differences in the effectiveness of different hotel quarantine programmes, and we could use this to learn from those representing the Gold Standard."
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "",
    "text": "While browsing Kaggle, I came across this interesting dataset, and I thought it would form the basis for some exciting blog posts.\nThe dataset contains 65M player deaths, from 720,000 different matches, from PlayerUnknown’s Battlegrounds (PUBG), a wildly popular online game."
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#an-introduction-to-pubg",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#an-introduction-to-pubg",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "An Introduction to PUBG",
    "text": "An Introduction to PUBG\nWikipedia sums up the aim of the game pretty well: > “In the game, up to one hundred players parachute onto an island and scavenge for weapons and equipment to kill others while avoiding getting killed themselves. The available safe area of the game’s map decreases in size over time, directing surviving players into tighter areas to force encounters. The last player or team standing wins the round.”\nBut for something a bit less dry but just as accurate, there is this video on Youtube."
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#data-preprocessing",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#data-preprocessing",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nFirst, let’s load some of the libraries we will need later.\n\nimport glob\nimport pandas as pd\nimport datashader as ds\nimport datashader.transfer_functions as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 15]\n\n\nBad key \"text.kerning_factor\" on line 4 in\n/opt/anaconda3/envs/PyMC3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\nYou probably need to get an updated matplotlibrc file from\nhttps://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\nor from the matplotlib source distribution\n\n\nThe dataset comes in several different .csv files, which we will load and concatenate.\n\ndef load_deaths():\n    li = []\n    for filename in glob.glob(\"/Users/cooke_c/Documents/Blog_Staging/PUBG/9372_13466_bundle_archive/deaths/*.csv\"):\n        df = pd.read_csv(filename)\n        df = df.drop(['match_id','victim_placement','killed_by','killer_name','killer_placement','killer_position_x','killer_position_y','victim_name'],axis='columns')\n        li.append(df)\n    df = pd.concat(li, axis=0, ignore_index=True)\n    return(df)\n\n\ndeaths_df = load_deaths()\n\nMatches in PUBG are limited in time to approximately 32.5 minutes. Let’s create a new categorical variable called “phase”. It will represent which of the following match phases a player died in:\n\nEarly Phase (0-10m) (Lime Green points)\nMid Phase (10-25m) (Cyan points)\nLate Phase (>25m) (Purple points)\n\n\ndef create_phase_category(deaths_df):\n    conditions = [\n        (1*60<deaths_df.time) & (deaths_df.time<10*60),\n        (10*60<deaths_df.time) & (deaths_df.time<25*60),\n        (25*60<deaths_df.time)]\n\n    choices = ['early', 'mid', 'late']\n    deaths_df['phase'] = np.select(conditions, choices, default='very_early')\n    deaths_df['phase'] = deaths_df['phase'].astype('category')\n    \n    return(deaths_df)\n\n\ndeaths_df = create_phase_category(deaths_df)"
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#datashader",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#datashader",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "Datashader",
    "text": "Datashader\nNow, this is where the fun begins.\nDatashader is a highly efficient Python library for visualising massive data.\nTaking Pandas data frames as inputs, Datashader aggregates the data to form visualisations.\nThere are three key components that we use to generate our visualisation:\n\nDefining a canvas. It’s going to be 4,000 by 4,000 pixels. The data range we want to visualise is 800,000 by 800,000.\n\ncvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000])\n\nWe want to aggregate data from deaths_df, using the ‘victim_position_x’ variable as the x coordinate and ‘victim_position_y’ as the y coordinate. Effectively, we are computing a separate 2D histogram for each category (game phase).\n\nagg = cvs.points(deaths_df, 'victim_position_x', 'victim_position_y',ds.count_cat('phase'))\n\nWe visualise our 2D histogram, colouring each bin/pixel according to our colour map. We also use histogram equalisation (how=‘eq_hist’).\n\nimg = tf.shade(agg, color_key=color_key, how='eq_hist')\nThis post is heavily inspired by this example, which is more detailed about the pipeline involved.\n\ndef visualise_with_datashader(deaths_df):\n    color_key = {'very_early':'black', 'early':'lime',  'mid':'aqua', 'late':'fuchsia'}\n    \n    cvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000])\n    \n    agg = cvs.points(deaths_df,'victim_position_x','victim_position_y',ds.count_cat('phase'))\n    \n    img = tf.shade(agg, color_key=color_key, how='eq_hist')\n    img = tf.set_background(img,\"black\", name=\"Black bg\")\n    return(img)\n\nOne minor detail is that we need to invert the y coordinates we want to render to match the coordinate system used for the game maps.\n\ndeaths_df.victim_position_y = 800_000 - deaths_df.victim_position_y"
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#erangel",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#erangel",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "Erangel",
    "text": "Erangel\n\nEarly Phase (0-10m) (Lime Green points)\nMid Phase (10-25m) (Cyan points)\nLate Phase (>25m) (Purple points)\n\n\nerangel_df = deaths_df[deaths_df.map=='ERANGEL']\n\nnum_points = erangel_df.shape[0]\nprint(f'Total points : {num_points}')\n\nimg = visualise_with_datashader(erangel_df)\n\nds.utils.export_image(img=img,filename='Erangel', fmt=\".png\");\n\nTotal points : 52964245"
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#miramar",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#miramar",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "Miramar",
    "text": "Miramar\n\nEarly Phase (0-10m) (Lime Green points)\nMid Phase (10-25m) (Cyan points)\nLate Phase (>25m) (Purple points)\n\n\nmiramar_df = deaths_df[deaths_df.map=='MIRAMAR']\n\nnum_points = miramar_df.shape[0]\nprint(f'Total points : {num_points}')\n\nimg = visualise_with_datashader(miramar_df)\nds.utils.export_image(img=img,filename='Miramar', fmt=\".png\");\n\nTotal points : 11622838"
  },
  {
    "objectID": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#analysis",
    "href": "posts/2020-05-31-visualising-pubg-deaths-with-datashader.html#analysis",
    "title": "Visualising PUBG Deaths with Datashader",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a closer look at the lower part of the Erangel map.\nWe can see three different phases of the game, the early phase in green, the mid-phase in cyan, and the later phase in purple.\nI will confess to having played a total of 2 games of PUBG before deciding that playing virtual hide and seek wasn’t that fun. Hence, we can see some clear patterns.\nIn the early phases of the game, deaths are in and around buildings as players search for supplies and weapons.\nIn the middle phase, the deaths appear to be more spread over the map, with concentrations on roads and natural chokepoints like bridges.\nIn the last phase of the game, the decreasing size of the “safe zone” forces the players into a concentrated area for a final stand. This results in the constellation of purple dots spread across the map.\n\nErangel subsection 1\n\n\n\nSubsection of Erangel 1\n\n\n\n\nErangel subsection 2\n\n\n\nSubsection of Erangel 2\n\n\n\n\nMiramar subsection\n\n\n\nSubsection of Miramar"
  },
  {
    "objectID": "posts/2020-07-06-vanishing-points-in-practice.html",
    "href": "posts/2020-07-06-vanishing-points-in-practice.html",
    "title": "Vanishing points in practice",
    "section": "",
    "text": "In a previous post, I used optimisation to try to calibrate a camera. Ultimately, while we arrived at a plausible solution, I was left with some questions.\nIn particular, I wasn’t sure how much I could trust this solution. The image’s principal point is usually located near the centre of the image. In this case, it was located far from it.\nThis is of itself isn’t fatal. The image could have been a crop of a photo; however, it’s extremely unusual.\nIf we want a more accurate answer, we can break this down into a two-step process, firstly, computing estimates of the attitude and the camera calibration matrix.\nWe can then combine these estimates with the points we have measured previously to produce a new estimate of the position and attitude of the camera. I will cover this in the next post.\nWe can compute the attitude and camera matrix using the method shown in a couple of previous blog posts.\n\n\nFirstly, let’s start by finding the vanishing points in the image. I’ve described how this happens in more detail in this [post] (https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html).\nLet’s start by loading in the libraries we will need.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\nimport scipy.linalg\nimport seaborn as sns\nimport scipy.stats\nfrom scipy.spatial.transform import Rotation as Rot\nplt.rcParams['figure.figsize'] = [15, 15]\nnp.random.seed(1)\n\n\nimg = Image.open('data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg')\nplt.imshow(img)\nplt.show()\n\n\n\n\n\n\nLet’s now load in the annotations I have made to this image.\n\nJSON = json.loads(open('data/2020-07-06-Vanishing-Points-In-Practice/A300.json','r').read())\n\n\ndef intersect_multiple_lines(P0,P1):\n    \"\"\"P0 and P1 are NxD arrays defining N lines.\n    D is the dimension of the space. This function \n    returns the least squares intersection of the N\n    lines from the system given by eq. 13 in \n    http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf.\n    \"\"\"\n    \n    # generate all line direction vectors \n    n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized\n\n    # generate the array of all projectors \n    projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis]  # I - n*n.T\n    \n    # generate R matrix and q vector\n    R = projs.sum(axis=0)\n    q = (projs @ P0[:,:,np.newaxis]).sum(axis=0)\n\n    # solve the least squares problem for the \n    # intersection point p: Rp = q\n    p = np.linalg.lstsq(R,q,rcond=None)[0]\n\n    return(p)\n\n\ndef load_line_data(point_name):\n    P0 = []\n    P1 = []\n    for shape in JSON['shapes']:\n        points = shape['points']\n        if shape['label'] == point_name:\n            P0.append(points[0])\n            P1.append(points[1])\n            \n    P0 = np.array(P0,dtype=np.float64)\n    P1 = np.array(P1,dtype=np.float64)\n    return(P0,P1)\n\ndef find_vanishing_point(point_name):\n    P0,P1 = load_line_data(point_name)\n    p = intersect_multiple_lines(P0,P1).ravel()\n    return(p)\n\nThere are an infinite set of parallel lines in any image, thus vanishing points; however, we typically live in a manhattan world of orthogonal 90-degree angles. Hence in most scenes, there are typically three different vanishing points. Using the annotations I manually created of the parallel lines in the images, Let’s compute three different vanishing points. \n\nvanishing_points = {}\nfor point_name in ['VP1','VP2','VP3']:\n    vanishing_points[point_name] = find_vanishing_point(point_name)\n    \n    \nplt.imshow(img)\nfor point_name,color in [('VP1','g'),('VP2','r'),('VP3','b')]:\n    vp =  vanishing_points[point_name]\n    p0,p1 = load_line_data(point_name)\n    \n    print(point_name,vp)\n    plt.scatter(vp[0],vp[1],color=color,label=point_name)\n    \n    for i in range(0,p0.shape[0]):\n        plt.plot([p0[i,0],p1[i,0]],[p0[i,1],p1[i,1]], color=color,alpha=0.5)\n        plt.plot([vp[0],p1[i,0]],[vp[1],p1[i,1]], color=color,alpha=0.5)\n        \nplt.legend()\nplt.ylim(1500,0)\nplt.show()\n\nVP1 [2908.86609693  665.60570984]\nVP2 [-1634.06911245   479.56926814]\nVP3 [2.45111941e+01 2.60455689e+04]\n\n\n\n\n\n\n\n\nOk, so this is a good start; we can find the MLE estimate for the location of the vanishing points. However, I’m also interested in the distribution of possible locations, given that I most likely made mistakes when annotating the points.\nI’m not sure exactly how significant the mistakes could be, but let’s start by assuming that the standard deviation of the error was 1 pixel and then perform a Monte Carlo simulation.\n\ndef monte_carlo_simulation(point_name,num_samples=1_000):\n    P0,P1 = load_line_data(point_name)\n    point_error_magnitude = 1 #px\n    vanishing_points = []\n    for i in range(0,num_samples):\n        P0_stochastic = P0 + point_error_magnitude * np.random.randn(P0.shape[0],P0.shape[1])\n        P1_stochastic = P1 + point_error_magnitude * np.random.randn(P1.shape[0],P1.shape[1])\n        p = intersect_multiple_lines(P0_stochastic,P1_stochastic)\n        vanishing_points.append(p)\n    vanishing_points = np.asarray(vanishing_points)\n    return(vanishing_points)\n\n\nplt.imshow(img)\nfor point_name,color in [('VP1','g'),('VP2','r'),('VP3','b')]:\n    vanishing_points = monte_carlo_simulation(point_name)\n    for p in vanishing_points:\n        plt.scatter(p[0],p[1],color=color,alpha=0.1)\nplt.ylim(1500,0)\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-06-vanishing-points-in-practice.html#finding-the-intrinsic-matrix",
    "href": "posts/2020-07-06-vanishing-points-in-practice.html#finding-the-intrinsic-matrix",
    "title": "Vanishing points in practice",
    "section": "Finding the Intrinsic Matrix",
    "text": "Finding the Intrinsic Matrix\n\ndef generate_A(vanishing_points):\n    A = []\n    for (point_name_1, point_name_2) in [('VP1','VP2'),('VP2','VP3'),('VP3','VP1')]:    \n        vp1 = vanishing_points[point_name_1]\n        vp2 = vanishing_points[point_name_2]\n\n        x1,y1 = vp1\n        x2,y2 = vp2\n\n        w1 = x2*x1 + y2*y1\n        w2 = x2 + x1 \n        w3 = y2 + y1\n        w4 = 1  \n        A.append([w1,w2,w3,w4])\n        \n    A = np.array(A)\n    return(A)\n\ndef compute_K(A):\n    w = scipy.linalg.null_space(A).ravel()\n    \n    w1 = w[0]\n    w2 = w[1]\n    w3 = w[2]\n    w4 = w[3]\n\n    omega = np.array([[w1,0,w2],\n                      [0,w1,w3],\n                      [w2,w3,w4]])\n    K = np.linalg.inv(np.linalg.cholesky(omega)).T\n    K/=K[2,2]\n    return(K)\n\n\nFinding the Rotation Matrix\n\ndef make_homogeneous(x):\n    x_homogeneous = np.array([x[0],x[1],1])\n    return(x_homogeneous)\n\ndef compute_R(K,vanishing_points):\n    v_x_h = make_homogeneous(vanishing_points['VP1'])\n    v_y_h = make_homogeneous(vanishing_points['VP2'])\n    v_z_h = make_homogeneous(vanishing_points['VP3'])\n    \n    K_inv = np.linalg.inv(K)\n    \n    R_1 = np.dot(K_inv,v_x_h)/np.linalg.norm(np.dot(K_inv,v_x_h)).T\n    R_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T\n    R_3 = np.cross(R_1,R_2)\n    \n    R = np.vstack([R_1,R_2,R_3])\n    return(R)\n\nOk, now let’s visualize the results from 1,000 iterations of the Monte Carlo simulation.\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nPTS1  = monte_carlo_simulation(point_name='VP1')\nPTS2  = monte_carlo_simulation(point_name='VP2')\nPTS3  = monte_carlo_simulation(point_name='VP3')\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n        \nfor i in range(0,1_000):\n    vanishing_points = {}\n    vanishing_points['VP1'] = (PTS1[i,0,0],PTS1[i,1,0])\n    vanishing_points['VP2'] = (PTS2[i,0,0],PTS2[i,1,0])\n    vanishing_points['VP3'] = (PTS3[i,0,0],PTS3[i,1,0])\n    \n    try:\n        A = generate_A(vanishing_points)\n        K = compute_K(A)\n        R = compute_R(K,vanishing_points)\n\n        ax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color='r',alpha=0.01)\n        ax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color='g',alpha=0.01)\n        ax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color='b',alpha=0.01)\n    except:\n        pass\n\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-06-vanishing-points-in-practice.html#parameter-distributions",
    "href": "posts/2020-07-06-vanishing-points-in-practice.html#parameter-distributions",
    "title": "Vanishing points in practice",
    "section": "Parameter distributions",
    "text": "Parameter distributions\n\nQuaternions\nFrom the above visualisation, we can see that we have a distribution of attitudes. Let’s make a subtle change and find the quaternion representation of the angles.\nQuaternions are an elegant way to represent a rotation in 3 dimensions, using four numbers. Without going into details, they have many valuable properties. If you want to learn more, 3Blue1Brown has done some incredible videos explaining them in more detail.\nLet’s do a Monte Carlo simulationand compute both the means, and variances of the attitudes.\n\nquats = []\nKs = []\nPTS1  = monte_carlo_simulation(point_name='VP1',num_samples = 10_000)\nPTS2  = monte_carlo_simulation(point_name='VP2',num_samples = 10_000)\nPTS3  = monte_carlo_simulation(point_name='VP3',num_samples = 10_000)\n\n\nfor i in range(0,10_000):\n    vanishing_points = {}\n    vanishing_points['VP1'] = (PTS1[i,0,0],PTS1[i,1,0])\n    vanishing_points['VP2'] = (PTS2[i,0,0],PTS2[i,1,0])\n    vanishing_points['VP3'] = (PTS3[i,0,0],PTS3[i,1,0])\n    try:\n        A = generate_A(vanishing_points)\n        K = compute_K(A)\n        R = compute_R(K,vanishing_points)\n        \n        R = R.T\n        r = Rot.from_matrix(R)\n        \n        Ks.append(K.ravel())\n        quats.append(r.as_quat())\n    except:\n        pass\n\nKs = np.vstack(Ks)\nquats = np.vstack(quats)\n\n\nsns.distplot(quats[:,0],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label='X');\nsns.distplot(quats[:,1],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label='Y');\nsns.distplot(quats[:,2],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label='Z');\nsns.distplot(quats[:,3],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label='W');\n\nplt.legend()\nplt.show()\n\n\n\n\nOk, now let’s fit Student’s T distribution to each quaternion component:\n\ncomponent_names = ['X','Y','Z','W']\nfor i in range(0,4):\n    nu, mu, sigma =  scipy.stats.distributions.t.fit(quats[:,i])\n    print(\"{}: nu: {:.3f}, mu: {:.3f}, sigma: {:.3f}\".format(component_names[i],nu,mu,sigma)) \n    \n\nX: nu: 1.824, mu: 0.706, sigma: 0.015\nY: nu: 1.694, mu: -0.298, sigma: 0.004\nZ: nu: 2.015, mu: 0.272, sigma: 0.011\nW: nu: 0.970, mu: 0.590, sigma: 0.019\n\n\n\n\nFocal Length\nNow let’s do the same for the focal length:\n\nsns.distplot(Ks[:,0],kde=False,fit=scipy.stats.norm,label='Focal length');\nplt.legend()\nplt.show()\n\nmean, var  = scipy.stats.distributions.norm.fit(Ks[:,0])\nprint(\"Mean: {:.2f}, Std: {:.2f}\".format(mean,np.sqrt(var)))\n\n\n\n\nMean: 2189.49,Std: 11.74\n\n\n\n\nPrinciple Point\nFinally, let’s see what we think the principal point of the camera might be:\n\nsns.distplot(Ks[:,2],kde=False,fit=scipy.stats.norm,label='cx');\nplt.legend()\nplt.show()\n\nmean, var  = scipy.stats.distributions.norm.fit(Ks[:,2])\nprint(\"Mean (CX): {:.2f}, Std: {:.2f}\".format(mean,np.sqrt(var))) \n\nsns.distplot(Ks[:,5],kde=False,fit=scipy.stats.norm,label='cy');\nplt.legend()\nplt.show()\n\nmean, var  = scipy.stats.distributions.norm.fit(Ks[:,5])\nprint(\"Mean (CY): {:.2f}, Std: {:.2f}\".format(mean,np.sqrt(var))) \n\n\n\n\nMean (CX): 845.77, Std: 9.73\n\n\n\n\n\nMean (CY): 1032.71, Std: 17.25\n\n\nNow let’s plot the joint distribution of the principal point in 2D space.\n\nsns.jointplot(Ks[:,2],Ks[:,5], kind=\"hex\");"
  },
  {
    "objectID": "posts/2021-03-22-Training-Object-Detection-Models-On-Synthetic-Data.html#training-and-evaluation-functions",
    "href": "posts/2021-03-22-Training-Object-Detection-Models-On-Synthetic-Data.html#training-and-evaluation-functions",
    "title": "Training object detection models on synthetic data",
    "section": "Training and evaluation functions",
    "text": "Training and evaluation functions\nIn references/detection/, we have several helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py.\nLet’s copy those files (and their dependencies) here, so they are available in the notebook.\n%%shell\n\n# Download TorchVision repo to use some files from\n# references/detection\ngit clone https://github.com/pytorch/vision.git\ncd vision\ngit checkout v0.3.0\n\ncp references/detection/utils.py ../\ncp references/detection/transforms.py ../\ncp references/detection/coco_eval.py ../\ncp references/detection/engine.py ../\ncp references/detection/coco_utils.py ../\nLet’s write some helper functions for data augmentation/transformation, which leverages the functions in references/detection that we have just copied:\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n        \n    return T.Compose(transforms)\n\nPutting everything together\nWe now have the dataset class, the models and the data transforms. Let’s instantiate them.\n\n# use our dataset and defined transformations\ndataset = TrainingDataset('Training_Data/', get_transform(train=True))\ndataset_test = TrainingDataset('Training_Data/', get_transform(train=False))\n\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)\nNow let’s instantiate the model and the optimizer.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\nAnd now, let’s train the model for ten epochs, evaluating at the end of every epoch.\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\nNow that training has finished let’s look at what it predicts in a test image.\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,10]\n\nfor i in range(0,5):\n  # pick one image from the test set\n  img, _ = dataset_test[i]\n  # put the model in evaluation mode\n  model.eval()\n  with torch.no_grad():\n      prediction = model([img.to(device)])\n  \n  for index in range(len(prediction[0]['boxes'])):\n\n    box = prediction[0]['boxes'][index]\n    score = prediction[0]['scores'][index]\n    [xmin,ymin,xmax,ymax] = box.cpu().numpy()\n\n    if score.cpu().numpy()>0.5:\n      plt.plot([xmin,xmax,xmax,xmin,xmin],[ymin,ymin,ymax,ymax,ymin], color='r', linewidth=2)\n\n  test_image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())  \n  plt.imshow(test_image)\n  plt.show()\n\n\n\n_config.yml\n\n\n\n\n\n_config.yml\n\n\n\n\n\n_config.yml\n\n\n\n\n\n_config.yml\n\n\n\n\n\n_config.yml\n\n\ntorch.save(model.state_dict(), 'Dragon_Model.tar')"
  },
  {
    "objectID": "posts/2020-08-21-afterpay-customer-defaults-part-1.html",
    "href": "posts/2020-08-21-afterpay-customer-defaults-part-1.html",
    "title": "How often do Afterpay transactions attract late fees? (Part 1)",
    "section": "",
    "text": "I’m interested in what a typical default with Afterpay looks like. I have read hundreds of pages of information published by Afterpay, but I’m yet to see them mention the average default size.\nBecause I’m curious and looking for a way to entertain myself on a long train ride, I decided to work it out myself.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#what-do-we-know",
    "href": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#what-do-we-know",
    "title": "How often do Afterpay transactions attract late fees? (Part 1)",
    "section": "What do we know?",
    "text": "What do we know?\n\nLate Fees Revenue: 46.1 million AUD (Page 54 FY2019 Annual report)\nAverage Transaction Value: Approximately 150 AUD (Page 25 FY2019 Annual report)\n\nFurthermore, we know that the lowest and highest fee you can charge for a single transaction is 10 AUD and 68 AUD. Hence, this, in turn, bounds the average of the late fees.\n\n\n\nPage 43 of the Afterpay FY2019 Annual Report\n\n\nLet’s think about the different paths a transaction could take.\n\nThe customer makes good on all their payments on time.\nThe customer makes no payments, including late fees.\nThe customer is continually late making payments but, in the end, makes all the payments required.\nA combination of 2 and 3, where the customer makes some payments before defaulting.\n\nIn cases 2 and 4, there will be a contribution to GROSS LOSS (Afterpay doesn’t get paid what’s owed in total).\nIn the case of 3 and 4, there will be a contribution to LATE FEES (Afterpay doesn’t get paid on time).\nI will use PyMC3 to perform a Monte Carlo simulation, estimating how often cases 3 and 4 occur.\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport scipy\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nWhile not strictly necessary, I’m modelling “underlying_sales_aud”, “late_fees_rev_aud”, and “average_transaction_value_aud” as random variables so that they show up in the variable graph.\nI’m also going to model average_transaction_value_aud, assuming they have rounded to the nearest 10 AUD.\n\nwith pm.Model() as model:\n    underlying_sales_aud = pm.Uniform('underlying_sales_aud', lower=5.24715*10**9, upper=5.247249*10**9)\n    \n    late_fees_rev_aud = pm.Uniform('late_fees_rev', lower=46.05 * 10**6, upper=46.149 * 10**6)\n    \n    average_transaction_value_aud = pm.Uniform('average_transaction_value', lower=144.50, upper=154.49)\n    \n    average_late_fee_aud = pm.Uniform('average_late_fee',lower = 10, upper = 68)\n    \n    number_of_transactions = pm.Deterministic('number_of_transactions', underlying_sales_aud / average_transaction_value_aud)\n    \n    late_payment_rate = pm.Deterministic('late_payment_rate',late_fees_rev_aud / (number_of_transactions * average_late_fee_aud))\n    \n\nNow that we have instantiated all the random variables, we will take 50,000 draws from them to perform our Monte Carlo simulation.\n\nwith model:\n    samples = pm.sample_prior_predictive(samples=50_000, random_seed=0)"
  },
  {
    "objectID": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#variable-graph",
    "href": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#variable-graph",
    "title": "How often do Afterpay transactions attract late fees? (Part 1)",
    "section": "Variable Graph",
    "text": "Variable Graph\nWe can graph the relationship between all our variables. From this, we can quickly see which variables are critical dependencies. \n\npm.model_to_graphviz(model)"
  },
  {
    "objectID": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#results",
    "href": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#results",
    "title": "How often do Afterpay transactions attract late fees? (Part 1)",
    "section": "Results",
    "text": "Results\nWe can now visualise the distribution of possible values for the late payment rate.\n\nsns.distplot(100*samples[\"late_payment_rate\"], kde=False, norm_hist=True, bins=100)\nplt.title('Distribution of frequency of late payements (%)')\nplt.xlabel('Percentage of transactions with late payment (%)')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\nFrom this chart, we can see a high likelihood that the value of this parameter is bounded between 2 and 14%.\nWe can find a 94% chance the value is between 1.9% and 9.8% using PyMC3’s summary function.\n\npm.summary(samples['late_payment_rate'])\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      0.043\n      0.025\n      0.019\n      0.097\n      0.0\n      0.0\n      48586.0\n      47975.0\n      49350.0\n      46251.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#conclusion",
    "href": "posts/2020-08-21-afterpay-customer-defaults-part-1.html#conclusion",
    "title": "How often do Afterpay transactions attract late fees? (Part 1)",
    "section": "Conclusion",
    "text": "Conclusion\nBased on our assumptions, we can understand how common it is for Afterpay customers to be late in payment. Based on our model and beliefs, it’s approximately 4.3% of the time. However, this is almost certainly wrong because:\nWe made several implicit assumptions: 1. All payments are the same size. 2. The average late fee is uniformly distributed between 10 AUD and 68 AUD.\nIn future posts, I want to refine the model further, build a more accurate distribution, narrow its bounds, and try to determine a result we can have more confidence in."
  },
  {
    "objectID": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html",
    "href": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html",
    "title": "How long is the Blacklist? (With PyMC3)",
    "section": "",
    "text": "In the Last post, I took a first-principles approach to solve a Bayesian analysis problem computationally. In practice, problems can be far more complex. Fortunately, sophisticated software libraries already exist. We can leverage that to solve equally tricky problems. While several options are available, my current favourite is PyMC3.\nIf you want to be inspired by what’s possible, I strongly suggest checking out the fantastic set of examples on the PyMC3 site."
  },
  {
    "objectID": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#putting-it-into-practice",
    "href": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#putting-it-into-practice",
    "title": "How long is the Blacklist? (With PyMC3)",
    "section": "Putting it into practice",
    "text": "Putting it into practice\nLet’s start as we did last time by taking a sample of the first ten elements from the blacklist.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 15]\n\n# E: the data\ny = np.array([52, 145, 84, 161, 85, 152,\n       47, 109, 16, 16, 106, 101,\n       64, 73, 57, 83, 88, 135,\n       119,120,121, 122, 42, 8, 8, 104, 112,\n       89, 82, 74, 114, 22, 12,\n       21, 21, 67, 71, 93, 94,\n       75, 7, 97, 117, 62, 87,\n       55, 11, 38, 80, 72, 43,\n       50, 86, 31, 108, 24, 24,\n       95, 132, 103, 77, 113, 78,\n       32, 32, 41, 18, 14, 14,\n       79, 66, 65, 81, 105, 53,\n       98, 98, 111, 163, 102,\n       34, 107, 59, 10, 61, 29,\n       46, 4, 4, 30, 37, 76, 44,\n       54, 90, 48, 13, 118, 100,\n       56, 63, 51, 68, 19, 25,\n       23, 13, 110, 26, 17, 33,\n       20, 124, 146, 147, 131, 91, 116,\n       58, 99, 160, 20, 20, 9,\n        6, 115, 69, 136, 92, 128,\n       60, 15, 27, 27, 151, 138,\n       130, 125, 162, 159, 3, 137,\n       155, 144, 126, 158, 149,\n       150])\n\n\ny_sample = y[0:10]\nplt.stem(np.sort(y_sample), use_line_collection=True)\nplt.show()\n\n\n\n\nNow, this is when things start getting interesting. Firstly I will lay out all the code we need to solve this problem, all seven lines.\n\nimport pymc3 as pm\n\nmodel = pm.Model()\nwith model:\n    # prior - P(N): N ~ uniform(max(y), 500)\n    N = pm.DiscreteUniform(\"N\", lower=y.max(), upper=500)\n\n    # likelihood - P(D|N): y ~ uniform(0, N)\n    y_obs = pm.DiscreteUniform(\"y_obs\", lower=0, upper=N, observed=y)\n    \n    trace = pm.sample(10_000, start={\"N\": y.max()}) \n\npm.plots.plot_posterior(trace)\n\nMultiprocess sampling (2 chains in 2 jobs)\nMetropolis: [N]\nSampling 2 chains, 0 divergences: 100%|██████████| 21000/21000 [00:05<00:00, 4179.11draws/s]\nThe number of effective samples is smaller than 10% for some parameters.\n\n\narray([<AxesSubplot:title={'center':'N'}>], dtype=object)\n\n\n\n\n\nOk, we are 94% confident the answer (the length of the Blacklist) is in the range of 163 to 166."
  },
  {
    "objectID": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#line-by-line.",
    "href": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#line-by-line.",
    "title": "How long is the Blacklist? (With PyMC3)",
    "section": "Line by Line.",
    "text": "Line by Line.\nLet’s step through now, line by line, to understand what’s going on.\n\nImport PyMC3.\n\nimport pymc3 as pm\n\nCreate a PyMC3 model.\n\nmodel = pm.Model()\n\nCreate a context manager so that that magic can happen behind the scenes\n\nwith model:\n\nCreate a distribution for \\(N\\), our prior belief of the length of the Blacklist (\\(P(H)\\)). In layperson’s terms, $ N $ is equally likely to be any integer between the highest index we observe in our sample of data, 500. The number 500 is arbitrary; ultimately, the data we observe will “wash out” any assumptions we have made.\n\nN = pm.DiscreteUniform(\"N\", lower=y.max(), upper=500)\n\nCreate our likelihood function, \\(P(E|H)\\). Given the data we have observed, how likely is any value of N? Again, we are using a discrete uniform distribution to model the probability of seeing any observed data point.\n\ny_obs = pm.DiscreteUniform(\"y_obs\", lower=0, upper=N, observed=y)\n\nSample the model. We are computationally finding \\(P(H|E)\\). This is where the real magic occurs. In this case, we generate 10,000 samples and store them in ‘trace’.\n\ntrace = pm.sample(10_000, start={\"N\": y.max()}) \n\nPlot the distribution of the parameters. This is our posterior, (\\(P(H|E)\\)).\n\npm.plots.plot_posterior(trace)"
  },
  {
    "objectID": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#conclusion",
    "href": "posts/2020-05-20-how-long-is-the-blacklist-with-pymc3.html#conclusion",
    "title": "How long is the Blacklist? (With PyMC3)",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, PyMC3 can quickly and efficiently conduct Bayesian analysis. I hope to do more examples in future posts, looking at other ‘real world’ problems."
  },
  {
    "objectID": "posts/2020-04-12-r-from-vanishing-points.html",
    "href": "posts/2020-04-12-r-from-vanishing-points.html",
    "title": "R From Vanishing points",
    "section": "",
    "text": "In this post, I will focus on determining the camera’s rotation with respect to the scene, using three orthogonal vanishing points present in the scene.\n\n\n\n\n\n\nNote\n\n\n\nHartley & Zisserman put it best, “Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.”\n\n\nThis is a great video to watch for another perspective on this problem.\nOk, now let’s import some valuable libraries and visualise the scene.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.set_printoptions(precision=3)\n\nplt.rcParams['figure.figsize'] = [15, 15]\n\n\nimg = Image.open('data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg')\nplt.imshow(img)\nplt.show()\n\n\n\n\nNow is an excellent time to pause and talk about homogeneous coordinates.\nLet’s assume we are comfortable with the Euclidean coordinate system (X, Y, Z). Let’s pretend that we have a point, \\(p\\), at:\n\n\\(X = 5\\)\n\\(Y = 7\\)\n\\(Z = 4\\)\n\nI.E.\n\\(p = \\begin{bmatrix} 5\\\\ 7\\\\ 4 \\end{bmatrix}\\)\nLet’s go out on a limb. Let’s take \\(p\\), and multiply each component by some amount (\\(k\\)). In addition, let’s append \\(k\\) as an extra dimension.\n\\(p = \\begin{bmatrix} 5k\\\\ 7k\\\\ 4k\\\\ k \\end{bmatrix}\\)\nThis is the homogeneous representation of \\(p\\).\nLet’s now take a point on the \\(Z\\) axis, perhaps \\(Z=1\\) for example:\n\\(p = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix}\\)\nor\n\\(p = \\begin{bmatrix} 0k\\\\ 0k\\\\ 1k\\\\ k \\end{bmatrix}\\)\nor\nIf k is 0 and \\(Z\\) is any other positive value (apart from 0), then it will be a point infinitely far away on the Z-axis. This is also the Z vanishing point.\n\\(p = \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ 0 \\end{bmatrix}\\)\nFrom my other post, we can see that we can map from the points in 3D space (\\(X\\)) to points in 2D space (\\(x\\)) using the matrix \\(P\\).\n\\[\\begin{equation*}\nx = PX\n\\end{equation*}\\]\nThe matrix \\(P\\), in turn, consists of 3 parts. 1. A intrinsic camera matrix \\(K\\) 2. A Rotation matrix \\(R\\) 3. A translation matrix \\(t\\)\n\\[\\begin{equation*}\nP = K[R | t]\n\\end{equation*}\\]\nAssuming that the camera is free from radial distortion, the Z vanishing point can be found as follows.\n$v_z = K\n\\[\\begin{bmatrix}\nR_1 & R_2 & R_3 & | & t\n\\end{bmatrix}\\begin{bmatrix}\n0\\\\\n0\\\\\n1\\\\\n0\n\\end{bmatrix}\\]\n$\nWe realise that we can knock out everything but the column \\(R_3\\) of the rotation matrix \\(R\\).\nIn the previous post, we already found the matrix \\(K\\).\n\\(K = \\begin{bmatrix} 728 & 0 & 1327\\\\ 0 & 728 & 706\\\\ 0 & 0 & 1 \\end{bmatrix}\\)\nNow, we can find \\(R_3\\) as follows:\n$R_3 = $\nLet’s put this into practice.\nNow, let’s make some assumptions.\n\nWorld X axis pointing right (Red arrow)\nWorld Y axis pointing into the scene (Green arrow)\nWorld Z axis pointing up (Blue arrow)\n\n\n\n\nDiagram of a pinhole camera\n\n\n\n\n\nDiagram of a pinhole camera\n\n\n\n\n\nDiagram of a pinhole camera\n\n\n\nvanishing_points = {'VP1': [1371.892, 630.421],\n                    'VP2': [-10651.54, 536.681],\n                    'VP3': [1272.225, 7683.02 ]}\n\nK = np.array([[7.276e+02,0.000e+00,1.327e+03],\n              [6.236e-14,7.276e+02,7.060e+02],\n              [1.218e-16,0.000e+00,1.000e+00]])\n\ndef make_homogeneous(x):\n    x_homogeneous = np.array([x[0],x[1],1])\n    return(x_homogeneous)\n\nv_x_h = make_homogeneous(vanishing_points['VP2']) \nv_y_h = make_homogeneous(vanishing_points['VP1'])\nv_z_h = make_homogeneous(vanishing_points['VP3'])\n\nK_inv = np.linalg.inv(K)\n\nR_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T\nR_3 = -1 * np.dot(K_inv,v_z_h)/np.linalg.norm(np.dot(K_inv,v_z_h)).T\nR_1 = np.cross(R_2,R_3) \n\nR = np.vstack([R_1,R_2,R_3])\n\nprint(R)\n\n[[ 0.998  0.014 -0.06 ]\n [ 0.061 -0.103  0.993]\n [ 0.008 -0.995 -0.104]]\n\n\nThis correlates well with what we were expecting. The rotation matrix takes us from the world coordinate system to the camera’s coordinate system. In particular, we can see that the matrix \\(R\\) maps (approximately):\n\nWorld X-axis to Camera X-axis\nWorld Y-axis to Camera Negative Z-axis\nWorld Z-axis to Camera Y-axis\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\n\nax = fig.add_subplot(111, projection='3d')\nax.plot(xs=[0,1], ys=[0,0], zs = [0,0],color='r',label='X World')\nax.plot(xs=[0,0], ys=[0,1], zs = [0,0],color='g',label='Y World')\nax.plot(xs=[0,0], ys=[0,0], zs = [0,1],color='b',label='Z World')\nplt.legend()\nplt.show()\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color='r',label='X Camera')\nax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color='g',label='Y Camera')\nax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color='b',label='Z Camera')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "",
    "text": "A lot is going on in this post, but our ultimate goal is to conduct a robustness study.\nFrom the previous post, I looked at how to model customer late payment rates from a single distribution. Now I want to turn things up a notch and model it using a range of distributions from two different families.\nTo achieve this, we need to:\n\nCompute distributions of late fees rapidly.\nCompute the average late fee under a wide range of assumptions.\nFind the distribution of how frequent late payments are.\n\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot.\n\n\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport scipy.stats as st\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#improving-performance",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#improving-performance",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "Improving performance",
    "text": "Improving performance\n\nDirect Model\nAs part of our simulation, we need to know the distribution of late fees for any given probability of late payment.\nWe can directly model this in PyMC3, using the Binomial distribution to model the number of times someone makes a late payment. While this model directly models what happens in reality, it takes 3 seconds to generate 100,000 samples.\n\ndef create_late_fee_distribution(late_payment_probability, num_samples=100_000):\n    with pm.Model() as fee_model:\n        number_of_delays_of_less_than_1_week = 1 + pm.Binomial('number_of_delays_of_less_than_1_week', n=3, p=late_payment_probability)\n        number_of_delays_of_more_than_1_week = pm.Binomial('number_of_delays_of_more_than_1_week', n=number_of_delays_of_less_than_1_week, p=late_payment_probability)\n        total_fees = pm.Deterministic('total_fees',10 * number_of_delays_of_less_than_1_week + 7 * number_of_delays_of_more_than_1_week)\n\n        samples = pm.sample_prior_predictive(samples = num_samples, random_seed=0)\n    return(samples)\n\nLet’s visualise the distribution; we would see if a customer has a 50% chance of defaulting on any given payment.\n\nsamples = create_late_fee_distribution(late_payment_probability = 0.5)\n\nsns.distplot(samples[\"total_fees\"],kde=False, norm_hist=True, bins=np.arange(10,70,1))\nplt.title('Histogram of late fees')\nplt.ylabel('Frequency')\nplt.xlabel('Late fee (AUD)')\nplt.show()\n\n\n\n\n\n\nCategorical Model\nOne solution to enable us to sample faster is to capture the output from the direct model, which precisely models the underlying process, and to create a surrogate model. This model uses a categorical distribution to efficiently generate samples with the same distribution.\nUsing this surrogate model, we can generate 100,000 samples in only 60ms, a 50-fold speedup.\n\ndef create_late_fee_surrogate_distribution(late_fee_samples):\n    unique, counts = np.unique(late_fee_samples, return_counts=True) \n    \n    categorical_map = {}\n    for i in range(len(unique)):\n        categorical_map[i] = unique[i]\n    \n    with pm.Model() as categorical_fee_model:\n        late_fee_distribution_categorical = pm.Categorical('late_fee_distribution_categorical',counts)\n    \n    return(categorical_fee_model, categorical_map)\n\ndef generate_samples(categorical_fee_model, categorical_map):\n    with categorical_fee_model:\n        samples = pm.sample_prior_predictive(samples=100_000)\n\n    late_fee_samples = np.zeros_like(samples[\"late_fee_distribution_categorical\"])\n\n    for i in range(len(categorical_map.keys())):\n        late_fee_samples[samples[\"late_fee_distribution_categorical\"] == i] = categorical_map[i]\n\n    return(late_fee_samples)\n\ncategorical_fee_model, categorical_map = create_late_fee_surrogate_distribution(samples[\"total_fees\"])\n\nlate_fee_samples = generate_samples(categorical_fee_model, categorical_map)\n\nplt.title('Histogram of late fees')\nplt.ylabel('Relative Frequency')\nplt.xlabel('Late fee (AUD)')\nsns.distplot(late_fee_samples,kde=False, norm_hist=True, bins=np.arange(10,70,1))\nplt.show()"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#grid-evaluation",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#grid-evaluation",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "Grid Evaluation",
    "text": "Grid Evaluation\nWe can now find the average late fee under different combinations of late payment probability and average transaction size.\nFirst, let’s create a dictionary of categorical models to draw new samples quickly.\n\nfee_models = {}\nfor late_payment_probability_percent in range(0, 101,5):\n    samples = create_late_fee_distribution(late_payment_probability = late_payment_probability_percent/100.0) \n    fee_models[late_payment_probability_percent] = create_late_fee_surrogate_distribution(samples[\"total_fees\"])\n\nNow it’s time to generate the grid.\n\ndef perform_parametric_study(transaction_value_sampling_function):\n    grid = np.zeros((21,20))\n    for average_transaction_value_aud in range(50,250,10):\n\n        transaction_value_samples = transaction_value_sampling_function(average_transaction_value_aud)\n        for late_payment_probability_percent in range(0, 101,5):\n            categorical_fee_model, categorical_map = fee_models[late_payment_probability_percent]\n\n            #Draw a distribution of late fees\n            late_fee_samples = generate_samples(categorical_fee_model,categorical_map)\n\n            #Limit the late fees to be at at most 25% of the transaction value\n            late_fees = np.minimum(late_fee_samples, 0.25*transaction_value_samples)\n            #And at least $10 AUD\n            late_fees = np.maximum(late_fees,10)\n\n            mean_late_fee = np.mean(late_fees)\n\n            grid[int(late_payment_probability_percent/5), int((average_transaction_value_aud-50)/10)] = mean_late_fee\n            \n    return(grid)\n\n\ndef visualise_grid(grid,title):\n    x_axis_labels = range(50,250,10) \n    y_axis_labels = range(0, 101,5)\n\n    sns.heatmap(grid,xticklabels=x_axis_labels, yticklabels=y_axis_labels,cbar_kws={'label': 'Average late fee (AUD)'},annot=True)\n    plt.xlabel('Average Transaction Value (AUD)')\n    plt.ylabel('Probability of late payment on any transaction (%)')\n    plt.title(title)\n    plt.show()"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#distributions",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#distributions",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "Distributions",
    "text": "Distributions\nOne of the big unknowns is the distribution of the transactions where there are late payments. To provide some robustness in our modelling, I’m using two different families of distributions.\n\nThe Exponential distribution\nThe Half Normal distribution\n\nI will generate distributions with means in the range of 50-250 AUD for each family of distributions.\nThis, in turn, will help provide conservatism and robustness to our modelling.\n\nExponential distribution\n\nx = np.linspace(0, 1500,1500)\nfor average_transaction_value_aud in range(50,250,10):\n    pdf = st.expon.pdf(x, scale = average_transaction_value_aud )\n    plt.plot(x, pdf,color='k',alpha=0.5)\nplt.xlabel('Transaction Value (AUD')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\ndef create_exponential_transaction_value_samples(average_transaction_value_aud):\n    with pm.Model() as transaction_value_model:\n        transaction_value_aud = pm.Bound(pm.Exponential, upper=1_500.0)('transaction_value_aud', lam = 1/average_transaction_value_aud)\n        transaction_value_samples = pm.sample_prior_predictive(samples=100_000)\n        transaction_value_samples = transaction_value_samples['transaction_value_aud']\n    return(transaction_value_samples)    \n\nexponential_grid = perform_parametric_study(create_exponential_transaction_value_samples)\nvisualise_grid(exponential_grid,'Parametric study of late fees')\n\n\n\n\n\n\n\n\n\nHalf Normal Distribution\n\nx = np.linspace(0, 1500,1500)\nfor average_transaction_value_aud in range(50,250,10):\n    sigma = average_transaction_value_aud * np.sqrt(np.pi) /  np.sqrt(2)  \n    pdf = st.halfnorm.pdf(x, scale=sigma)\n    plt.plot(x, pdf,color='k',alpha=0.5)\nplt.xlabel('Transaction Value (AUD')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\ndef create_halfnormal_transaction_value_samples(average_transaction_value_aud):\n    sigma = average_transaction_value_aud * np.sqrt(np.pi) /  np.sqrt(2)     \n    with pm.Model() as transaction_value_model:\n        transaction_value_aud = pm.Bound(pm.HalfNormal,lower=0, upper=1_500.0)('transaction_value_aud', sigma = sigma)\n        transaction_value_samples = pm.sample_prior_predictive(samples = 100_000)\n        transaction_value_samples = transaction_value_samples['transaction_value_aud']\n    return(transaction_value_samples)    \n\n\nhalfnormal_grid = perform_parametric_study(create_halfnormal_transaction_value_samples)\nvisualise_grid(halfnormal_grid,'Parametric study of late fees')"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#updating-our-priors",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#updating-our-priors",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "Updating our Priors",
    "text": "Updating our Priors\nIn a previous post, we developed a model for calculating the distribution of transactions attracting late fees. One of the assumptions we used was that the average late fee was uniformly distributed between 10 AUD and 68 AUD.\nGiven what we now know, we can find a more realistic distribution. I’ve chosen to go with Half Normal, which fits quite well.\n\nlate_fees_aggregated = np.stack([halfnormal_grid, exponential_grid]).ravel()\n\noffset, std = st.halfnorm.fit(late_fees_aggregated)\n\nprint('Offset: {:0.2f}, Standard Deviation: {:0.2f}'.format(offset,std))\n\nsns.distplot(late_fees_aggregated,fit=st.halfnorm,  kde=False, bins = np.arange(10,68,5))\nplt.ylabel('Relative Frequency')\nplt.xlabel('Average late fee (AUD)')\nplt.show()\n\nOffset: 10.00, Standard Deviation: 14.30\n\n\n\n\n\nWe can also find the average late fee (21.7 AUD).\n\npm.summary(late_fees_aggregated)\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 840), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      21.673\n      8.264\n      10.0\n      36.41\n      2.541\n      1.849\n      11.0\n      11.0\n      10.0\n      40.0\n      NaN\n    \n  \n\n\n\n\nNow let’s re-do the model from this post, except now using an updated distribution for the late fee:\n\nwith pm.Model() as model:\n    underlying_sales_aud = pm.Uniform('underlying_sales_aud', lower=5.24715*10**9, upper=5.247249*10**9)\n    \n    late_fees_rev_aud = pm.Uniform('late_fees_rev', lower=46.05 * 10**6, upper=46.149 * 10**6)\n    \n    average_transaction_value_aud = pm.Uniform('average_transaction_value', lower=144.50, upper=154.49)\n    \n    #The updated distribution\n    average_late_fee_aud = 10 + pm.HalfNormal('average_late_fee',sigma = 14.298)\n    \n    number_of_transactions = pm.Deterministic('number_of_transactions', underlying_sales_aud / average_transaction_value_aud)\n    \n    late_payment_rate = pm.Deterministic('late_payment_rate',late_fees_rev_aud / (number_of_transactions * average_late_fee_aud))\n    \n\nWe can now sample this distribution and find an updated distribution.\n\nwith model:\n    samples = pm.sample_prior_predictive(samples=50_000, random_seed=0)\n\n\nsns.distplot(100*samples[\"late_payment_rate\"], kde=False, norm_hist=True, bins=100)\nplt.title('Distribution of frequency of late payements (%)')\nplt.xlabel('Percentage of transactions with late payment (%)')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\n\npm.summary(100*samples[\"late_payment_rate\"])\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      7.11\n      2.644\n      3.011\n      12.23\n      0.012\n      0.008\n      50870.0\n      50799.0\n      50628.0\n      49878.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#conclusion",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-3.html#conclusion",
    "title": "How often do Afterpay transactions attract late fees? (Part 3)",
    "section": "Conclusion",
    "text": "Conclusion\nBased on our analysis and making some conservative assumptions, we can see that the average late fee is likely 10 to 36 AUD.\nIf we use the distribution of late fees as an updated input to the model we developed in this post, we find that between 3 and 12% of transactions are attracting late fees. Overall, our best estimate is 7%."
  },
  {
    "objectID": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html",
    "href": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html",
    "title": "Synthetic training data with Blender",
    "section": "",
    "text": "Supervised learning in computer vision is fundamentally about building a model that can transform an input x into an output y.\nUsing Blender, we have seen how we can generate arbitrary scenes, and in this post, I will look at how we can create Semantic Segmentation Maps and Depth Maps.\nWhile writing this post, I found this post by Tobias Weis to be beneficial for understanding rendering nodes.\nFor a more industrial solution, I suggest looking at the bpycv project on GitHub."
  },
  {
    "objectID": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html#the-code",
    "href": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html#the-code",
    "title": "Synthetic training data with Blender",
    "section": "The Code",
    "text": "The Code\nLet’s start by importing the bpy library and removing a cube in the scene.\nimport os\nimport bpy\nbpy.data.objects.remove(bpy.data.objects['Cube'], do_unlink = True)\n\nObjects\nLet’s create a ground plane and then place three dragons on it.\nI’m using the dragon model from the Stanford scanning dataset.\nA key point to note is that I’m assigning each dragon a unique index or identifier. Later during the rendering process, the renderer will tell us the object’s index that makes up each pixel. This will allow us to generate a pixel-by-pixel semantic map of the image.\ndef create_dragon(location, rotation, rgba, index):\n        #Load the mesh\n        bpy.ops.import_mesh.ply(filepath=os.getcwd()+\"/dragon_vrip.ply\")\n        ob = bpy.context.active_object #Set active object to the variable\n\n        ob.scale = (10,10,10)\n        ob.location = location\n        ob.rotation_euler = rotation\n\n        #Assign the object an index, which is used to generate a semantic segmentation map\n        bpy.context.object.pass_index = index\n\n        #Create and add material to the object\n        mat = create_dragon_material('Dragon_'+str(index)+'_Material',rgba=rgba)\n        ob.data.materials.append(mat)\n\ndef create_floor():\n        bpy.ops.mesh.primitive_plane_add(size=1000, enter_editmode=False, align='WORLD', location=(0, 0, 0), scale=(100, 100, 1))\n        mat = create_floor_material(material_name='Floor', rgba =  (0.9, 0.9, 0.9, 0)) \n        activeObject = bpy.context.active_object #Set active object to the variable\n        activeObject.data.materials.append(mat)\n\ncreate_floor()\ncreate_dragon(location=(0,0.78,-0.56), rotation=(np.radians(90),0,0), rgba=(0.799, 0.125, 0.0423, 1), index=1)\ncreate_dragon(location=(-1.5,4.12,-0.56), rotation=(np.radians(90),0,np.radians(227)), rgba=(0.0252, 0.376, 0.799, 1), index=2)\ncreate_dragon(location=(1.04,2.7,-0.56), rotation=(np.radians(90),0,np.radians(129)), rgba=(0.133, 0.539, 0.292, 1), index=3)\n\n\nMaterials\nI will create a semi-translucent plastic material with some subsurface scattering and a reflective coating for the dragons.\ndef create_dragon_material(material_name,rgba):\n        mat = bpy.data.materials.new(name=material_name)\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        \n        nodes[\"Principled BSDF\"].inputs['Base Color'].default_value = rgba\n        nodes[\"Principled BSDF\"].inputs['Subsurface'].default_value = 0.5\n        nodes[\"Principled BSDF\"].inputs['Subsurface Color'].default_value = rgba\n        nodes[\"Principled BSDF\"].inputs['Clearoat'].default_value = 0.5\n        return(mat)\n\n\ndef create_floor_material(material_name,rgba):\n        mat = bpy.data.materials.new(name=material_name)\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        \n        nodes[\"Principled BSDF\"].inputs['Base Color'].default_value = rgba\n        nodes[\"Principled BSDF\"].inputs['Clearcoat'].default_value = 0.5\n        return(mat)\n\n\nLight & Camera\ndef configure_light():\n        bpy.data.objects[\"Light\"].data.type = 'AREA'\n        bpy.data.objects[\"Light\"].scale[0] = 20\n        bpy.data.objects[\"Light\"].scale[1] = 20\n\ndef configure_camera():\n        bpy.data.objects[\"Camera\"].location = (0,-4.96579,2.45831)\n        bpy.data.objects[\"Camera\"].rotation_euler = (np.radians(75),0,0)\n\nconfigure_camera()\nconfigure_light()\n\n\nRenderer\nMuch of the complexity comes in configuring the renderer.\nIn particular, we need to create three different output nodes and then link the relevant output from the render to each node.\nWe also need to configure the renderer to record the object index, which we use for building our semantic map.\ndef configure_render():\n        bpy.context.scene.render.engine = 'CYCLES'\n        bpy.context.scene.render.filepath = os.getcwd()+\"/Metadata\"\n\n        #Output open exr .exr files\n        bpy.context.scene.render.image_settings.file_format = 'OPEN_EXR'\n        bpy.context.scene.cycles.samples = 1\n\n        # Configure renderer to record object index\n        bpy.context.scene.view_layers[\"ViewLayer\"].use_pass_object_index = True\n        bpy.context.scene.view_layers[\"ViewLayer\"].use_pass_z = True\n\n\n        # Switch on nodes and get reference\n        bpy.context.scene.use_nodes = True\n        tree = bpy.context.scene.node_tree\n        links = tree.links\n\n        ## Clear default nodes\n        for node in tree.nodes:\n            tree.nodes.remove(node)\n\n        # Create a node for outputting the rendered image\n        image_output_node = tree.nodes.new(type=\"CompositorNodeOutputFile\")\n        image_output_node.label = \"Image_Output\"\n        image_output_node.base_path = \"Metadata/Image\"\n        image_output_node.location = 400,0\n\n        # Create a node for outputting the depth of each pixel from the camera\n        depth_output_node = tree.nodes.new(type=\"CompositorNodeOutputFile\")\n        depth_output_node.label = \"Depth_Output\"\n        depth_output_node.base_path = \"Metadata/Depth\"\n        depth_output_node.location = 400,-100\n\n        # Create a node for outputting the index of each object\n        index_output_node = tree.nodes.new(type=\"CompositorNodeOutputFile\")\n        index_output_node.label = \"Index_Output\"\n        index_output_node.base_path = \"Metadata/Index\"\n        index_output_node.location = 400,-200\n\n        # Create a node for the output from the renderer\n        render_layers_node = tree.nodes.new(type=\"CompositorNodeRLayers\")\n        render_layers_node.location = 0,0\n\n        # Link all the nodes together\n        links.new(render_layers_node.outputs['Image'], image_output_node.inputs['Image'])\n        links.new(render_layers_node.outputs['Depth'], depth_output_node.inputs['Image'])\n        links.new(render_layers_node.outputs['IndexOB'], index_output_node.inputs['Image'])\n\nconfigure_render()\nThis is what it looks like in the Blender compositing interface when the configuration is complete:\n\n\n\n_config.yml\n\n\nFinally, we can render the scene.\nbpy.ops.render.render(write_still=True)"
  },
  {
    "objectID": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html#the-results",
    "href": "posts/2020-10-23-Synthetic-Training-Data-With-Blender.html#the-results",
    "title": "Synthetic training data with Blender",
    "section": "The Results",
    "text": "The Results\n\nImage output\nThe render has generated three outputs in OpenEXR format: an image output, a depth map, and a semantic segmentation map.\n\n\n\n_config.yml\n\n\n\n\nDepth map\n\n\n\n_config.yml\n\n\n\n\nSemantic Segmentation map\n\n\n\n_config.yml\n\n\nIn the next post, I will look at working with the OpenEXR maps in Python."
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-6.html",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-6.html",
    "title": "How often do Afterpay’s customer’s default? (Part 6)",
    "section": "",
    "text": "Important\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#raison-dêtre",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#raison-dêtre",
    "title": "How often do Afterpay’s customer’s default? (Part 6)",
    "section": "Raison d’être",
    "text": "Raison d’être\n“Buy Now, Pay Later” (BNPL) service providers like Afterpay have stormed onto the scene, attracting the attention of both consumers as well as investors.\nI’m intrigued by Afterpay. A cohort of consumers may require the cost of a purchase to be spread over eight weeks to afford it. However, I need to see the attraction for most creditworthy customers. Unless Afterpay dramatically allows me to buy shares, I’m not interested.\nHowever, because I don’t see the value doesn’t mean that other people might adopt and love the product. Millions of people in Australia and overseas regularly use Afterpay.\nAll things being equal, I can see the consumer benefits of Afterpayvs vs. other more classical forms of consumer credit (Credit cards, payday loans etc.). However, Afterpay’s customers don’t live in a vacuum, and some cohort of their customers may be using a medley of products and services to finance their lifestyle.\nI wanted to dig deeper and disentangle what was happening unobserved beneath the surface. Afterpay must protect its brand image, so I understand why Afterpay might prefer to talk highly about adverse outcomes. However, these things exist outside the reality of what Afterpay reports to the ASX and outside my focus.\nMy goal is to try and reconstruct the most transparent picture possible, even though the officially published information forces us to look through a glass darkly."
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#what-are-the-numbers",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#what-are-the-numbers",
    "title": "How often do Afterpay’s customer’s default? (Part 6)",
    "section": "What are the numbers?",
    "text": "What are the numbers?\n\nApproximately 10% of purchases incur a late fee*.\nIf you are late on one payment, there is a moderate chance that you will be late on at least one of the other 3 (Roughly 20%)*.\nLate fees are 12-14 AUD on average*.\n\nAltogether, this is interesting and brings a lot of clarity to any discussion around Afterpay and late fees. However, what we are seeing are these numbers in isolation. We will never know what trade-offs someone may have made to pay Afterpay one time."
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#next-steps",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-6.html#next-steps",
    "title": "How often do Afterpay’s customer’s default? (Part 6)",
    "section": "Next steps",
    "text": "Next steps\nNow that we have a clearer picture of late fees, I want to focus on defaults. I’m interested in how often people walk away. This is a topic where Afterpay needs to provide more information apart from a headline of losses. Hence, I think any clarity would be valuable in helping us understand Afterpayand its impact on society for better or worse."
  },
  {
    "objectID": "posts/2022-05-16-Why-I-Blog.html",
    "href": "posts/2022-05-16-Why-I-Blog.html",
    "title": "Why I blog",
    "section": "",
    "text": "It’s a question I’ve been debating with myself. It’s easy to argue that writing a blog is an investment, but is it a good one?\nWhen I’m doing a post, it’s about something I’m learning or something I think is interesting. Writing a blog helps me crystalize what is in my mind, and often explaining something allows me to see new connections and perspectives. I don’t think you truly understand a concept until you can concisely explain it to someone else.\nIt’s also challenging, and I love nothing more than solving gnarly challenges.\nIt’s also a great way to share with the community. While I love exciting new papers, the symbiotic blog posts accompanying them are where the real value lies.\nI’m blogging for a future self, as much as anyone else. A blog post is an elegant way to encapsulate, in a concrete form, lessons that have been learned at the cost of much time, sweat and treasure.\nWhile I love writing, it’s a big-time commitment, and I haven’t been able to invest the time for about a year now. However, I want to get back to writing because, like any skill, it takes practice. I hope the next year allows me to bring you many new posts.\nThis is why I blog.\nCameron"
  },
  {
    "objectID": "posts/2020-08-27-afterpay-customer-defaults-part-4.html",
    "href": "posts/2020-08-27-afterpay-customer-defaults-part-4.html",
    "title": "How often do Afterpay transactions attract late fees? (Part 4)",
    "section": "",
    "text": "Afterpay has just released its FY20 Annual Results. I’ve had a quick skim through it, and two statistics caught my eye.\n\n\n\nLate Fees\n\n\nThis is precisely the information we tried to infer over the last couple of posts, and now we have some actual ground truth to compare!\nAt the end of the last post, I finished by saying: “We find that between 3 and 12% of transactions are attracting late fees. Overall, our best estimate is 7%.” Hence I’m pretty happy that the actual value of ±10% is within the bounds of what we expected.\nNow let’s take this new information and try to infer more about our model’s underlying, unobserved parameters.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot.\n\n\nLet’s think about this at a high level.\nWe now know that approximately 10% of purchases had one or more late payments. This 10% of purchases represent about 10% of payments. Hence, about 30% (±5% points) of payments are late within this cohort.\nThere are four payments, at least one of which is late."
  },
  {
    "objectID": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#the-model",
    "href": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#the-model",
    "title": "How often do Afterpay transactions attract late fees? (Part 4)",
    "section": "The Model",
    "text": "The Model\nLet’s model this out in code; the variable names get pretty verbose!\nIn the case of both “percentage_of_purchases_incuring_late_fees” and “ercentage_of_all_transactions_incuring_late_fees”, I’m modelling them as a uniform distribution. This is because Afterpay is rounding both of them to the nearest integer.\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport scipy.stats as st\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\nwith pm.Model() as model:\n    percentage_of_purchases_incuring_late_fees = pm.Uniform('percentage_of_purchases_incuring_late_fees', lower=9.5, upper=10.49)\n    percentage_of_all_transactions_incuring_late_fees = pm.Uniform('percentage_of_all_transactions_incuring_late_fees', lower=2.5, upper = 3.49)\n    percentage_of_late_purchase_transactions_incuring_late_fees =  pm.Deterministic('percentage_of_late_purchase_transactions_incuring_late_fees', 100 * percentage_of_all_transactions_incuring_late_fees / percentage_of_purchases_incuring_late_fees)\n    average_num_late_purchase_transactions_incuring_late_fees = pm.Deterministic('average_num_late_purchase_transactions_incuring_late_fees',4 * percentage_of_late_purchase_transactions_incuring_late_fees / 100)\n\nLooking at the graph, we can see that the two pieces of information we have provided are used to create a distribution of “percentage_of_late_purchase_transactions_incuring_late_fees”.\n\npm.model_to_graphviz(model)\n\n\n\n\nNow we can draw samples from our model.\n\nwith model:\n    samples = pm.sample_prior_predictive(samples=50_000, random_seed=0)"
  },
  {
    "objectID": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#results",
    "href": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#results",
    "title": "How often do Afterpay transactions attract late fees? (Part 4)",
    "section": "Results",
    "text": "Results\n\nAverage late payments per purchase\nWe can now visualise the distribution of the number of late transactions per purchase, given that there are one or more late fees. Because we have considered the rounding, our model shows that sometimes there is less than one late payment per purchase. This is impossible in practice!\n\nsamples = samples['average_num_late_purchase_transactions_incuring_late_fees']\n\nsns.distplot(samples,bins=np.arange(0.75,1.75,0.01),kde=False,norm_hist=True)\nplt.xlabel('Average number of late payments per purchase')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\n\n\nBinomial assumption\nFinally, we can make one last modelling assumption to help.\nGiven that one of the four payments was late, we might assume that each of the three is equally and independently likely to be late. IE, we use the Binomial distribution to model late payments.\nBinomial(\\(n\\),\\(p\\)), where \\(n\\) is 3, and we want to find \\(p\\).\nGiven the mean of this distribution is \\(n\\times p\\), we can find \\(p = \\frac{(samples - 1)}{3}\\).\n\nprobability_of_late_payment_per_payment = 100 * (samples - 1) /3.0\n\nPutting this all together, we can now find a distribution for \\(p\\).\nI’m also going to fit a Beta distribution to it so I can draw samples from it in the next post.\n\nprobability_of_late_payment_per_payment = probability_of_late_payment_per_payment[np.where(probability_of_late_payment_per_payment>0)]\n\na1, b1, loc1, scale1  = st.beta.fit(probability_of_late_payment_per_payment)\nprint('Alpha: {:0.4f}, Beta: {:0.4f}, Location: {:0.4f}, Scale: {:0.4f}'.format(a1, b1, loc1, scale1 ))\n\nAlpha: 1.3447, Beta: 1.7199, Location: -0.0142, Scale: 15.5965\n\n\nLet’s also visualise the distribution and the fitted Beta distribution.\n\nsns.distplot(probability_of_late_payment_per_payment,fit=st.beta, bins=np.arange(0,20),kde=False,norm_hist=True)\nplt.xlabel('Probability of late payment for each payment (%)')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\n\npm.summary(probability_of_late_payment_per_payment)\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 48482), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      6.898\n      3.844\n      0.051\n      12.912\n      0.018\n      0.012\n      47746.0\n      47746.0\n      47675.0\n      48623.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#conclusion",
    "href": "posts/2020-08-27-afterpay-customer-defaults-part-4.html#conclusion",
    "title": "How often do Afterpay transactions attract late fees? (Part 4)",
    "section": "Conclusion",
    "text": "Conclusion\nBased on what Afterpay has publicly released, we now have a more concrete perspective on how often people pay late.\nInterestingly, while there is a ±10% chance of someone having at least one late payment on any given purchase, the probability of any subsequent individual payment being late is relatively low. It’s at most ±13%, with the best estimate of 7%."
  },
  {
    "objectID": "posts/2021-02-25-precise-positioning-using-aruco-tag-interior-points.html",
    "href": "posts/2021-02-25-precise-positioning-using-aruco-tag-interior-points.html",
    "title": "Precise Positioning using ArUco Tag Interior Points",
    "section": "",
    "text": "In a previous post, I showed how we could use an ArUco tag to find the position and orientation of a camera. This was done using the four exterior points on the tag. If we can detect more points on the tag, we could potentially position the camera more accurately with respect to the tag.\nAt a high level, what we can do is: 1. Detect the exterior corners of the tag 2. Compute the homography (relationship) between the tag and its image 3. Detect points in the image 4. Transform them using the homography 5. Detect points in the tag 6. Match the transformed points from the image with points in the tag 7. Use all of the points to position the camera\nAs always, let’s start by importing what we need.\n\nimport numpy as np\nimport cv2\nimport cv2.aruco as aruco\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as Rot\nimport glob\nfrom sklearn.neighbors import KDTree\n\nplt.rcParams['figure.figsize'] = [20,20]\n\nI’ve already calibrated the camera, which means that we have both the camera matrix (K) and the distortion parameters for the camera.\n\nK = np.array([[3286.9, 0.0 , 2032.8],\n              [0.0   , 3282.3, 1519.1],\n              [0.0   , 0.0   ,    1.0]])\n\ndist = np.array([[2.44663616e-01],[-1.48023303e+00],[2.04607109e-03],[1.53484564e-03],[2.56349651e+00]])\n\npoints_3d = np.array([[0.0,1.0,0.0],\n                      [1.0,1.0,0.0],\n                      [1.0,0.0,0.0],\n                      [0.0,0.0,0.0]])\n\ntag_size_pixels = 1_000\n\nNow we need to configure our ArUco code detector.\nFirstly, specify that it should be looking for a “4X4_50” type tag.\n\narucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n\nSecondly, we tell it that we want to configure the detector; in particular, we want to find the corners to a very high degree of accuracy (Sub-Pixel).\n\narucoParams = cv2.aruco.DetectorParameters_create()\narucoParams.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX\narucoParams.cornerRefinementWinSize = 25\n\n\n\nNow let’s extract the exterior points of the tag.\n\nfname = 'data/2021-02-24-Positioning-Using-ArUco-Tags/ArUco.jpg'\nimg = cv2.imread(fname)\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n\n(aruco_tag_corners, ids, rejected) = cv2.aruco.detectMarkers(gray, arucoDict, parameters=arucoParams)\n\naruco_tag_corners = np.array(aruco_tag_corners).squeeze()\n\n\n\n\nNow let’s find the homography/relationship between the tag and the image.\n\ndef find_homography(corners,tag_size_pixels):\n    src_pts = corners\n    dst_pts = np.array([[0, tag_size_pixels], [tag_size_pixels, tag_size_pixels],\n                        [tag_size_pixels, 0], [0, 0]])\n    \n    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n    return(M)\n\ntag_index = 0\ntag_number = int(ids[tag_index])\n\n\nM = find_homography(aruco_tag_corners[tag_index,:,:], tag_size_pixels)\n\n\n\n\nNow let’s detect the points in the image and the tag.\n\ndef detect_features(image,max_num_features=200,threshold=0.01,min_distance=50):\n    detected_corners = cv2.goodFeaturesToTrack(image,max_num_features,threshold,min_distance)\n    winSize = (5, 5)\n    zeroZone = (-1, -1)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TermCriteria_COUNT, 40, 0.001)\n    # Calculate the refined corner locations\n    detected_corners = cv2.cornerSubPix(image, detected_corners, winSize, zeroZone, criteria)\n    return(detected_corners)\n\n\ndetected_features_image = detect_features(gray)\nfor i in detected_features_image:\n    x,y = i.ravel()\n    plt.scatter(x,y)\n    \nplt.imshow(img[:,:,::-1])\nplt.show()\n\n\n\n\n\n# load the ArUCo dictionary\ndef load_tag(tag_number, tag_size_pixels):\n    arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n    tag = np.zeros((tag_size_pixels, tag_size_pixels, 1), dtype=\"uint8\")\n    cv2.aruco.drawMarker(arucoDict, tag_number, tag_size_pixels, tag, 1);\n    tag = tag.squeeze();\n    return(tag)\n\n\n\n\nNow let’s transform the points detected in the image using the homography. This means that a point detected in the image should be in the same x,y position as the corresponding point in the tag after the transform.\n\ndetected_features_image_transformed = cv2.perspectiveTransform(detected_features_image, M)\ndetected_features_image_transformed[:,:,1] = tag_size_pixels - detected_features_image_transformed[:,:,1]\n\n\n\n\n\ntag = load_tag(tag_number, tag_size_pixels)\ndetected_features_tag = detect_features(tag, max_num_features=200, threshold=0.1, min_distance=50)\n\nNow let’s visualise the extracted key points. I’m plotting the transformed points from the image in red and the features from the tag in blue.\n\nfor feature in detected_features_image_transformed:\n    x,y = feature.ravel()\n    plt.scatter(x,y,color='r',alpha=0.5, s=1_000)\n    \nfor feature in detected_features_tag:\n    x,y = feature.ravel()\n    plt.scatter(x,y,color='b',alpha=0.5, s=1_000)\n\nplt.imshow(tag,cmap='gray')\nplt.show()\n\n\n\n\n\ndetected_features_image_transformed[:,:,1] = tag_size_pixels - detected_features_image_transformed[:,:,1]\ndetected_features_tag[:,:,1] = tag_size_pixels - detected_features_tag[:,:,1]\n\n\n\n\n\ndef match_features(detected_features_image_transformed, detected_features_tag):\n    X = detected_features_image_transformed.squeeze()  # 10 points in 3 dimensions\n    tree = KDTree(X, leaf_size=2)       \n\n    detected_features_tag = detected_features_tag.squeeze()\n\n    points_3d = []\n    points_2d = []\n\n    for i in range(detected_features_tag.shape[0]):\n        dist, ind = tree.query(detected_features_tag[i].reshape((1,-1)), k=1)\n        points_3d.append(detected_features_tag[i])\n        points_2d.append(detected_features_image[int(ind)])\n        \n    points_3d = np.array(points_3d)\n    points_2d = np.array(points_2d)\n    points_2d = points_2d.squeeze()\n    \n    points_3d /= tag_size_pixels\n\n    return(points_2d, points_3d)\n\n\npoints_2d_interior, points_3d_interior =  match_features(detected_features_image_transformed, detected_features_tag)\n\npoints_3d_exterior = np.array([[0.0,1.0],\n                              [1.0,1.0],\n                              [1.0,0.0],\n                              [0.0,0.0]])\n\npoints_2d_exterior = aruco_tag_corners[tag_index,:,:].squeeze()\n\npoints_3d = np.vstack([points_3d_exterior, points_3d_interior])\npoints_2d = np.vstack([points_2d_exterior, points_2d_interior])\n\npoints_3d = np.hstack([points_3d, np.zeros((points_3d.shape[0],1))])\n\nplt.scatter(points_3d[:,0], points_3d[:,1])\nplt.show()\n\n\n\n\n\n\n\nInstead of using all of the points, let’s use RANSAC, an algorithm that can pick good inliers.\n\nret, rvec, tvec, inliers = cv2.solvePnPRansac(objectPoints = points_3d,\n                                              imagePoints = points_2d,\n                                              cameraMatrix = K,\n                                              distCoeffs = dist,\n                                              reprojectionError = 5**2,\n                                              iterationsCount=1_000)\n\nNow let’s find the camera centre with respect to the marker.\n\nr = Rot.from_rotvec(rvec[:,0])\nR = r.as_matrix()\nt = np.array(tvec)\nC = -np.dot(R.T,tvec)\nprint(C)\n\n[[ 0.26481151]\n [-1.9600099 ]\n [ 4.03681725]]\n\n\nNow, let’s visualise the coordinates axes to do a quick sanity check.\n\naxis = np.float32([[1,0,0], [0,1,0], [0,0,1]]).reshape(-1,3)\nimgpts, jac = cv2.projectPoints(axis, rvec, tvec, K, dist)\nimgpts = np.squeeze(imgpts)\n\n[cx,cy] = points_2d_exterior[3]\n\nplt.imshow(img[:,:,::-1])\nplt.plot([cx,imgpts[0,0]],[cy, imgpts[0,1]], color='r', alpha=0.75, linewidth=5, label='X Axis')\nplt.plot([cx,imgpts[1,0]],[cy, imgpts[1,1]], color='g', alpha=0.75, linewidth=5, label='Y Axis')\nplt.plot([cx,imgpts[2,0]],[cy, imgpts[2,1]], color='b', alpha=0.75, linewidth=5, label='Z Axis')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html",
    "title": "Modelling Afterpay’s customer growth",
    "section": "",
    "text": "Founded in 2014, Afterpay is a wildly successful Australian fintech startup. With a market capitalisation of circa 14 billion USD (At the time of writing), it has rapidly grown to one of Australia’s largest companies. Afterpay lets customers pay for products in 4 separate payments and charges stores a margin for this.\nI’m interested in applying Bayesian analysis to understand more about Afterpay, based on the information it has provided to the market, plus a little common sense.\nWhy Bayesian analysis? After all, we could use least means squares to fit a curve.\nIn short, Bayesian analysis will give us a better idea of how wrong we could be.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot.\n\n\nNow, with that out of the way, let’s get started."
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#our-goal",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#our-goal",
    "title": "Modelling Afterpay’s customer growth",
    "section": "Our goal",
    "text": "Our goal\nWhile reading through Afterpay’s releases to the markets, I came across this chart, which appears on page 3 of this release.\n\n\n\nImage of Afterpay customer growth\n\n\nGiven this graph, you might try to answer some questions:\n\nWhat is the “saturation level” of Afterpay in the Australian market?\nHow long will it take to get there?\nWhen will it be growing the fastest?\nHow confident can you be in your answer?\n\nBayesian analysis can help us answer these questions."
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#the-model",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#the-model",
    "title": "Modelling Afterpay’s customer growth",
    "section": "The Model",
    "text": "The Model\nFirst off, let’s load the libraries we will later need.\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom io import StringIO\nimport pandas as pd\n\nimport math\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\ncsv_data = StringIO('''millions_customers,date,month_count\n            0.0,30-sep-2015,0 \n            0.0,31-dec-2015,3 \n            0.0,31-mar-2016,6 \n            0.1,30-jun-2016,9 \n            0.2,30-sep-2016,12\n            0.4,31-dec-2016,15\n            0.6,31-mar-2017,18\n            0.8,30-jun-2017,21\n            1.1,30-sep-2017,24\n            1.5,31-dec-2017,27''')\n\n\ndf = pd.read_csv(csv_data, sep=\",\")\nplt.plot(df.month_count,df.millions_customers,color='r')\nplt.ylabel('Millions of customers')\nplt.xlabel('Months after launch')\nplt.grid()\nplt.show()\n\n\n\n\nWe can see a rapid, accelerating growth in the number of customers over time.\nOne model we could use is the sigmoidal model. Also known as “The S shaped curve”, it’s a model where growth starts slow, accelerates, before slowing again. It’s often used in technology adoption or the introduction of a new product.\n\\[\\frac{1}{1+e^{-x}}\\]\n\nx = np.arange(-10,10,0.01)\ny = 1/(1+math.e**-x)\nplt.plot(x,y,color='k')\nplt.grid()\nplt.show()\n\n\n\n\nWe can modify the model to modify the scale (L), how fast it grows (k), or when the fastest growth occurs (x0).\n\\[\\frac{L}{1+e^{-k(x-x_0)}}\\]\nWe can fit this model to the data provided by Afterpay using PyMC3.\nWe can also utilise priors to inject things we might know or suspect.\nFor example, while I am open-minded about what proportion of Australians may choose to become customers of Afterpay, I limit the model to a maximum of 25M customers (population of Australia). I’m using a Uniform distribution to emphasise that I have no firm belief about the final number of customers.\nI’m also using uniform priors for \\(k\\), the growth rate, and \\(x_0\\), the time in months at which growth is the fastest.\nIn general, this isn’t great practice because I explicitly exclude the possibility that there could be more than 25M customers, and no amount of data will be able to change my mind.\nTo quote Cromwell : I beseech you, in the bowels of Christ, think it possible that you may be mistaken.\nLet’s compose this as a Bayesian Regression problem.\n\nwith pm.Model() as model:\n    millions_customers = df.millions_customers.values\n    x = df.month_count.values.astype(np.float64)\n    L =  pm.Uniform('L', lower = 0, upper = 25) \n    k =  pm.Uniform('k', lower=0, upper=1) \n    x0 = pm.Uniform('x0', lower=0, upper=100)\n    \n    customers_predicted = L/(1+math.e**(-k*(x-x0)))\n    customers = pm.Normal('customers', mu = customers_predicted, sigma = 0.1, observed = millions_customers)\n    \n\n\nwith model:\n    trace = pm.sample(draws=10_000,tune=5_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x0, k, L]\nSampling 2 chains, 0 divergences: 100%|██████████| 30000/30000 [01:15<00:00, 397.13draws/s]\nThe number of effective samples is smaller than 25% for some parameters."
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#the-results",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#the-results",
    "title": "Modelling Afterpay’s customer growth",
    "section": "The Results",
    "text": "The Results\nFrom the following plots, we can notice several things. Firstly, and most importantly, our model converged well. Secondly, according to our model, we expect the number of customers to saturate below 5m people. Still, we might have a lot more customers, even 25m.\nFinally, there is a lot of uncertainty around the timing of the peak of the growth. This ranges from approximately two to four years after the introduction of Afterpay.\n\npm.traceplot(trace);\n\n\n\n\n\npm.plot_posterior(trace);\n\n\n\n\n\npm.summary(trace)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      L\n      9.545\n      7.107\n      1.393\n      22.628\n      0.144\n      0.102\n      2441.0\n      2441.0\n      2226.0\n      3639.0\n      1.0\n    \n    \n      k\n      0.154\n      0.037\n      0.105\n      0.227\n      0.001\n      0.001\n      2431.0\n      2359.0\n      2876.0\n      2962.0\n      1.0\n    \n    \n      x0\n      35.685\n      8.967\n      20.444\n      48.898\n      0.185\n      0.131\n      2339.0\n      2339.0\n      2363.0\n      3996.0\n      1.0"
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#posterior-predictive-checks",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#posterior-predictive-checks",
    "title": "Modelling Afterpay’s customer growth",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\nLet’s see if our model makes sense. By drawing samples from our model, we can further investigate the potential growth rates of Afterpay.\n\nfor i in range(0,1_000):\n    x = np.arange(0,30)\n    plt.plot(x,trace['L'][i]/(1+math.e**(-trace['k'][i]*(x-trace['x0'][i]))),color='k',alpha=0.01)\nplt.plot(df.month_count,df.millions_customers,color='r',label='Reported Customers')\n\nplt.legend()\nplt.ylabel('Millions of customers')\nplt.xlabel('Months after launch')\nplt.show()\n\n\n\n\nNow let’s start forecasting the future. We can compare what we modelled to the actual customer numbers (3.1M) reported by Afterpay 54 months after they launched.\n\nfor i in range(0,10_000):\n    x = np.arange(0,51)\n    plt.plot(x,trace['L'][i]/(1+math.e**(-trace['k'][i]*(x-trace['x0'][i]))),color='k',alpha=0.01)\nplt.plot(df.month_count,df.millions_customers,color='r',label='Reported Customers')\n\n\nplt.scatter(51,3.1,label='Most recent report')\nplt.legend()\n\nplt.ylabel('Millions of customers')\nplt.xlabel('Months after launch')\nplt.show()\n\n\n\n\nWe can also generate a histogram and compare the predictions of our model with the actual true number of 3.1M.\n\nx = 51*np.ones(20000)\ny = trace['L']/(1+math.e**(-trace['k']*(x-trace['x0'])))                              \nplt.hist(y,bins=np.arange(0,25,0.5),alpha=0.5,density=True)\nplt.vlines(3.1,0,0.175,label='Most recent report')\n        \nplt.xlabel('Millions of customers')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#conclusion",
    "href": "posts/2020-08-13-modelling-afterpay's-customer-growth.html#conclusion",
    "title": "Modelling Afterpay’s customer growth",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, Bayesian analysis is a powerful tool because it allows us to understand how confident we are in our predictions. It’s also powerful because we can feed in the information we might already believe. If you are 80% sure that at most 20% of the Australian population will use Afterpay, you can feed this in as a prior.\nThis type of analysis is also essential, as the AFR makes clear, Afterpay’s fate is intricately linked to their customer growth.\nWhile this is a relatively rough and ready model, It’s interesting. We can see considerable uncertainty in the final number of customers we expect Afterpay to have.\nWe also must remember that we picked an inappropriate model to fit, which might not fit reality appropriately."
  },
  {
    "objectID": "posts/2020-04-05-inverse-radial-distortion.pynb.html",
    "href": "posts/2020-04-05-inverse-radial-distortion.pynb.html",
    "title": "Inverse Radial Distortion",
    "section": "",
    "text": "Real cameras with real optics suffer from radial distortion. Radial distortion is a common type of lens distortion. It’s a deviation away from a perfect pinhole camera, where straight lines in 3D space are mapped to straight lines in 2D space. Let’s define this distortion mathematically."
  },
  {
    "objectID": "posts/2020-04-05-inverse-radial-distortion.pynb.html#defining-a-forward-function",
    "href": "posts/2020-04-05-inverse-radial-distortion.pynb.html#defining-a-forward-function",
    "title": "Inverse Radial Distortion",
    "section": "Defining a forward function",
    "text": "Defining a forward function\nRadial distortion functions map points according to their distance (r) from the optical centre of the lens.\nWe can model the function using a Taylor series:\n\\(\\begin{equation*} x_d = x(1 + k_1  r + k_2  r^2 + k_3 r^3) \\end{equation*}\\)\n\\(\\begin{equation*} y_d = y(1 + k_1  r + k_2 r^2 + k_3 r^3) \\end{equation*}\\)\nI.e., the function is described using three numbers, k_1, k_2 and k_3.\nFirstly, let’s import some things we need.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import least_squares\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (10,10)\n\nNext, let’s use some distortion coefficients from an actual lens and plot how it maps points based on their radius from the optical centre of the lens.\n\nk_1 = -0.04436\nk_2 = -0.35894\nk_3 =  0.14944\n\nr = np.linspace(0,1,1000)\nr_distorted = r*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3)\n\nplt.xlabel('Initial R')\nplt.ylabel('Distorted R')\nplt.plot(r,r_distorted)\nplt.show()\n\n\n\n\nTo help understand this mapping, we can visualise the impact on a grid of straight lines. Note how straight lines are mapped to curves.\n\ndef distort_line(x,y,k_1,k_2,k_3):\n    r = np.sqrt(x**2 + y**2)\n    x_distorted = x*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3)\n    y_distorted = y*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3)\n    return(x_distorted,y_distorted)\n\nfor y in np.linspace(-1,1,10):\n    x = np.linspace(-1,1,1000)\n    x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3)\n    plt.plot(x_distorted,y_distorted,color='k',alpha=0.8)\n    \nfor x in np.linspace(-1,1,10):\n    y = np.linspace(-1,1,1000)\n    x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3)\n    plt.plot(x_distorted,y_distorted,color='k',alpha=0.8)\n    \nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.show()"
  },
  {
    "objectID": "posts/2020-04-05-inverse-radial-distortion.pynb.html#finding-an-inverse-function",
    "href": "posts/2020-04-05-inverse-radial-distortion.pynb.html#finding-an-inverse-function",
    "title": "Inverse Radial Distortion",
    "section": "Finding an Inverse Function",
    "text": "Finding an Inverse Function\nNow it’s time to find an inverse function, a function that will allow us to take points that have been distorted, and map them back to where they would have been, had the lens been free from distortion.\nUnfortunately, we can’t find an inverse function based on the forward function algebraically. However, we can find an inverse function through a process of optimization.\n\ndef undistort_point(undistortion_params,r_distorted):\n    undistorted = r_distorted*(1 + undistortion_params[0] * r_distorted\n                               + undistortion_params[1] * r_distorted**2\n                               + undistortion_params[2] * r_distorted**3\n                               + undistortion_params[3] * r_distorted**4\n                               + undistortion_params[4] * r_distorted**5)\n    return(undistorted)\n\ndef fun(undistortion_params,r_distorted):\n    #Compute residuals.\n    undistorted = undistort_point(undistortion_params, r_distorted)\n    return((undistorted - np.linspace(0,1,1000))).ravel()\n\n\nx0 = np.zeros(5).ravel()\nres = least_squares(fun, x0,  verbose=2, ftol=1e-12,loss='linear', args=([r_distorted]))\n\n   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n       0              1         5.6524e+00                                    3.02e+01    \n       1              2         2.3481e-07      5.65e+00       9.82e-01       1.88e-08    \n       2              3         2.3481e-07      1.62e-17       1.58e-06       1.04e-11    \n`gtol` termination condition is satisfied.\nFunction evaluations 3, initial cost 5.6524e+00, final cost 2.3481e-07, first-order optimality 1.04e-11.\n\n\nThe optimisation process tries to find a set of coefficients that allow us to map the output from the distortion function back to its input.\n\nundistorted = undistort_point(res.x,r_distorted)    \nplt.plot(r_distorted,label='distorted',alpha=0.5)\nplt.plot(undistorted,label='un distorted',alpha=0.5)\nplt.plot(np.linspace(0,1,1000),label='target',alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\nprint(res.x)\n\n[ 0.04599498  0.32120247  0.22196835 -0.46283148  0.77191211]\n\n\nVoilà, we have found the coefficients of a Taylor series that allow us to invert the distortion function."
  },
  {
    "objectID": "posts/2020-07-02-on-target-with-pymc3.html",
    "href": "posts/2020-07-02-on-target-with-pymc3.html",
    "title": "On Target With PyMC3",
    "section": "",
    "text": "Last weekend I was relaxing in the countryside at a colleague’s château, and we decided to take turns shooting at a target with an air rifle.\nUnfortunately, we were all terrible shots, and the rifle was uncalibrated, so we could have done a better job of hitting the target.\nTo improve our aim, we need to adjust the rifle sights so that the pellet impacts are centred on the middle of the target."
  },
  {
    "objectID": "posts/2020-07-02-on-target-with-pymc3.html#the-model",
    "href": "posts/2020-07-02-on-target-with-pymc3.html#the-model",
    "title": "On Target With PyMC3",
    "section": "The model",
    "text": "The model\nLet’s start modelling this situation in Python.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc3 as pm\nfrom PIL import Image\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = [20,20]\nnp.random.seed(1337)\n\nLet’s pretend our rifle is incorrectly sighted/calibrated so that when we aim at the target’s centre, our pellets will land 120 units to the right and 80 pixels down.\nLet’s also pretend that they will be normally distributed around this point, with a standard deviation of 85 pixels in each axis due to the wind/humidity/our terrible aim.\nThis is our generative model, and we use it to generate some fake data. We will later use the same model construct and try and infer the x_offset and y_offset.\nLet’s go ahead and randomly generate the results of 5 shots at the target.\n\nx_offset = 120 #units/pixels\ny_offset = -80 #units/pixels\nstandard_deviation = 85 #units/pixels\nnum_samples = 5 \nx_observed = np.random.normal(x_offset,standard_deviation,5)\ny_observed = np.random.normal(y_offset,standard_deviation,5)\n\n\nimg = Image.open('data/2020-07-02-On-Target-With-PyMC3/1000px-10_m_Air_Rifle_target.svg.png')\n\nplt.imshow(img)\nplt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000)\n\nplt.grid()\nplt.show()\n\n\n\n\nNow, given we have observed this target. What adjustments should we make to improve our aim?\nAs part of Bayesian analysis, we need to provide a prior distribution, which tells us what plausible values of x_offset and y_offset could be.\nBased on the impact locations, PyMC3 will try to infer potential values x_offset and y_offset, which are the adjustments we need to make to our rifle.\nThe beauty of Bayesian analysis is that we don’t get a single value but a distribution of values. This allows us to understand how certain we can be about the results.\n\nwith pm.Model() as model: \n    #Set up our model\n    x_offset = pm.Normal('x_offset',mu = 0, sigma=250)\n    y_offset = pm.Normal('y_offset',mu = 0, sigma=250)\n    standard_deviation = pm.HalfNormal('standard_deviation',sigma=200)    \n    \n    impact_x = pm.Normal('impact_x', mu = x_offset, sigma = standard_deviation, observed = x_observed)\n    impact_y = pm.Normal('impact_y', mu = y_offset, sigma = standard_deviation, observed = y_observed)\n    \n    \n\nNow that we have finished setting up our model, we can use Markov Chain Monte-Carlo (MCMC) to infer what x_offset and y_offset could be.\n\nwith pm.Model() as model: \n    #The magic line that \n    trace = pm.sample(draws=10_000, tune=1_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [standard_deviation, y_offset, x_offset]\nSampling 2 chains, 1 divergences: 100%|██████████| 22000/22000 [00:09<00:00, 2309.14draws/s]\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize."
  },
  {
    "objectID": "posts/2020-07-02-on-target-with-pymc3.html#results",
    "href": "posts/2020-07-02-on-target-with-pymc3.html#results",
    "title": "On Target With PyMC3",
    "section": "Results",
    "text": "Results\nNow it’s time to look at the results; PyMC3 provides several options for understanding them.\nLet’s start with a numerical summary. From this, we can see the mean value of x_offset is 65.6 and y_offset is -109.7. Our best guess of where we need to aim is 65.6 units to the left and -109.7 units up.\n\npm.summary(trace)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x_offset\n      65.645\n      50.662\n      -36.013\n      156.687\n      0.420\n      0.327\n      14566.0\n      12024.0\n      15361.0\n      10949.0\n      1.0\n    \n    \n      y_offset\n      -109.744\n      50.016\n      -203.327\n      -13.457\n      0.475\n      0.336\n      11107.0\n      11107.0\n      11512.0\n      9532.0\n      1.0\n    \n    \n      standard_deviation\n      109.028\n      31.722\n      59.562\n      165.361\n      0.324\n      0.235\n      9578.0\n      9085.0\n      10639.0\n      11167.0\n      1.0\n    \n  \n\n\n\n\nA trace plot is a helpful way to diagnose what is going on. I’m still looking for an excellent tutorial on interpreting it, but here are a few key points I’ve picked up.\n\nLeft-Hand Column: Curves should closely overlap. If they don’t, then it means that you can’t rely on the results.\nRight-Hand Column: The chart should look like a “Fuzzy Caterpillar”. This means that you are effectively exploring the parameter space.\n\n\npm.traceplot(trace);\n\n\n\n\nThe posterior plot tells you what values your parameters are likely to have. For example, according to our model, there is a 94% chance that x_offset is between -36 and 157. \n\npm.plot_posterior(trace);\n\n\n\n\nFinally, because we have two variables, we can plot them together and understand their joint distribution using Seaborn.\n\nplot = sns.jointplot(trace[:]['x_offset']+500, trace[:]['y_offset']+500, kind=\"hex\");\nplt.show()\n\n\n\n\nPutting it all together, we can visualise the potential locations (in red) for the centroid of the actual location of where the pellets will land. With only five samples observed, there is a considerable amount of uncertainty. As the number of samples increases, this uncertainty decreases.\nWe can also see that the green point, which marks the actual offset, is within this distribution of potential red locations.\n\nplt.imshow(img)\nplt.scatter(trace[:]['x_offset']+500,500-trace[:]['y_offset'],alpha=0.5,s = 1,color='r')\nplt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000)\nplt.scatter(120+500,500--80,alpha=0.9,s = 1000,color='g')\nplt.grid()\nplt.show()\n\nNo handles with labels found to put in legend."
  },
  {
    "objectID": "posts/2021-02-24-positioning-using-aruco-tags.html",
    "href": "posts/2021-02-24-positioning-using-aruco-tags.html",
    "title": "Positioning using ArUco Tags",
    "section": "",
    "text": "In a previous post, I looked at how we could integrate synthetic objects into real-world imagery by determining the position and orientation of the camera.\nUsing the OpenCV solvePnP function, we can find the position and orientation of the “chessboard” with respect to the camera. I used a “chessboard” type pattern in the post and used the detected corners to orient the camera. This is because the detected 2d points in the image have corresponding points in 3d space.\nIn practice, an ArUco Tag, a type of fiducial/target, is often used. ArUco tags come in sets called dictionaries; the image below is tag number 0, out of a group of 50. When we detect a tag, we can decode its number.\n\n\n\nAn ArUco tag\n\n\nLet’s walk through how to detect tags and then determine a camera’s position and orientation with respect to them in OpenCV."
  },
  {
    "objectID": "posts/2021-02-24-positioning-using-aruco-tags.html#detecting-tags",
    "href": "posts/2021-02-24-positioning-using-aruco-tags.html#detecting-tags",
    "title": "Positioning using ArUco Tags",
    "section": "Detecting Tags",
    "text": "Detecting Tags\nAs always, let’s start by importing what we need.\n\nimport numpy as np\nimport cv2\nimport cv2.aruco as aruco\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as Rot\n\nplt.rcParams['figure.figsize'] = [20,20]\n\nI’ve already calibrated the camera, which means that we have both the camera matrix (K) and the distortion parameters for the camera.\n\nK = np.array([[3286.9, 0.0 , 2032.8],\n              [0.0   , 3282.3, 1519.1],\n              [0.0   , 0.0   ,    1.0]])\n\ndist = np.array([[2.44663616e-01],[-1.48023303e+00],[2.04607109e-03],[1.53484564e-03],[2.56349651e+00]])\n\nNow we need to configure our ArUco code detector.\nFirstly, we configure it to look for a “4X4_50” type tag.\n\narucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n\nSecondly, we tell it that we want to configure the detector; in particular, we want to find the corners to a very high degree of accuracy (Sub-Pixel).\n\narucoParams = cv2.aruco.DetectorParameters_create()\narucoParams.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX\narucoParams.cornerRefinementWinSize = 25\n\nNow we can: * Load an image * Convert to grayscale * Detect the tags\n\nfname = 'data/2021-02-24-Positioning-Using-ArUco-Tags/ArUco.jpg'\nimg = cv2.imread(fname)\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n\n(detected_tags, tag_ids, rejected) = cv2.aruco.detectMarkers(gray, arucoDict, parameters=arucoParams)\n\nNow that we have detected the tags, let’s visualise them:\n\ndef plot_squares(detected_tags, tag_ids):\n    plt.imshow(img[:,:,::-1])\n    for i in range(0,len(detected_tags)):\n        square = detected_tags[i]\n        square = square.squeeze()\n        x = []\n        y = []\n        for j in range(0,5):\n            v1 = square[j%4]\n            x.append(v1[0])\n            y.append(v1[1])\n                     \n        plt.plot(x, y, label='tag: ' + str(tag_ids[i]), linewidth=5)\n    plt.legend()\n    plt.show()\n\nplot_squares(detected_tags, tag_ids)"
  },
  {
    "objectID": "posts/2021-02-24-positioning-using-aruco-tags.html#finding-the-tag-position-and-orientation",
    "href": "posts/2021-02-24-positioning-using-aruco-tags.html#finding-the-tag-position-and-orientation",
    "title": "Positioning using ArUco Tags",
    "section": "Finding the tag position and orientation",
    "text": "Finding the tag position and orientation\nNow that we have detected the tags, we can try to compute the position and orientation of the tag with respect to the camera.\nIn practice, if we can find the position and orientation of the tag with respect to the camera, we can conversely find the position and orientation of the camera with respect to the tag.\nWhen cv2.aruco.detectMarkers detects a tag; it gives us the coordinates of the tag, starting from the top left and proceeding in a counterclockwise direction around the tag.\nIf we use the tag as the origin for a right-handed - coordinate system and place the origin at the lower-left corner of the tag, we find the 3D coordinates of the corners of the tags are as follows.\n\npoints_3d = np.array([[0.0,1.0,0.0],\n                      [1.0,1.0,0.0],\n                      [1.0,0.0,0.0],\n                      [0.0,0.0,0.0]])\n\nWith the 2d coordinates and corresponding 3d coordinates for the corners of each tag, we can find the relationship between the camera and tag.\nThe can be computed using cv2.solvePnP, which will give us the rvec and tvec, which describes the position and orientation of the tag according to the coordinate system of the camera.\n\nIn this case rvec is in Rodrigues format.\nFinally, let’s visualize the coordinate system defined by each ArUco tag.\n\nfor i in range(len(detected_tags)):\n    points_2d = detected_tags[i].squeeze()\n\n    ret, rvec, tvec = cv2.solvePnP(objectPoints = points_3d, imagePoints = points_2d, cameraMatrix = K, distCoeffs = dist)#, flags = cv2.SOLVEPNP_IPPE_SQUARE)\n    \n    axis = np.float32([[1,0,0], [0,1,0], [0,0,1]]).reshape(-1,3)\n    imgpts, jac = cv2.projectPoints(axis, rvec, tvec, K, dist)\n    imgpts = np.squeeze(imgpts)\n    cx, cy = points_2d[3,0], points_2d[3,1]\n    plt.imshow(img[:,:,::-1])\n\n    if i ==0:\n        plt.plot([cx,imgpts[0,0]],[cy, imgpts[0,1]], color='r', alpha=0.75, linewidth=5, label='X Axis')\n        plt.plot([cx,imgpts[1,0]],[cy, imgpts[1,1]], color='g', alpha=0.75, linewidth=5, label='Y Axis')\n        plt.plot([cx,imgpts[2,0]],[cy, imgpts[2,1]], color='b', alpha=0.75, linewidth=5, label='Z Axis')\n    else:\n        plt.plot([cx,imgpts[0,0]],[cy, imgpts[0,1]], color='r', alpha=0.75, linewidth=5)\n        plt.plot([cx,imgpts[1,0]],[cy, imgpts[1,1]], color='g', alpha=0.75, linewidth=5)\n        plt.plot([cx,imgpts[2,0]],[cy, imgpts[2,1]], color='b', alpha=0.75, linewidth=5)\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2021-02-24-positioning-using-aruco-tags.html#finding-the-camera-position-and-orientation",
    "href": "posts/2021-02-24-positioning-using-aruco-tags.html#finding-the-camera-position-and-orientation",
    "title": "Positioning using ArUco Tags",
    "section": "Finding the camera position and orientation",
    "text": "Finding the camera position and orientation\nFor completeness, we can find the location and orientation of the camera with respect to one of the tags.\nIn this case, it will be the first tag that we detected, tag number 27 in the low left of the image.\n\ntag_index = 0\npoints_2d = detected_tags[tag_index].squeeze()\n\nret, rvec, tvec = cv2.solvePnP(objectPoints = points_3d, imagePoints = points_2d, cameraMatrix = K, distCoeffs = dist)#, flags = cv2.SOLVEPNP_IPPE_SQUARE)\n\nNow we can find the camera position Camera_C, and its orientation Camera_R (in Quaternions), with respect to the coordinate system defined by the tag.\n\nr = Rot.from_rotvec(rvec[:,0])\nR = r.as_matrix()\nt = np.array(tvec)\n\nCamera_C = -np.dot(R.T,tvec)\nprint(Camera_C)\n\nCamera_R = Rot.from_matrix(R.T).as_quat()\nprint(Camera_R)\n\n[[ 0.27833657]\n [-1.92925962]\n [ 4.07499752]]\n[ 0.9225515  -0.20544298  0.09037747 -0.31388505]"
  },
  {
    "objectID": "posts/2021-02-24-positioning-using-aruco-tags.html#conclusion",
    "href": "posts/2021-02-24-positioning-using-aruco-tags.html#conclusion",
    "title": "Positioning using ArUco Tags",
    "section": "Conclusion",
    "text": "Conclusion\nFor me, ArUco tags offer many exciting possibilities beyond Augmented Reality.\nIn particular, they can be applied during photogrammetric surveying projects to provide quick estimates of camera positions and orientations.\nThey also allow us to inject scale estimates into a photogrammetric reconstruction, given their known real-world size."
  },
  {
    "objectID": "posts/2021-05-02-covid-conjugate-priors.html",
    "href": "posts/2021-05-02-covid-conjugate-priors.html",
    "title": "COVID Conjugate Priors",
    "section": "",
    "text": "Australia has adopted an isolationist policy in response to the global coronavirus pandemic. Upon arriving in Australia, passengers are isolated in designated hotels for at least 14 days. While this process is highly effective, leaks can and do occur. Fortunately, these are regularly brought under control with effective contact tracing.\nI had previously written a post applying MCMC to infer & estimate the effectiveness of different quarantine programmes. In this post, I’m going to build upon the fantastic work done by Antony Macali, who has diligently catalogued information about leaks for hotel quarantine. You can find the information he’s collected here.\nWith this information, we can chart how effective you would have believed the quarantine system of any state was over time. It’s the belief you would have had, based on the number of people successfully processed and the number of failures that occurred.\nAs more infected people are successfully processed, the estimated risk falls; conversely, if there is a breach, the estimated risk immediately rises.\nTo be clear, it’s not a chart of how effective the system was at any point in time, just our underlying belief, based on the data we have observed, up until that point in time."
  },
  {
    "objectID": "posts/2021-05-02-covid-conjugate-priors.html#the-model",
    "href": "posts/2021-05-02-covid-conjugate-priors.html#the-model",
    "title": "COVID Conjugate Priors",
    "section": "The Model",
    "text": "The Model\nWe can build a simple but effective model by treating the number of leaks as a binomial variable.\nWe can then infer the probability each processed person results in a leak based on the number of people processed and the number of leaks observed.\nWe need to provide a prior estimate for the probability of processing anyone infected, resulting in a leak. A sensible estimate is that 1% of them will result in a leak. To encode this prior, I will use a beta distribution.\nPreviously I used Markov chain Monte Carlo (MCMC) to find the posterior distribution; however, in this case, we can sidestep MCMC by taking advantage of the fact that the beta distribution is a conjugate prior of the binomial distribution. This means that we can algebraically find the posterior without resorting to MCMC.\nLet’s start by importing some useful libraries.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as st\n\nplt.style.use('seaborn-darkgrid')\nplt.rcParams[\"figure.figsize\"] = (15,15)\n\nLet’s load the dataset for each state from a CSV into a Pandas data frame.\n\ndef load_data(state):\n    data_df = pd.read_csv(f'data/2021-05-02-Conjugate-Priors/COVID Live HQ Breaches - {state} 7d.csv')\n    data_df['BREACH'] = data_df['BREACH'] > 0\n    data_df = data_df.rename(columns={\"DATE\": \"date\", \"BREACH\": \"breach\", \"OSEAS 7d AV\": \"avg_infections\"})\n    return(data_df)\n\nThis is where the magic happens.  \nLooking at a slice of our dataset,\n\nWe define a prior probability of 1 (a) breach per 100 (b) infected people processed.\nWe find the number of infected people (N) processed by the system.\nWe find the count of the breaches (count).\nWe find the posterior probability as beta(a+count, int(b+N-count))\n\n\ndef calculate_posterior(index, a = 1, b = 100, window_size_days = 360):\n    N = np.sum(data_df.avg_infections[max([0,index - window_size_days]):index])\n    count = np.sum(data_df.breach[max([0,index - window_size_days]):index])\n    mean =  st.beta.mean(a+count, int(b+N-count))\n    interval = st.beta.interval(0.80, a+count, int(b+N-count))\n    return(mean, interval)\n\nNow it’s just a matter of visualizing the results for different states.\n\nstates = ['NSW','VIC','NT','QLD','WA', 'SA']\nfor target_state in states:\n    fig, ax = plt.subplots()\n    fmt_half_year = mdates.MonthLocator(interval=1)\n    ax.xaxis.set_major_locator(fmt_half_year)\n    ax.grid(True)\n\n    for state in states:\n        data_df = load_data(state)\n        lowers = []\n        uppers = []\n        means = []\n        for index in range(0, data_df.date.shape[0]):\n            mean, interval = calculate_posterior(index)\n            lowers.append(100 * interval[0])\n            uppers.append(100 * interval[1])\n            means.append(100 * mean)\n\n        if state == target_state:\n            plt.plot(data_df.date, means, label = state, linewidth = 3)\n            plt.fill_between(data_df.date, lowers, uppers, label = '90% Credible Interval', alpha = 0.25)\n            \n        else:\n            plt.plot(data_df.date, means, alpha = 0.25, color = 'k')\n\n    plt.title(target_state)\n    plt.xlabel('Date')\n    plt.ylabel('Estimated probability of breach per infected person processed (%)')\n\n    plt.ylim(0,3.5)\n\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "posts/2021-05-02-covid-conjugate-priors.html#conclusion",
    "href": "posts/2021-05-02-covid-conjugate-priors.html#conclusion",
    "title": "COVID Conjugate Priors",
    "section": "Conclusion",
    "text": "Conclusion\nWe can now create a real-time estimate of the probability of a breach and understand how it evolves.\nAs more infected people are successfully processed, the estimated risk falls; conversely, if there is a breach, the estimated risk immediately rises.\nAn implicit assumption is that the underlying risk inherent in the system is static, which may not be accurate in practice as the underlying system evolves. In the case of Victoria, the troubled quarantine system has undergone two evolutions. Time and data will tell if it’s 3rd time lucky."
  },
  {
    "objectID": "posts/2020-08-14-how-does-afterpay-work.html",
    "href": "posts/2020-08-14-how-does-afterpay-work.html",
    "title": "How does Afterpay work?",
    "section": "",
    "text": "Afterpay’s business model is pretty simple. Afterpay allows customers to pay for products in 4 separate payments every two weeks and charges stores a margin for this. At a high level, Afterpays revenues are a function of how many customers they have and how often they use Afterpay.\nSometimes things don’t go to plan, and customers default on their payments, and this is where things get a bit more complex.\nIt’s also an area that I feel, in my opinion, that Afterpay has been somewhat nebulous. Even if Afterpay doesn’t tell us directly, it might be possible to infer and gain more clarity about what we have not been directly told.\nHowever, I think there are many positive aspects to Afterpay’s business model compared to credit cards.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-08-14-how-does-afterpay-work.html#why-create-synthetic-data",
    "href": "posts/2020-08-14-how-does-afterpay-work.html#why-create-synthetic-data",
    "title": "How does Afterpay work?",
    "section": "Why create synthetic data?",
    "text": "Why create synthetic data?\nBefore we try and infer based on what Afterpay has told us, a great first step might be to build a model to generate synthetic data. IE, let’s simulate Afterpay, then generate some synthetic outcomes. From these outcomes, we can try and infer “hidden” parameters in our model. Because this is a simulation, we can see how well we can infer based on what we can observe.\nWhile this might seem like a contrived toy example, I think it’s a compelling first step. If we can’t effectively infer from our synthetic data, we can’t hope to infer anything about reality either."
  },
  {
    "objectID": "posts/2020-08-14-how-does-afterpay-work.html#the-model",
    "href": "posts/2020-08-14-how-does-afterpay-work.html#the-model",
    "title": "How does Afterpay work?",
    "section": "The model",
    "text": "The model\nOk, let’s keep things as simple as possible initially. We can always create a more sophisticated but wrong model later.\nOk. So let’s assume that Afterpay does some number of transactions per year. These transactions have an average value; let’s assume 100 AUD. For each transaction, they are paid some margin by the merchant. Let’s assume 4%.\nFor some of these transactions, let’s say 1% of the time, the customer defaults. This means that they make no repayments at all against the 100 AUD.  \nGlossing over the Afterpay fee structure, let’s assume they pay Afterpay 25 AUD in fees, and Afterpay is liable for the transaction value (100 AUD).\nLet’s now model this out in code.\n\ndef compute_revenue(average_transaction_value = 100, margin_percent = 0.04, number_of_transactions = 10**6, default_rate = 0.01, average_default_fee  = 25):\n    \n    gross_transaction_value = number_of_transactions * average_transaction_value\n    \n    default_losses = default_rate * number_of_transactions * average_transaction_value\n    \n    default_fee_rev = default_rate * number_of_transactions * average_default_fee \n    \n    gross_afterpay_rev = margin_percent * gross_transaction_value +  default_fee_rev - default_losses\n    \n    return(gross_afterpay_rev, default_fee_rev, gross_transaction_value, default_losses)\n\n\ncompute_revenue()\n\n(3250000.0, 250000.0, 100000000, 1000000.0)\n\n\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\nwith pm.Model() as model:\n    gross_afterpay_rev = 3_250_000.0\n    default_fee_rev =  250_000.0\n    gross_transaction_value = 100_000_000.0\n    default_fee = 25\n    margin_percent = 0.04\n    \n    \n    default_rate =  pm.Beta('loss_rate', alpha=1, beta=4) \n    \n    average_default_fee = pm.Uniform('average_default_fee',lower = 10, upper = 68)\n    \n    number_of_transactions = pm.Exponential('number_of_transactions', lam= 1.0/10**6)\n    \n    average_transaction_value = pm.Exponential('average_transaction_value', lam= 1.0/100)\n    \n    gross_transaction_value =  pm.Normal('gross_transaction_value', mu = number_of_transactions * average_transaction_value, sigma = 10_000_000 , observed = [100_000_000])\n    \n    default_losses = pm.Normal('default_losses', mu = default_rate * gross_transaction_value, sigma = 100_000, observed =  [1_000_000])\n    \n    default_fee_rev = pm.Normal('default_fee_rev', mu = default_rate * number_of_transactions * average_default_fee, sigma = 25_000, observed =  [250_000])\n    \n    gross_afterpay_rev = pm.Normal('gross_afterpay_rev', mu = margin_percent * gross_transaction_value +  default_fee_rev - default_losses, sigma = 325_000, observed = [3_250_000])\n    \n    \n\n\nwith model:\n    trace = pm.sample(draws=10_000,tune=5_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [average_transaction_value, number_of_transactions, average_default_fee, loss_rate]\nSampling 2 chains, 0 divergences: 100%|██████████| 30000/30000 [00:41<00:00, 716.68draws/s]\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\npm.traceplot(trace);\n\n\n\n\n\npm.plot_posterior(trace);\n\n\n\n\n\npm.summary(trace)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      loss_rate\n      0.010\n      0.001\n      0.008\n      0.012\n      0.000\n      0.000\n      7264.0\n      7264.0\n      7263.0\n      7344.0\n      1.0\n    \n    \n      average_default_fee\n      35.699\n      15.320\n      10.012\n      62.076\n      0.232\n      0.164\n      4345.0\n      4345.0\n      4213.0\n      6038.0\n      1.0\n    \n    \n      number_of_transactions\n      872262.401\n      451793.009\n      300926.348\n      1760136.553\n      7010.986\n      4957.864\n      4153.0\n      4153.0\n      4183.0\n      5987.0\n      1.0\n    \n    \n      average_transaction_value\n      140.647\n      62.953\n      41.637\n      255.076\n      0.949\n      0.671\n      4398.0\n      4398.0\n      4258.0\n      6067.0\n      1.0\n    \n  \n\n\n\n\n\npm.model_to_graphviz(model)"
  },
  {
    "objectID": "posts/2020-12-20-the-log-polar-transform.html",
    "href": "posts/2020-12-20-the-log-polar-transform.html",
    "title": "The Log Polar Transform",
    "section": "",
    "text": "As we saw in the last post, we can use FFT Phase Correlation to find the x & y translation between two images/arrays. However, what if there is a rotation between two images/arrays instead?\nThis is a problem when comparing two mini-maps where the player might have rotated between them.\nOne solution is the Log-Polar transformation, which transforms the image from x,y space to \\(\\rho\\),\\(\\theta\\) space. In \\(\\rho\\),\\(\\theta\\) space, scaling and rotation between the two images manifest as 2d translations. Once we have identified the rotation between the two images, we can correct it before finding the 2d translation.\nWhile understanding this process, I found this excellent explanation by Santosh Thoduka, which helped make things much clearer for me. Scikit-image also has a good demo, from which I learnt a lot."
  },
  {
    "objectID": "posts/2020-12-20-the-log-polar-transform.html#the-log-polar-transform",
    "href": "posts/2020-12-20-the-log-polar-transform.html#the-log-polar-transform",
    "title": "The Log Polar Transform",
    "section": "The Log Polar Transform",
    "text": "The Log Polar Transform\nThe log polar transform points from (x,y) to (\\(\\rho\\),\\(\\theta\\)) as follows:\n\\(\\rho = \\ln\\sqrt{x^2 + y^2}\\)\n\\(\\theta = atan2(y-y_c,x-x_c)\\)\nTo help visualise this process, let’s experiment with Mondarian’s Tableau I.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport skimage.io\nimport skimage.transform\n\nplt.rcParams['figure.figsize'] = [10, 10]\n\nFirst, let’s load and plot the image:\n\nmondarin = skimage.io.imread('data/2020-12-20-FFT-Phase-Correlation-Rotation/mondrian.jpg')\nmondarin = mondarin[0:1000,0:1000,:]\nshape = mondarin.shape\n\nplt.title('Original')\nplt.scatter(shape[0]/2,shape[1]/2,color='r',label = 'Image Center')\nplt.imshow(mondarin)\nplt.legend()\nplt.xlabel('x (pixels)')\nplt.ylabel('y (pixels)')\nplt.show()\n\n\n\n\nNow let’s compute the Log Polar Transform of the image and visualise that as well:\n\nmonarin_warped = skimage.transform.warp_polar(mondarin, radius=700, output_shape=shape, scaling='linear', order=0, multichannel=True)\n\nplt.title('Warped')\nplt.imshow(monarin_warped)\nplt.xlabel('Radius (pixels)')\nplt.ylabel('Theta')\nplt.show()\n\n\n\n\nNow let’s rotate the Mondrian to understand the power of the log-polar transform better.\n\nmondarin_rotated = skimage.transform.rotate(mondarin, 90)\n\nplt.title('Rotated')\nplt.scatter(shape[0]/2,shape[1]/2,color='r',label = 'Image Center')\nplt.imshow(mondarin_rotated)\nplt.legend()\nplt.xlabel('x (pixels)')\nplt.ylabel('y (pixels)')\nplt.show()\n\n\n\n\nBy comparing the original and the rotated Mondrian, we can see that a rotation in the original domain corresponds to a translation/shift in the log-polar domain.\n\nmonarin_rotated_warped = skimage.transform.warp_polar(mondarin_rotated, radius=700, output_shape=shape, scaling='linear', order=0, multichannel=True)\n\nplt.title('Side by Side comparison')\nplt.imshow(np.hstack([monarin_warped,monarin_rotated_warped]))\nplt.ylabel('Theta')\nplt.show()\n\n\n\n\nYou can read about the Log Polar Transform in more detail here."
  },
  {
    "objectID": "posts/2020-10-14-Robotic-Blender.html",
    "href": "posts/2020-10-14-Robotic-Blender.html",
    "title": "Robotic Blender",
    "section": "",
    "text": "Blender is a software application for open-source 3D scene creation and rendering."
  },
  {
    "objectID": "posts/2020-10-14-Robotic-Blender.html#why-would-we-want-to-automate-blender",
    "href": "posts/2020-10-14-Robotic-Blender.html#why-would-we-want-to-automate-blender",
    "title": "Robotic Blender",
    "section": "Why would we want to automate Blender?",
    "text": "Why would we want to automate Blender?\nI have many reasons, but my focus is on creating synthetic data to train new machine learning algorithms. I’m interested in doing more with less annotated data, and mixing actual data with synthetic data is an up-and-coming solution.\nUsing Blender, we can generate arbitrary amounts of synthetic data and precisely control the scene. Because of this, we can create metadata simultaneously and generate data that covers the available input space better.\nA classic example is the CLEVR dataset, used for Visual Reasoning."
  },
  {
    "objectID": "posts/2020-10-14-Robotic-Blender.html#how-can-we-automate-blender",
    "href": "posts/2020-10-14-Robotic-Blender.html#how-can-we-automate-blender",
    "title": "Robotic Blender",
    "section": "How can we automate Blender?",
    "text": "How can we automate Blender?\nBlender has a comprehensively documented API. However, I love using Blender’s scripting mode to experiment.\nIn short, we can create a python script, which we can run using Blender.\nblender --background --python myscript.py"
  },
  {
    "objectID": "posts/2020-10-14-Robotic-Blender.html#hello-world",
    "href": "posts/2020-10-14-Robotic-Blender.html#hello-world",
    "title": "Robotic Blender",
    "section": "Hello World",
    "text": "Hello World\nLet’s walk through what myscript.py could look like:\nimport os\nimport bpy\n\nObjects\nWhen Blender loads, the default scene contains a cube called Cube. Let’s adjust its position and scale.\ncube_scale = 0.5\nbpy.data.objects[\"Cube\"].scale = (cube_scale,cube_scale,cube_scale)\nbpy.data.objects[\"Cube\"].location = (0,0,cube_scale)\nWe can also create a ground plane and add that to the scene.\nbpy.ops.mesh.primitive_plane_add(size=1000, enter_editmode=False, align='WORLD', location=(0, 0, 0), scale=(1, 1, 1))\nOur next task is to create materials for the objects we have added to the scene.\n\n\nMaterial\ndef create_material(object_name,material_name, rgba):\n        mat = bpy.data.materials.new(name=material_name)\n        bpy.data.objects[object_name].active_material = mat\n        \n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n\n        nodes[\"Principled BSDF\"].inputs['Base Color'].default_value = rgba\n        nodes[\"Principled BSDF\"].inputs['Specular'].default_value = 1\n        nodes[\"Principled BSDF\"].inputs['Roughness'].default_value = 0.1\ncreate_material(\"Cube\",\"Cube_material\",(3/255.0, 223/255.0, 252/255.0,1))\ncreate_material(\"Plane\",\"Plane_material\",(252/255.0, 3/255.0, 235/255.0,1))\n\n\nLights\ndef configure_light():\n        bpy.data.objects[\"Light\"].data.type = 'AREA'\n        bpy.data.objects[\"Light\"].scale[0] = 10\n        bpy.data.objects[\"Light\"].scale[1] = 10\n\nconfigure_light()\n\n\nCamera\nNow let’s configure the camera’s position and orientation/attitude (Using quaternions).\ndef configure_camera():\n        bpy.data.objects[\"Camera\"].location = (5, -5, 4)\n        bpy.data.objects[\"Camera\"].rotation_mode = 'QUATERNION'\n        bpy.data.objects[\"Camera\"].rotation_quaternion = (0.892399, 0.369644, 0.099046, 0.239118_\n\nconfigure_camera()\n\n\nAction! (Renderer)\nFinally, let’s configure the renderer. I’ve chosen to use Cycles, a physically-based renderer/ray tracer.\ndef configure_render():\n        bpy.context.scene.render.engine = 'CYCLES'\n        bpy.context.scene.render.filepath = os.getcwd()+\"/render.png\"\n        bpy.context.scene.render.resolution_x = 1920\n        bpy.context.scene.render.resolution_y = 1080\n\nconfigure_render()\nAnd we can finish by rendering the image and writing it out as render.png.\nbpy.ops.render.render(write_still=True)"
  },
  {
    "objectID": "posts/2020-10-14-Robotic-Blender.html#the-results",
    "href": "posts/2020-10-14-Robotic-Blender.html#the-results",
    "title": "Robotic Blender",
    "section": "The Results",
    "text": "The Results\n\n\n\n_config.yml"
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-5.html",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-5.html",
    "title": "How often do Afterpay transactions attract late fees? (Part 5)",
    "section": "",
    "text": "Now that we have more data from Afterpayon the actual rate of purchases that attract late fees (±10%), I wanted to go back and compute the average late fees. This post is effectively an adaption of the work I did in this post.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#average-late-fees",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#average-late-fees",
    "title": "How often do Afterpay transactions attract late fees? (Part 5)",
    "section": "Average Late Fees",
    "text": "Average Late Fees\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport scipy.stats as st\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\nwith pm.Model() as average_late_fee_model:\n    underlying_sales_aud = pm.Uniform('underlying_sales_aud', lower=5.24715*10**9, upper=5.247249*10**9)\n    \n    late_fees_rev_aud = pm.Uniform('late_fees_rev', lower=46.05 * 10**6, upper=46.149 * 10**6)\n    \n    average_transaction_value_aud = pm.Uniform('average_transaction_value', lower=144.50, upper=154.49)\n    \n    late_payment_rate = pm.Uniform('late_payment_rate',lower=9.5,upper=10.5)/100.0\n    \n    number_of_transactions = pm.Deterministic('number_of_transactions', underlying_sales_aud / average_transaction_value_aud)\n    \n    late_transactions = pm.Deterministic('late_transactions', late_payment_rate * number_of_transactions)\n                                         \n    average_late_fee_aud = pm.Deterministic('average_late_fee_aud', late_fees_rev_aud / late_transactions)\n    \npm.model_to_graphviz(model)\n\nwith average_late_fee_model:\n    samples = pm.sample_prior_predictive(samples = 100_000)"
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#results",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#results",
    "title": "How often do Afterpay transactions attract late fees? (Part 5)",
    "section": "Results",
    "text": "Results\n\nlate_fee_samples = samples['average_late_fee_aud']\nplt.title('Histogram of late fees')\nplt.ylabel('Relative Frequency')\nplt.xlabel('Late fee (AUD)')\nsns.distplot(late_fee_samples,kde=False, norm_hist=True)\nplt.show()\n\n\n\n\n\npm.summary(samples[\"average_late_fee_aud\"])\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 100000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      13.146\n      0.458\n      12.313\n      13.982\n      0.001\n      0.001\n      99393.0\n      99393.0\n      99248.0\n      99302.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#conclusion",
    "href": "posts/2020-08-29-afterpay-customer-defaults-part-5.html#conclusion",
    "title": "How often do Afterpay transactions attract late fees? (Part 5)",
    "section": "Conclusion",
    "text": "Conclusion\nThe late fees that Afterpay customers pay are low, somewhere between 12 AUD and 14 AUD on average. This fits what we observed in previous posts, where customers were typically late on only one of the four payments."
  },
  {
    "objectID": "posts/2018-11-18-256-Shades-of-Grey.html",
    "href": "posts/2018-11-18-256-Shades-of-Grey.html",
    "title": "256 shades of grey",
    "section": "",
    "text": "The resulting Digital Elevation Map (DEM) is in the public domain and provides the measured terrain height at ~90-meter resolution. The mission mapped 99.98% of the area between 60 degrees North and 56 degrees South.\nThis post will examine how to process the raw DEM to be more intuitively interpreted through hill-shading, slope shading & hypsometric tinting.\nTransforming the raw GeoTIFF into the final imagery product is simple. GDAL, the Geospatial Data Abstraction Library, carries out much of the grunt work.\nIn order, we need to:\n\nDownload a DEM as a GeoTIFF\nExtract a subsection of the GeoTIFF\nReproject the subsection\nMake an image with hill shading\nMake an image by colouring the subsection according to altitude\nMake an image by colouring the subsection according to the slope\nCombine the three images into a final composite\n\n\nDEM\nSeveral different DEMs have been created from the data collected on the SRTM mission. I will use the CGIAR SRTM 90m Digital Elevation Database. Data is provided in 5x5 degree tiles, with each degree of latitude equal to approximately 111 km.\nOur first task is to acquire a tile. Tiles can be downloaded from here using wget.\nimport os\nimport math\nfrom PIL import Image, ImageChops, ImageEnhance\nfrom matplotlib import cm\ndef downloadDEMFromCGIAR(lat,lon):\n    ''' Download a DEM from CGIAR FTP repository '''\n    fileName = lonLatToFileName(lon,lat)+'.zip'\n\n    ''' Check to see if we have already downloaded the file '''\n    if fileName not in os.listdir('.'):\n        os.system('''wget --user=data_public --password='GDdci' http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/'''+fileName)\n    os.system('unzip '+fileName)\ndef lonLatToFileName(lon,lat):\n    ''' Compute the input file name '''\n    tileX = int(math.ceil((lon+180)/5.0))\n    tileY = -1*int(math.ceil((lat-65)/5.0))\n    inputFileName = 'srtm_'+str(tileX).zfill(2)+'_'+str(tileY).zfill(2)\n    return(inputFileName)\nlon,lat = -123,49\ninputFileName = lonLatToFileName(lon,lat)\ndownloadDEMFromCGIAR(lat,lon)\n\n\nSlicing\nThe area I have selected covers Washington State and British Columbia, with file name srtm_12_03.tif.\nLet’s use GDAL to extract a subsection of the tile. The subsection covers Vancouver Island and the Pacific Ranges, stretching from 125ºW - 122ºW & 48ºN - 50ºN. Using gdalwarp:\n!! gdalwarp -q -te -125 48 -122 50 -srcnodata -32768 -dstnodata 0 srtm_12_03.tif subset.tif\nOur next step is to transform the subsection of the tile to a different projection. The points in the subsection are located on a grid 1/1200th of a degree apart. A separate scale exists between the latitude & longitude axis and a longitude scale that depends on the latitude. While degrees of latitude are always ~110 km in size, resulting in ~92.5M resolution, degrees of longitude decrease from ~111 km at the equator to 0 km at the poles.\nA solution is to project that points to have a consistent and equal scale in the X/Y plane. One choice is to use a family of projections called Universal Transverse Mercator. Each UTM projection can map points from longitude & latitude to X & Y coordinates in meters. The UTM projection is helpful because it locally preserves both shapes and distances over distances of up to several hundred kilometres.\nThe tradeoff is that several different UTM projections are required for different points on earth, 120 to be precise. Fortunately, working out the required projection based on the longitude and latitude is relatively trivial. Almost every conceivable projection has been assigned a code by the European Petroleum Survey Group (EPSG). You can use this EPSG code to specify the projection being used unambiguously. With UTM, each code starts with either 327 or 326, depending on the hemisphere of the projection.\nutmZone = int((math.floor((lon + 180)/6) % 60) + 1)\n\n''' Check to see if the file is in the northern or southern hemisphere '''\nif lat<0:\n    EPSGCode = 'EPSG:327'+str(utmZone)\nelse:\n    EPSGCode = 'EPSG:326'+str(utmZone)\nOnce we have identified the correct EPSG code to use, warping the subset to a new projection is relatively straightforward.\nIn the following system call to gdalwarp, t_srs denotes the target projection, and tr specifies the resolution in the X and Y plane. The Y resolution is negative because the GDAL file uses a (row, column) coordinate system.\nIn this coordinate system, the origin is in the top left-hand corner of the file. The row value increases as you move down the file, like an excel spreadsheet; however, the UTM Y coordinate decreases. This results in a negative sign in the resolution.\nos.system('gdalwarp -q -t_srs '+EPSGCode+' -tr 100 -100 -r cubic subset.tif warped.tif')\n\n\nHillshading\nAt this point, we can begin to visualise the DEM. One highly effective method is hill-shading, which models how the surface of the DEM would be illuminated by light projected onto it. Shading of the slopes allows the DEM to be more intuitively interpreted than just colouring by height alone.\n\n!! gdaldem hillshade -q -az 45 -alt 45 warped.tif hillshade.tif\n\n\nHypsometric Tinting\nHillshading can also be combined with height information to aid the interpretation of the topography. The process is simple, with GDAL mapping colours to cell heights using a provided colour scheme. The technical name for colouring a DEM based on height is hypsometric tinting.\n\ndef createColorMapLUT(minHeight,maxHeight,cmap = cm.YlGn_r,numSteps=256):\n    '''\n    Create a colourmap for visualisation\n    '''\n    f =open('color_relief.txt','w')\n    f.write('-0.1,135,206,250 \\n')\n    f.write('0.1,135,206,250 \\n')\n    \n    for i in range(0,numSteps):\n        r,g,b,a= cmap(i/float(numSteps))\n        height = minHeight + (maxHeight-minHeight)*(i/numSteps)\n        f.write(str(height)+','+str(int(255*r))+','+str(int(255*g))+','+str(int(255*b))+'\\n')\n    f.write(str(-1)+','+str(int(255*r))+','+str(int(255*g))+','+str(int(255*b))+'\\n')\n    \n\ncreateColorMapLUT(minHeight=10,maxHeight=2658)\n!! gdaldem color-relief -q warped.tif color_relief.txt color_relief.tif\n\n\nSlope Shading\nAnother technique for visualising terrain is slope shading. While hypsometric tinting assigns colours to cells based on elevation, slope shading assigns colours to pixels based on the slope (0º to 90º). In this case, white (255,255,255) is assigned to slopes of 0º and black (0,0,0) is assigned to slopes of 90º, with varying shades of grey for slopes in-between.\n\nThis colour scheme is encoded in a txt file for gdaldem as follows:\nf = open('color_slope.txt','w')\nf.write('0 255 255 255\\n')\nf.write('90 0 0 0\\n')\nf.close()\nThe computation of the slope shaded dem takes place over two steps.\n\nThe slope of each cell is computed.\nEach cell is assigned a shade of grey depending on the slope.\n\n!! gdaldem slope -q warped.tif slope.tif\n!! gdaldem color-relief -q slope.tif color_slope.txt slopeshade.tif\n\n\nLayer Merging\nThe final step in producing the final product is to merge the three different created images. The Python Image Library (PIL) is a quick and dirty way to accomplish this task, combining the three layers using pixel-by-pixel multiplication.\nOne crucial detail to note is that the pixel-by-pixel multiplication occurs in the RGB space. Theoretically, each pixel should be first transformed to the Hue, Saturation, Value (HSV) colour space. The value is then multiplied by the hill-shade and slope-shade values before being transformed into the RGB colour space. However, the RGB space multiplication is a very reasonable approximation in practical terms.\nIn one final tweak, the brightness of the output image is increased by 40% to offset the average reduction in brightness caused by multiplying the layers together.\n\n''' Merge components using Python Image Lib '''\nslopeshade = Image.open(\"slopeshade.tif\").convert('L')\nhillshade = Image.open(\"hillshade.tif\")\ncolorRelief = Image.open(\"color_relief.tif\")\n\n# Let's fill in any gaps in the hill shading\nref = Image.new('L', slopeshade.size,180)\nhillshade = ImageChops.lighter(hillshade,ref)\n\nshading = ImageChops.multiply(slopeshade, hillshade).convert('RGB')\nmerged = ImageChops.multiply(shading,colorRelief)\n\n''' Adjust the brightness to take into account the reduction caused by hill shading.''\nenhancer = ImageEnhance.Brightness(merged)\nimg_enhanced = enhancer.enhance(1.4)\nimg_enhanced.save('Merged.png')\n\n\nFurther reading\nI found the following sources to be invaluable in compiling this post:\n\nCreating colour relief and slope shading\nA workflow for creating beautiful relief-shaded DEMs using gdal\nShaded relief map in python\nStamen Design"
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html",
    "title": "Understanding the impact of timing on defaults",
    "section": "",
    "text": "The thesis of this post is pretty simple. There is a delay between customers making a transaction and when Afterpay realises they have defaulted. Because of this delay, combined with the rapid growth in the total value of transactions, defaults may be artificially reduced as a percentage of transaction value.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#the-model",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#the-model",
    "title": "Understanding the impact of timing on defaults",
    "section": "The Model",
    "text": "The Model\nFirst off, let’s load in a bunch of libraries.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom io import StringIO\nimport pandas as pd\n\nimport scipy.optimize\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nWhile reading through Afterpay’s releases to the markets, I came across this chart, which appears on page 3 of this release. Let’s use this to build a simple quadratic model of the reported sales.\n\n\n\nImage of Afterpay customer growth"
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#loading-the-data",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#loading-the-data",
    "title": "Understanding the impact of timing on defaults",
    "section": "Loading the data",
    "text": "Loading the data\n\n#Underlying sales\ncsv_data = StringIO('''anz_underlying_sales_value,date,month_count\n0,FY15,0\n37.3,FY16,12\n561.2,FY17,24\n2184.6,FY18,36\n4314.1,FY19,48\n6566.9,FY20,60''')\n\ndf = pd.read_csv(csv_data, sep=\",\")"
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#fitting-a-curve",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#fitting-a-curve",
    "title": "Understanding the impact of timing on defaults",
    "section": "Fitting a curve",
    "text": "Fitting a curve\nLet’s first fit quadratic:\n\ndef quadratic(t, a, b, c):\n    y = a * t**2 + b * t + c\n    return y\n\nxdata = df.month_count.values\nydata = df.anz_underlying_sales_value.values\n\npopt, pcov = scipy.optimize.curve_fit(quadratic, xdata, ydata)\n\nprint(popt)\n\n[  2.17012649 -17.61639881 -58.725     ]\n\n\n\nx = np.linspace(0,60, 61)\ny = quadratic(x, *popt)\n\nplt.plot(xdata, ydata, 'o', label='data')\nplt.plot(x,y, label='fit')\n\nplt.title('ANZ Sales by preceding Financial Year ($M AUD)')\nplt.xlabel('Months after launch')\nplt.ylabel('ANZ Sales ($M AUD)')\nplt.legend(loc='best')\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#delays-in-reporting.",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#delays-in-reporting.",
    "title": "Understanding the impact of timing on defaults",
    "section": "Delays in reporting.",
    "text": "Delays in reporting.\nSo we found that we could model the annual reported sales as \\[2.170 t^2 - 17.61t - 58.725\\]\nThe instantaneous sales rate is \\[0.1808t^2 + 0.7021t -9.36\\].\nDon’t worry about how I arrived at this; I will show how in the appendix of this post.\n\nt = np.linspace(0,60, 61)\nsales = 0.1808*t**2 + 0.7021* t - 9.36\n\nplt.plot(sales)\n\nplt.title('ANZ Sales by month ($M AUD)')\nplt.xlabel('Months after launch')\nplt.ylabel('ANZ Sales by month ($M AUD)')\nplt.show()\n\n\n\n\nNow let’s model a delay of 6 months between when the transaction happens and when Afterpay finally realised there was a default.\nFrom this, we can see a significant difference between the actual rate at which losses occur and the rate at which we observe them occurring at any time.\n\ndelay = 6 #months \ntrue_loss_rate = 0.01\n\nlosses_true = true_loss_rate*(0.1808*t**2 + 0.7021* t - 9.36)\nlosses_observed = true_loss_rate*(0.1808*(t-delay)**2 + 0.7021* (t-delay) - 9.36)\nplt.plot(losses_observed,label='Observed')\nplt.plot(losses_true,label='True')\n\nplt.legend()\nplt.title('ANZ losses by month ($M AUD)')\nplt.xlabel('Months after launch')\nplt.ylabel('ANZ losses by month ($M AUD)')\nplt.show()\n\n\n\n\nNow let’s integrate by financial year.\n\ndef integrate_by_year(y):\n    integrated = np.array([0,np.sum(y[0:12]),np.sum(y[12:24]),np.sum(y[24:36]),np.sum(y[36:48]),np.sum(y[48:60])])\n    return(integrated)\n\n\nobserved_loss_rate = integrate_by_year(losses_true)/integrate_by_year(losses_observed)\n\nplt.plot(observed_loss_rate)\n\nplt.title('Ratio of true losses to observed losses')\nplt.xlabel('Years after launch')\nplt.ylabel('Ratio of true losses to observed losses')\nplt.ylim(1,2.5)\nplt.xlim(2,5)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#conclusion",
    "href": "posts/2020-10-03-afterpay-customer-defaults-part-7.html#conclusion",
    "title": "Understanding the impact of timing on defaults",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we can see the impact of a delay in recognising losses in situations with rapid growth. Even after years of growth, with a six-month delay in recognising losses, the actual losses could be 30-40% higher than reported."
  },
  {
    "objectID": "posts/2021-02-17-an-introduction-to-augemented-reality.html",
    "href": "posts/2021-02-17-an-introduction-to-augemented-reality.html",
    "title": "An Introduction to Augmented Reality",
    "section": "",
    "text": "Augmented Reality is a way of stitching together the virtual and real worlds.\nIf we have enough information about a camera, we can render virtual imagery and then composite it with captured imagery of the real world. In particular, we need information about the camera’s intrinsic matrix (K) and its extrinsic matrix, which contains information about its position and attitude in the real world.\nTo keep things simple, I will use this chessboard style calibration pattern to orient the pattern. The advantage of using this type of pattern is that we can accurately measure the position of each corner at the intersection between white and black squares."
  },
  {
    "objectID": "posts/2021-02-17-an-introduction-to-augemented-reality.html#the-code",
    "href": "posts/2021-02-17-an-introduction-to-augemented-reality.html#the-code",
    "title": "An Introduction to Augmented Reality",
    "section": "The Code",
    "text": "The Code\nAs always, let’s start by importing what we need.\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as Rot\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\n\nI’ve already calibrated the camera, which means that we have both the camera matrix (K) and the camera’s distortion parameters.\n\nK = np.array([[3286.9, 0.0 , 2032.8],\n              [0.0   , 3282.3, 1519.1],\n              [0.0   , 0.0   ,    1.0]])\n\ndist = np.array([[2.44663616e-01],[-1.48023303e+00],[2.04607109e-03],[1.53484564e-03],[2.56349651e+00]])\n\nLet’s now load the image and find the corners.\n\nfname = 'data/2021-02-17-An-Introduction-To-Augemented-Reality/chessboard.jpg'\nimg = cv2.imread(fname)\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n\n# Find the chess board corners\nret, points_2d = cv2.findChessboardCorners(gray, (9,6),None)\n\n#Refine the position of the corners\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\npoints_2d_precise = cv2.cornerSubPix(gray,points_2d,(11,11),(-1,-1),criteria)\n\nLet’s now visualise the corners that we found.\n\npoints_2d_precise = np.squeeze(points_2d_precise)\nplt.imshow(img[:,:,::-1])\nplt.scatter(points_2d_precise[:,0],points_2d_precise[:,1], color='r', alpha=0.75)\nplt.show()\n\n\n\n\nWe also know the 3D location of the points in the corners because it’s a flat grid.\n\n# Create a grid of points\npoints_3d = np.zeros((6*9,3), np.float32)\npoints_3d[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n\n# Put everything in the correct shape\npoints_3d = np.expand_dims(points_3d,2)\npoints_2d_precise = np.expand_dims(points_2d_precise, 2)\n\nUsing the point correspondences and the camera’s intrinsic parameters, we can solve to find the relationship between the camera’s coordinate frame and the world’s coordinate frame.\n\nIn this case rvec is in Rodrigues format.\n\nret, rvec, tvec = cv2.solvePnP(points_3d, points_2d_precise, K, dist)\n\n\nprint(rvec)\n\n[[-0.63902069]\n [-0.59625232]\n [-1.46556394]]\n\n\n\nprint(tvec)\n\n[[-2.83521712]\n [ 0.77473203]\n [ 7.22217967]]\n\n\nLet’s now visualise the X, Y and Z axis overlayed on top of our image. Note that the Z-axis, in this case, is going down into the table.\n\n# project 3D points to image plane\naxis = np.float32([[3,0,0], [0,3,0], [0,0,3]]).reshape(-1,3)\nimgpts, jac = cv2.projectPoints(axis, rvec, tvec, K, dist)\nimgpts = np.squeeze(imgpts)\n\nplt.imshow(img[:,:,::-1])\nplt.plot([points_2d_precise[0,0],imgpts[0,0]],[points_2d_precise[0,1], imgpts[0,1]], color='r', alpha=0.75, linewidth=5, label='X Axis')\nplt.plot([points_2d_precise[0,0],imgpts[1,0]],[points_2d_precise[0,1], imgpts[1,1]], color='g', alpha=0.75, linewidth=5, label='Y Axis')\nplt.plot([points_2d_precise[0,0],imgpts[2,0]],[points_2d_precise[0,1], imgpts[2,1]], color='b', alpha=0.75, linewidth=5, label='Z Axis')\n\nplt.legend()\nplt.show()\n\n\n\n\nWe have both the rotation, and the translation, which transform points from the world frame to the camera frame. From this, we can find the camera’s location in the world (Camera_C) and orientation with respect to the world frame (Camera_R).\n\nr = Rot.from_rotvec(rvec[:,0])\nR = r.as_matrix()\nt = np.array(tvec)\n\nWe can see that the camera is located at (-4.5,3,-5.5) units (grid squares).\n\nCamera_C = -np.dot(R.T,tvec)\nprint(Camera_C)\n\n[[-4.52503556]\n [ 3.05060167]\n [-5.56923942]]\n\n\nNow let’s find its orientation as a quaternion.\n\nCamera_R = Rot.from_matrix(R.T).as_quat()\nprint(Camera_R)\n\n[0.2821332  0.26325059 0.64705923 0.65758219]"
  },
  {
    "objectID": "posts/2021-02-17-an-introduction-to-augemented-reality.html#the-results",
    "href": "posts/2021-02-17-an-introduction-to-augemented-reality.html#the-results",
    "title": "An Introduction to Augmented Reality",
    "section": "The Results",
    "text": "The Results\nGiven that we have both intrinsic and extrinsic parameters,  we can render images using Blender, using the same set of parameters. Then we can composite the synthetically rendered objects back into the original photograph. This results in a convincing illusion, which forms the basis for Augmented Reality."
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "",
    "text": "From the previous post, I made some high-level estimates for how often Afterpay transactions attracted late fees. If we could better understand the distribution of late fees, we could better estimate the frequency of late fees.\n\n\n\n\n\n\nImportant\n\n\n\nObviously, I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot."
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#what-do-we-know",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#what-do-we-know",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "What do we know?",
    "text": "What do we know?\n\nLate Fees Revenue: 46.1 million AUD\nAverage Transaction Value: Approximately 150 AUD\n\nFurthermore, we know that the lowest and highest fee you can charge for a single transaction is 10 AUD and 68 AUD. Hence, this, in turn, bounds the average of the late fees.\nAfterpay is pretty transparent about their late fees; I’ve taken the following from here.\n\n\n\nAfterpay late fees policy\n\n\nWe can model this policy in code, keeping in mind that the customer pays Afterpay 4 separate times.\nThere are some pieces of information that we still need.\n\nThe distribution of values for transactions where customers pay late.\nHow late do they make each payment.\n\nAt this stage, we have to accept that we can’t find an exact solution, but we can settle for a good approximation.\nOne assumption is that the number of late payments is uncorrelated with the transaction value and uniformly distributed for delays of less than and more than one week."
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#computing-late-fees",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#computing-late-fees",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "Computing late fees",
    "text": "Computing late fees\n\nimport random\ndef compute_late_fee(transaction_value,probability_of_late_payment = 0.5):\n    total_fees = 0\n    \n    number_of_delays_of_less_than_1_week = 1 + np.random.binomial(3,probability_of_late_payment,1)[0]\n    \n    if number_of_delays_of_less_than_1_week > 0:\n        number_of_delays_of_more_than_1_week = np.random.binomial(number_of_delays_of_less_than_1_week,probability_of_late_payment,1)[0]\n    \n    total_fees = 10 * number_of_delays_of_less_than_1_week + 7 * number_of_delays_of_more_than_1_week\n    late_fee = min([total_fees,0.25*transaction_value])\n    late_fee = max([late_fee,10])\n    return(late_fee)\n\n\n%matplotlib inline\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport scipy.stats as st\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#modelling-transaction-values",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#modelling-transaction-values",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "Modelling transaction values",
    "text": "Modelling transaction values\nLet’s model the transaction values.\nI’m making two assumptions:\n\nTransactions, where the customer is late paying have the same average transaction value as other payments.\nTransaction values are Exponentially distributed.\n\n\nx = np.linspace(0, 1500, 300)\npdf = st.expon.pdf(x, scale=150)\nplt.plot(x, pdf)\nplt.xlabel('Transaction value (AUD)')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\n\nwith pm.Model() as model:\n    average_transaction_value_aud = pm.Uniform('average_transaction_value_aud', lower=144.50, upper=154.49)\n    transaction_value_aud = pm.Bound(pm.Exponential, upper=1_500.0)('transaction_value_aud', lam = 1/average_transaction_value_aud)\n\nNow that we have instantiated all of the random variables, we will take 50,000 draws from them to perform our Monte Carlo simulation.\n\nwith model:\n    samples = pm.sample_prior_predictive(samples=50_000, random_seed=0)\n\n\nsns.distplot(samples[\"transaction_value_aud\"], kde=False, norm_hist=True, bins=100)\nplt.title('Histogram of transaction values')\nplt.xlabel('Transaction value (AUD)')\nplt.ylabel('Relative Frequency')\nplt.show()\n\n\n\n\nAs we can see, this distribution has a mean of approximately 150 AUD.\n\npm.summary(samples['transaction_value_aud'])\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      150.103\n      150.269\n      0.0\n      422.672\n      0.675\n      0.477\n      49547.0\n      49547.0\n      49322.0\n      49878.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#modelling-late-fees",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#modelling-late-fees",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "Modelling late fees",
    "text": "Modelling late fees\n\nlate_fees = []\nfor transaction_value in samples[\"transaction_value_aud\"]:\n    late_fee = compute_late_fee(transaction_value)\n    late_fees.append(late_fee)\nlate_fees = np.array(late_fees)\n\n\nsns.distplot(late_fees, kde=False, norm_hist=True, bins=np.arange(10,70,1))\nplt.xlabel('Average Late Fee (AUD)')\nplt.show()\n\n\n\n\n\npm.summary(late_fees)\n\narviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      x\n      22.585\n      12.514\n      10.0\n      44.0\n      0.056\n      0.04\n      49161.0\n      49161.0\n      48824.0\n      49670.0\n      NaN"
  },
  {
    "objectID": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#conclusion",
    "href": "posts/2020-08-22-afterpay-customer-defaults-part-2.html#conclusion",
    "title": "How often do Afterpay transactions attract late fees? (Part 2)",
    "section": "Conclusion",
    "text": "Conclusion\nWe can now compute the expected average late fee, given the distribution of transaction values and the probability of making each payment on time. In our case, we found the average late fee was 37.49 AUD.\nHowever, we can’t conclude anything meaningful from a single distribution of transaction values and a single probability of making each payment on time.\nGiven this, we can try many different distributions and use this to find the average late fees distribution. Next post, I will make the way I do the sampling more computationally efficient. This will allow us to draw some meaningful conclusions."
  },
  {
    "objectID": "posts/2020-07-07-bayesian-camera-calibration.html",
    "href": "posts/2020-07-07-bayesian-camera-calibration.html",
    "title": "Bayesian Camera Calibration",
    "section": "",
    "text": "df = pd.read_csv('data/2020-07-05-Bayesian-Camera-Calibration/points.csv',sep =' ')\npx = df.i.values\npy = df.j.values\n\nX_input = df.X.values\nY_input = df.Y.values\nZ_input = df.Z.values\n\nnumber_points = px.shape[0]\n\npoints3d = np.vstack([X_input,Y_input,Z_input]).T\n\n\ndef create_rotation_matrix(Q0,Q1,Q2,Q3):\n    R =[[Q0**2 + Q1**2 - Q2**2 - Q3**2, 2*(Q1*Q2 - Q0*Q3), 2*(Q0*Q2 + Q1*Q3)],\n        [2*(Q1*Q2 + Q0*Q3), Q0**2 - Q1**2 + Q2**2 - Q3**2, 2*(Q2*Q3 - Q0*Q1)],\n        [2*(Q1*Q3 - Q0*Q2), 2*(Q0*Q1 + Q2*Q3), (Q0**2 - Q1**2 - Q2**2 + Q3**2)]]\n    return(R)\n\ndef normalize_quaternions(Q0,Q1,Q2,Q3):\n    norm = pm.math.sqrt(Q0**2 + Q1**2 + Q2**2 + Q3**2)\n    Q0 /= norm\n    Q1 /= norm\n    Q2 /= norm\n    Q3 /= norm\n    return(Q0,Q1,Q2,Q3)\n\ndef Rotate_Translate(X_est, Y_est, Z_est):\n    Q1 = pm.StudentT('Xq', nu = 1.824, mu = 0.706, sigma = 0.015)\n    Q2 = pm.StudentT('Yq', nu = 1.694, mu = -0.298, sigma = 0.004)\n    Q3 = pm.StudentT('Zq', nu = 2.015, mu = 0.272, sigma = 0.011)\n    Q0 = pm.StudentT('Wq', nu = 0.970, mu = 0.590, sigma = 0.019)\n    \n    Q0,Q1,Q2,Q3 = normalize_quaternions(Q0,Q1,Q2,Q3)\n    \n    R = create_rotation_matrix(Q0,Q1,Q2,Q3)\n    \n    # Define priors \n    X_centroid = pm.Normal('X_centroid', mu = -6.85, sigma = 10)\n    Y_centroid = pm.Normal('Y_centroid', mu = -12.92, sigma = 10)\n    Z_centroid = pm.Normal('Z_centroid', mu = 2.75, sigma = 10)\n    \n    RIC_0_3 = R[0][0] * -X_centroid + R[0][1] * -Y_centroid + R[0][2] * -Z_centroid\n    RIC_1_3 = R[1][0] * -X_centroid + R[1][1] * -Y_centroid + R[1][2] * -Z_centroid\n    RIC_2_3 = R[2][0] * -X_centroid + R[2][1] * -Y_centroid + R[2][2] * -Z_centroid\n    \n    X_out = X_est * R[0][0] + Y_est * R[0][1] + Z_est * R[0][2] + RIC_0_3\n    Y_out = X_est * R[1][0] + Y_est * R[1][1] + Z_est * R[1][2] + RIC_1_3\n    Z_out = X_est * R[2][0] + Y_est * R[2][1] + Z_est * R[2][2] + RIC_2_3\n    \n    return(X_out, Y_out, Z_out)\n    \n\n\nwith pm.Model() as model: \n    X, Y, Z = Rotate_Translate(points3d[:,0], points3d[:,1], points3d[:,2])\n    \n    focal_length = pm.Normal('focal_length',mu = 2191, sigma = 11.50)\n    \n    k1 = pm.Normal('k1', mu = -0.327041, sigma = 0.5 * 0.327041)\n    k2 = pm.Normal('k2', mu = 0.175031,  sigma = 0.5 * 0.175031)\n    k3 = pm.Normal('k3', mu = -0.030751, sigma = 0.5 * 0.030751)\n    \n    c_x = pm.Normal('c_x', mu = 2268/2.0, sigma = 1000)\n    c_y = pm.Normal('c_y', mu = 1503/2.0, sigma = 1000)\n    \n    px_est = X / Z\n    py_est = Y / Z\n    \n    #Radial distortion\n    r = pm.math.sqrt(px_est**2 + py_est**2)\n    \n    radial_distortion_factor = (1 + k1 * r + k2 * r**2 + k3 * r**3)\n    px_est *= radial_distortion_factor\n    py_est *= radial_distortion_factor\n    \n    px_est *= focal_length\n    py_est *= focal_length\n\n    px_est += c_x\n    py_est += c_y\n    \n    error_scale = 5 #px\n    \n    delta = pm.math.sqrt((px - px_est)**2 + (py - py_est)**2)\n    \n    # Define likelihood\n    likelihood = pm.Normal('rms_pixel_error', mu = delta, sigma = error_scale, observed=np.zeros(number_points))\n\n    # Inference!\n    trace = pm.sample(draws=10_000, init='adapt_diag', cores=4, tune=5_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [c_y, c_x, k3, k2, k1, focal_length, Z_centroid, Y_centroid, X_centroid, Wq, Zq, Yq, Xq]\nSampling 4 chains, 0 divergences: 100%|██████████| 60000/60000 [1:11:55<00:00, 13.90draws/s]\nThe chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\nThe chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\nplt.figure(figsize=(7, 7))\npm.traceplot(trace);\nplt.tight_layout();\n\n<Figure size 504x504 with 0 Axes>\n\n\n\n\n\n\npm.plot_posterior(trace);\n\n\n\n\n\npm.summary(trace)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hpd_3%\n      hpd_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Xq\n      0.674\n      0.025\n      0.630\n      0.719\n      0.000\n      0.000\n      11170.0\n      11170.0\n      11931.0\n      17643.0\n      1.0\n    \n    \n      Yq\n      -0.309\n      0.012\n      -0.331\n      -0.292\n      0.000\n      0.000\n      10639.0\n      10549.0\n      12481.0\n      16032.0\n      1.0\n    \n    \n      Zq\n      0.282\n      0.013\n      0.259\n      0.307\n      0.000\n      0.000\n      14060.0\n      13985.0\n      14346.0\n      20241.0\n      1.0\n    \n    \n      Wq\n      0.553\n      0.028\n      0.502\n      0.603\n      0.000\n      0.000\n      10825.0\n      10825.0\n      11893.0\n      12353.0\n      1.0\n    \n    \n      X_centroid\n      -11.255\n      0.235\n      -11.714\n      -10.833\n      0.002\n      0.002\n      9757.0\n      9748.0\n      9862.0\n      13180.0\n      1.0\n    \n    \n      Y_centroid\n      -16.454\n      0.188\n      -16.818\n      -16.118\n      0.002\n      0.001\n      11012.0\n      10990.0\n      11271.0\n      14854.0\n      1.0\n    \n    \n      Z_centroid\n      7.443\n      0.084\n      7.286\n      7.601\n      0.001\n      0.000\n      18893.0\n      18893.0\n      19120.0\n      19610.0\n      1.0\n    \n    \n      focal_length\n      2193.775\n      11.109\n      2173.096\n      2214.626\n      0.068\n      0.048\n      27045.0\n      27045.0\n      27044.0\n      27713.0\n      1.0\n    \n    \n      k1\n      -0.065\n      0.040\n      -0.141\n      0.011\n      0.000\n      0.000\n      14046.0\n      12577.0\n      14140.0\n      17074.0\n      1.0\n    \n    \n      k2\n      0.101\n      0.056\n      -0.004\n      0.207\n      0.000\n      0.000\n      18678.0\n      18678.0\n      18655.0\n      22622.0\n      1.0\n    \n    \n      k3\n      -0.034\n      0.015\n      -0.062\n      -0.005\n      0.000\n      0.000\n      35433.0\n      31917.0\n      35431.0\n      26077.0\n      1.0\n    \n    \n      c_x\n      1119.535\n      37.046\n      1059.220\n      1183.436\n      0.370\n      0.273\n      10021.0\n      9175.0\n      19798.0\n      11469.0\n      1.0\n    \n    \n      c_y\n      993.413\n      107.416\n      786.346\n      1193.531\n      1.031\n      0.729\n      10861.0\n      10861.0\n      10964.0\n      14739.0\n      1.0\n    \n  \n\n\n\n\n\npm.pairplot(trace, kind = 'hexbin', var_names=['X_centroid','Y_centroid','Z_centroid'], divergences=True);\n\n\n\n\n\npm.pairplot(trace, kind = 'hexbin', var_names=['k1', 'k2', 'k3'], divergences=True);\n\n\n\n\n\npm.pairplot(trace, kind = 'hexbin', var_names=['c_x', 'c_y'], divergences=True);\n\n\n\n\n\npm.pairplot(trace,kind = 'hexbin', var_names=['Wq', 'Xq','Yq','Zq'], divergences=True);\n\n\n\n\n\nsns.jointplot(trace[:]['X_centroid'], trace[:]['Y_centroid'], kind=\"hex\");\n\n\n\n\n\nsns.jointplot(trace[:]['X_centroid'], trace[:]['Z_centroid'], kind=\"hex\");\n\n\n\n\n\nsns.jointplot(trace[:]['c_x'], trace[:]['c_y'], kind=\"hex\");\n\n\n\n\n\nsns.jointplot(trace[:]['Wq'], trace[:]['Xq'], kind=\"hex\");"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html",
    "title": "An Adventure in Camera Calibration",
    "section": "",
    "text": "It’s February 1972; the A300 airliner is being unveiled in Toulouse; let’s go on an adventure (In-camera calibration!).\n\n\n\nAn image of the Airbus A300 Final Assembly line in Toulouse\n\n\nWe have seen this photo published in a magazine, and we want to learn as much about the dimensions of Airbus’s new aircraft as possible. To do so, we will need to mathematically reconstruct the camera used to take the photo and the scene itself."
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#control-points",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#control-points",
    "title": "An Adventure in Camera Calibration",
    "section": "Control points",
    "text": "Control points\nIn this case, we are lucky because we notice the hexagonal pattern on the floor. In particular, it’s a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions.\nWhile we don’t know the hexagon’s dimensions, each side is approximately 1.6m long, based on the height of the people in the photo. If we assume some point on the ground, say the centre of a polygon is the point 0,0, we can work out the X & Y location of each other polygon vertex we can see. Hence the Z coordinate of each point is 0. Furthermore, we could also assume that the factory floor is flat and level.\nLet’s spend ±5 minutes annotating the image using an annotation tool like label me. I’ve generated a file, which you can find attached here:\n\n\n\nAn annotated image of the Airbus A300 Final Assembly line in Toulouse\n\n\nFirstly, lets load in all of the x and y points:\n\nimport json\nimport numpy as np\n\nJSON = json.loads(open('data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.json','r').read())\npolygons = {}\nfor shape in JSON['shapes']:\n    coords = shape['label'].split(',')\n    x,y = int(coords[0]),int(coords[1])\n    polygons[x,y] = shape['points']    \n\nOk, now do some maths, and work out the locations of each vertex of our hexagons.\n\nfrom sklearn.neighbors import KDTree\n\npoints = []\nkeys = sorted(polygons.keys())\n\nfor key in keys:\n    poly = polygons[key]    \n    (pts_x, pts_y) = zip(*poly)\n    \n    pts_x = list(pts_x)\n    pts_y = list(pts_y)\n    \n    #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is.\n    x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1])\n    y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)])\n    \n    row,col = key\n    x = row * 1.5 + x_vertex\n    y = col * 0.5 * np.sqrt(3) + y_vertex\n    \n    #From before, we assume the sides of each polygon is 1.6m\n    x*=1.6 #meters\n    y*=1.6 #meters\n    \n    for idx in range(0,6):\n        point = []\n        i = pts_x[idx]\n        j = pts_y[idx]\n        X = x[idx]\n        Y = y[idx]\n        Z = 0.0\n        points.append([i,j,X,Y,Z])\n        \n\nNow we are presented with a minor problem; in many cases, we annotated the same point up to 3 times, where the vertices of the hexagons meet. So let’s go and find points that are within 10 pixels and then take their average. If we do this, we can effectively over-weight some points in the image at the expense of others.\n\npoints = np.asarray(points)\nnp.savetxt(\"data/2020-02-23-An-Adventure-In-Camera-Calibration/points.csv\", points)\n\ntree = KDTree(points[:,0:2], leaf_size=5)\n\nmerged_indicies = []\nunique_points = []\nfor i in range(0,points.shape[0]):\n    if i not in merged_indicies:\n        dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3)\n        \n        indicies_to_merge = []\n        for j in range(0,3):\n            if dist[0][j]<10:\n                indicies_to_merge.append(ind[0][j]) \n                merged_indicies.append(ind[0][j])\n\n        mean_points = np.mean(points[indicies_to_merge,:],axis=0)\n        unique_points.append(mean_points)\n        \nunique_points = np.asarray(unique_points)"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#camera-parameters",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#camera-parameters",
    "title": "An Adventure in Camera Calibration",
    "section": "Camera Parameters",
    "text": "Camera Parameters\nSo, now we have many 3D points and corresponding 2D points in the photo.\nNow it’s time to turn to the real magic, bundle adjustment. Our task is to find a camera that best fits our measured data.\nLet’s talk more about cameras.\n\n\n\n\n\n\nImportant\n\n\n\nThere are many correct ways to model a camera mathematically. This is one way.\n\n\nMathematically, cameras are composed of two parameters, Intrinsic and Extrinsic. The Extrinsic parameters define the position and rotation of the camera with respect to the origin of the points it’s observing.\nThe Intrinsic parameters define the camera’s parameters, such as the Focal length, the location of the camera’s radial centre, and the distortion induced by the lens.\nThe Extrinsic parameters comprise 6 degrees of freedom, given our world is three-dimensional, and there are three dimensions to rotate around.\nThe Intrinsic parameters are more complex. There are some great resources, for example, Multiple View Geometry in Computer Vision or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown.\n\n\n\n\n\n\nNote\n\n\n\nTo be clear, I’m building on the shoulders of giants; I’ve heavily adapted this example from this incredible demo by Nikolay Mayorov which you can find here\n\n\nFirst, let’s import a bunch of stuff we will need later.\n\nfrom __future__ import print_function\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.optimize import least_squares\nfrom scipy.spatial.transform import Rotation as Rot\n\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\n\npoints_2d = unique_points[:,0:2]\npoints_3d = unique_points[:,2:5]\n\nprint('We have {} unique points'.format(points_2d.shape[0]))\n\nWe have 51 unique points"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#modelling-the-camera",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#modelling-the-camera",
    "title": "An Adventure in Camera Calibration",
    "section": "Modelling the Camera",
    "text": "Modelling the Camera\n\nIntrinsic Matrix\nNow we come to the real magic.\n\\[\\begin{equation*}\nx = PX\n\\end{equation*}\\]\nThis function models the camera, taking points in 3D space and converting them into points in 2D space.\nThere are lots of things going on here.\n\n\n\nDiagram of a pinhole camera\n\n\nFirstly, let’s talk about the camera’s intrinsic matrix.\nIt converts points from 3D space to 2D space.\n\\[\\begin{equation*}\nK =\n\\begin{bmatrix}\nf & 0 & c_{x} \\\\\n0 & f & c_{y} \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{equation*}\\]\nWe have the focal length, \\(f\\), and the camera optical centre \\(c_x\\) and \\(c_y\\).\n\n\nExtrinsic Matrix\nNow let’s talk about the camera’s extrinsic matrix.\nThe 6 degrees of freedom describe its position and orientation within the world. That’s 3 degrees for the position and 3 for the orientation. At its heart, what we are doing is simple but confusing.\nThere are so many ways to represent our setup: 1. Coordinate systems: 2D and 3D. * Left-Handed or Right Handed?\n\nRotations:\n\nQuaternions?\nProper Euler angles (6 different ways)?\nTait–Bryan angles (6 different ways)?\nRodrigues rotation formula?\nA rotation matrix?\n\nThe location of the camera in the world. (2 Different ways).\n\nToday, we will use two ways to represent the rotations: a Rodrigues rotation vector representation and a rotation matrix.\nWe use two different representations because it’s easier to optimise when we have 3 degrees of freedom than a naive rotation matrix with nine numbers representing 3 degrees of freedom.\nR represents the camera’s orientation in the World Coordinate Frame (The frame we use to describe our 3D points).\nIn python, we can use convert from the Rodrigues rotation vector to the Rotation matrix as follows:\nfrom scipy.spatial.transform import Rotation as Rot\nrotation_vector = camera_params[:3]\nR = Rot.from_rotvec(rotation_vector).as_matrix()\n\\[\\begin{equation*}\nR =\n\\begin{bmatrix}\nR_1 & R_2 & R_3 \\\\\nR_4 & R_5 & R_6 \\\\\nR_7 & R_8 & R_9\n\\end{bmatrix}\n\\end{equation*}\\]\nNow, let’s talk about the Project Matrix \\(P\\) of the camera. This takes the points from their location in 3D world coordinates to pixel coordinates, assuming we have a camera without radial distortion. There are two main ways this could be formulated.\nFirstly: \\[\\begin{equation*}\nP = KR[I|−C]\n\\end{equation*}\\]\nSecondly: \\[\\begin{equation*}\nP = K[R | t]\n\\end{equation*}\\]\nWhere \\(t\\) is: \\[\\begin{equation*}\nt = −RC\n\\end{equation*}\\]\nLet’s go with the first method, where C is :\n\\[\\begin{equation*}\nC =\n\\begin{bmatrix}\n-C_X\\\\\n-C_Y\\\\\n-C_Z\n\\end{bmatrix}\n\\end{equation*}\\]\n\n\nLens distortion\nHowever, there is one subtlety alluded to before, which is the impact of radial distortion. The camera’s lens distorts the rays of light coming in, in a non-linear way.\nWe can model it using a Taylor series:\n\\[\\begin{equation*}\nx_c = x(1 + k_1  r + k_2  r^2 + k_3 r^3)\n\\end{equation*}\\] \\[\\begin{equation*}\ny_c = y(1 + k_1  r + k_2 r^2 + k_3 r^3)\n\\end{equation*}\\]\nIn python, we end up with the following:\nr = np.sqrt(np.sum(points_proj**2, axis=1)))\nr = 1 + k1 \\times r + k2 * r**2 + k3 * r**3\npoints_proj *= r[:, np.newaxis]"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#putting-it-all-together",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#putting-it-all-together",
    "title": "An Adventure in Camera Calibration",
    "section": "Putting it all together",
    "text": "Putting it all together\n\ndef project(points, camera_params):\n    \"\"\"Convert 3-D points to 2-D by projecting onto images.\"\"\"\n    #Rotation\n    rotation_vector = camera_params[:3]\n    R = Rot.from_rotvec(rotation_vector).as_matrix()\n    \n    #Camera Center\n    C = camera_params[3:6].reshape(3,1)\n    IC = np.hstack([np.eye(3),-C])\n    RIC = np.matmul(R,IC)\n    \n    #Make points Homogeneous\n    points = np.hstack([points,np.ones((points.shape[0],1))])\n    \n    #Perform Rotation and Translation\n    #(n,k), (k,m) -> (n,m)\n    points_proj = np.matmul(points,RIC.T)\n    \n    #perspective divide \n    points_proj = points_proj[:, :2] / points_proj[:, 2, np.newaxis]\n    \n    f  = camera_params[6]\n    \n    k1 = camera_params[7]\n    k2 = camera_params[8]\n    k3 = camera_params[9]\n    c_x = camera_params[10]\n    c_y = camera_params[11]\n    \n    #Radial distortion\n    r = np.sqrt(np.sum(points_proj**2, axis=1))\n    \n    x = points_proj[:,0]\n    y = points_proj[:,1]\n    \n    points_proj[:,0] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*x\n    points_proj[:,1] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*y\n    \n    #Make points Homogeneous\n    points_proj = np.hstack([points_proj, np.ones((points_proj.shape[0],1))])\n    \n    K = np.asarray([[f, 0, c_x],\n                    [0, f, c_y],\n                    [0, 0,   1.0]])\n    \n    points_proj = np.dot(points_proj,K.T)\n    points_proj = points_proj[:,:2]\n    return(points_proj)"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#initial-parameters",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#initial-parameters",
    "title": "An Adventure in Camera Calibration",
    "section": "Initial Parameters",
    "text": "Initial Parameters\nLet’s start by providing hints to the optimiser about the solution by putting in some reasonable starting conditions.\nWe know both the image width and height, and we can assume that the principal point is in the centre of the image.\nThe camera is about 10 meters off the ground.\nLet’s rotate the camera, so it faces directly down to make optimisation easier. This means that the points should be in front of/below it.\nLet’s also assume that the camera is centred above the points. It’s not strictly correct, based on what we see in the image, but it’s not horrifically wrong.\n\nimage_width =  2251\nimage_height = 1508\n\nestimated_focal_length_px = 2000\n\ncamera_params = np.zeros(12)\n\nr = Rot.from_euler('x', 180, degrees=True).as_rotvec()\n\n#Rotation matrix\ncamera_params[0] = r[0]\ncamera_params[1] = r[1]\ncamera_params[2] = r[2]\n\n#C\ncamera_params[3] = points_3d[:,0].mean()\ncamera_params[4] = points_3d[:,1].mean()\ncamera_params[5] = 10\n\n#f,k1,k2,\ncamera_params[6] = estimated_focal_length_px\ncamera_params[7] = 0\ncamera_params[8] = 0\ncamera_params[9] = 0\n\n#c_x,c_y\ncamera_params[10] = image_width/2.0\ncamera_params[11] = image_height/2.0"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#optimisation",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#optimisation",
    "title": "An Adventure in Camera Calibration",
    "section": "Optimisation",
    "text": "Optimisation\nThis section below is well explained by here.\nWe are optimising to minimise a geometric error, that is, the distance between the 2D points we see and the projection of their 3D counterparts.\nThrough optimisation, we aim to find parameters that result in a low error, which means they should represent the actual parameters of the camera.\n\ndef fun(camera_params, points_2d, points_3d):\n    #Compute residuals.\n    points_proj = project(points_3d, camera_params)\n    return(points_proj - points_2d).ravel()\n\n\nx0 = camera_params.ravel()\n\noptimization_results = least_squares(fun, x0,  verbose=1, x_scale='jac', ftol=1e-4, method='lm',\n                                     loss='linear',args=(points_2d, points_3d))\n\n`ftol` termination condition is satisfied.\nFunction evaluations 970, initial cost 3.7406e+07, final cost 1.7398e+02, first-order optimality 3.63e+03."
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#results",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#results",
    "title": "An Adventure in Camera Calibration",
    "section": "Results",
    "text": "Results\nNow let’s go and check out the results of our optimization process.\n\ncamera_params = optimization_results.x\n\nR_Rodrigues = camera_params[0:3]\nC = camera_params[3:6]\n\nr = Rot.from_rotvec(R_Rodrigues)\nR_matrix = r.as_matrix()\n\nr = Rot.from_matrix(R_matrix.T)\nR_Quaternion = r.as_quat()\n\nprint('Quaternions: X: {:.3f} Y: {:.3f} Z: {:.3f} W: {:.3f} '.format(R_Quaternion[0],R_Quaternion[1],R_Quaternion[2],R_Quaternion[3]))\nprint('Camera position relative to the origin in (M): X: {:.2f}, Y: {:.2f}, Z: {:.2f}'.format(C[0],C[1],C[2]))\n\nfocal_length_px = camera_params[6]\nk1 = camera_params[7]\nk2 = camera_params[8]\nk3 = camera_params[9]\n\nc_x = camera_params[10]\nc_y = camera_params[11]\n\nprint('Focal length (Pixels): {:.2f}'.format(focal_length_px))\nprint('CX, CY: {:.2f} {:.2f}'.format(c_x,c_y))\n\nprint('K_1, K_2, K_3 : {:.6f}, {:.6f}, {:.6f}'.format(k1,k2,k3))\nprint('Mean error per point: {:.2f} pixels '.format(optimization_results.cost/points_2d.shape[0]))\n\nQuaternions: X: 0.894 Y: -0.408 Z: 0.084 W: -0.166 \nCamera position relative to the origin in (M): X: -6.85, Y: -12.92, Z: 2.75\nFocal length (Pixels): 1010.93\nCX, CY: 1038.58 2663.52\nK_1, K_2, K_3 : -0.327041, 0.175031, -0.030751\nMean error per point: 3.41 pixels \n\n\nOk, the mean error per point is 3-4 pixels. We have found a decent solution; however, some interesting things are happening.\nIn particular, the principal point lies outside the image, which is curious. One possibility is that the image was cropped.\nNow let’s have a quick look at the errors for each point.\n\nplt.hist(abs(optimization_results.fun),density=True)\nplt.title('Histogram of Residuals')\nplt.xlabel('Absolute Residual (Pixels)')\nplt.grid()\nplt.show()\n\n\n\n\nSo the histogram looks pretty good, apart from the one point with a high residual, probably due to sloppy labelling/annotation.\nNow let’s compare the points we annotated, with where they would be projected, using the camera parameters we found:\n\npoints_2d_proj = project(points_3d, optimization_results.x)\n\nimg = plt.imread('data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg')      \nplt.imshow(img)\nplt.scatter(points_2d[:,0],points_2d[:,1],label='Actual',c='r',alpha=0.5)\nplt.scatter(points_2d_proj[:,0],points_2d[:,1],label='Optimised',c='k',alpha=0.5)\nplt.show()\n\n\n\n\nAgain, this looks great. Finally, let’s overlay the hexagons on the floor, to build confidence in our solution visually.\n\ndef plot_verticies(row,col):\n    x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1,1])\n    y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3),np.sqrt(3)])\n\n    x = row * 1.5 + x_vertex\n    y = col * 0.5 * np.sqrt(3) + y_vertex\n\n    x*=1.6\n    y*=1.6\n\n    points_3d = np.vstack([x,y,np.zeros(7)]).T\n    points_2d_proj = project(points_3d, optimization_results.x)\n    \n    return(points_2d_proj)\n    \nplt.imshow(img)\nfor row in range(0,10,2):\n    for col in range(0,10,2):\n        points_2d_proj = plot_verticies(row,col)\n        plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color='B',alpha=0.25)\n        plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+','+str(col), horizontalalignment='center',verticalalignment='center')\n        \n\nfor row in range(1,11,2):\n    for col in range(1,11,2):\n        points_2d_proj = plot_verticies(row,col)\n        plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color='R',alpha=0.25)\n        plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+','+str(col), horizontalalignment='center',verticalalignment='center')\n         \nplt.show()"
  },
  {
    "objectID": "posts/2020-02-23-an-adventure-in-camera-calibration.html#in-conclusion",
    "href": "posts/2020-02-23-an-adventure-in-camera-calibration.html#in-conclusion",
    "title": "An Adventure in Camera Calibration",
    "section": "In Conclusion",
    "text": "In Conclusion\nWe have found a semi-reasonable solution.\nHowever, I need to figure out the location of the principal point of the image. Usually, this is near the centre of the image with most cameras. In our case, it isn’t. There are several reasons why this could be the case; for example, the image might have been cropped; however, it’s a little concerning.\nI’m also worried about the camera’s height, only 2.75M above the ground. The camera looks approximately the same height as the aircraft’s roof, which is 7-10m above the ground.\nIn the future, Let’s look at how we can extract more useful information from this image and understand how confident we can be in our solution.\nThanks to Nikolay Mayorov who created the excellent optimisation demo in Scipy that I built upon, you can find the original code here.\nMultiple View Geometry in Computer Vision is an incredible book that I learn more from each time I read it. in particular, for further information, see:\n\nFinite cameras. Page 153, Multiple View Geometry in Computer Vision (Second edition)\nNote: Minimizing geometric error. Page 176, Multiple View Geometry in Computer Vision (Second edition)"
  },
  {
    "objectID": "posts/2019-06-05-Three-Interesting-Papers.html",
    "href": "posts/2019-06-05-Three-Interesting-Papers.html",
    "title": "Three Interesting papers",
    "section": "",
    "text": "The papers, in no particular order, are MixMatch, Selfie and Unsupervised Data Augmentation, however, let’s first discuss why they are exciting.\nIn my daily work, I’m faced with an avalanche of data. Raw data may be cheap, but labelled data is precious, often relying on expensive experiments or busy experts. Even then, when labelled data is available, there is an insatiable demand to do more with it.\nSemi-supervised learning allows us to leverage the raw, unlabelled data to improve our models, reducing the barriers to building a model and democratising AI.\nI’m not going to discuss how the papers are implemented in detail, but I will say that the papers are very promising. They will be rapidly implemented and adapted as a standard part of deep learning workflows.\nIn a sentence, MixMatch uses MixUp and label sharpening (A fancy way of saying “Artificially boosting your model’s confidence”) to propagate labels effectively. My first impression was, “I can’t believe that works,” but then I saw that it decreases error rates 4x when training with small samples on CIFAR-10.\nConversely, Selfie is inspired by the pre-training method in BERT and extends it to CNN’s. At a high level, the pre-training task is analogous to removing pieces from a jigsaw puzzle and asking, “what piece should go in each hole?”. Given the power of transfer learning, this is hugely exciting for many problems where the data you want to train on is very different to what is found in ImageNet.\nFinally, Unsupervised Data Augmentation (UDA) prosecutes the thesis that “better data augmentation can lead to significantly better semi-supervised learning”. As with Selfie and MixMatch, You can apply the techniques used in this paper to image data.\nDeep learning is built on a history of rapidly evolving best practices, including Xavier initialisation, Data Augmentation, One Cycle Policy and MixUp. I hope that adoptions of that MixMatch, Selfie and UDA will soon join this grab bag of best practices."
  },
  {
    "objectID": "posts/2021-04-30-the-gold-standard-redux.html",
    "href": "posts/2021-04-30-the-gold-standard-redux.html",
    "title": "The Gold Standard Redux",
    "section": "",
    "text": "Australia has adopted an isolationist policy in response to the global coronavirus pandemic. Upon arriving in Australia, passengers are isolated in designated hotels for at least 14 days. While this process is highly effective, leaks can and do occur. Fortunately, these are regularly brought under control with effective contact tracing.\nI had previously written a post applying MCMC to infer & estimate the effectiveness of different quarantine programmes. I’m using the same method in this post, building on top of the data provided in this paper."
  },
  {
    "objectID": "posts/2021-04-30-the-gold-standard-redux.html#the-model",
    "href": "posts/2021-04-30-the-gold-standard-redux.html#the-model",
    "title": "The Gold Standard Redux",
    "section": "The Model",
    "text": "The Model\nWe can build a simple but effective model by treating the number of leaks as a binomial variable.\nWe can then infer the probability each processed person results in a leak based on the number of people processed and the number of leaks observed.\nWe need to provide a prior estimate for the probability of processing any one person resulting in a leak. We could guess that 1% of people returning have covid, and 1% of them will result in a leak, for a probability of leak of 0.0001. This seems like a reasonable assumption, and to encode it; I will use a beta distribution.\nWhat is the impact of using a more informed prior than when I last looked at this problem?\nWell, in the case where we have less data, for example, in the case of ACT & Tasmania, we will end up with a smaller posterior estimate because we have used a more informed prior.\nThere will be little change in the case of other states/territories and countries, like NZ & NSW. The data will drown out our choice of prior.\n\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nLet’s create a pandas dataframe containing the data from the paper.\n\nstates = ['NZ','ACT','QLD','NSW','NT','SA','TAS','VIC','WA']\nnumber_of_people_processed = [127_730,765,40_896,128_646,9_180, 13_023, 405, 33_300, 29_383, ]\nnumber_of_leaks_observed = [10, 0, 3, 3, 0, 1, 0, 4, 1]\ncases_identified_in_quarantine = [758, 25, 583, 1_581, 88, 230, 21, 462, 450]\ndata = {'State':states,'number_of_people_processed':number_of_people_processed, 'number_of_leaks_observed':number_of_leaks_observed, 'cases_identified_in_quarantine':cases_identified_in_quarantine}\ndata_df =  pd.DataFrame.from_dict(data)\n\ndisplay(data_df)\n\n\n\n\n\n  \n    \n      \n      State\n      number_of_people_processed\n      number_of_leaks_observed\n      cases_identified_in_quarantine\n    \n  \n  \n    \n      0\n      NZ\n      127730\n      10\n      758\n    \n    \n      1\n      ACT\n      765\n      0\n      25\n    \n    \n      2\n      QLD\n      40896\n      3\n      583\n    \n    \n      3\n      NSW\n      128646\n      3\n      1581\n    \n    \n      4\n      NT\n      9180\n      0\n      88\n    \n    \n      5\n      SA\n      13023\n      1\n      230\n    \n    \n      6\n      TAS\n      405\n      0\n      21\n    \n    \n      7\n      VIC\n      33300\n      4\n      462\n    \n    \n      8\n      WA\n      29383\n      1\n      450\n    \n  \n\n\n\n\n\nnum_chains = 2\nnum_samples = 1_000\nwith pm.Model() as leak_model: \n    #Set up our model\n    prob_of_leak = pm.Beta('prob_of_leak',alpha=1,beta=10_000, shape=9)  \n    number_of_leaks = pm.Binomial('number_of_leaks', n = data_df.number_of_people_processed, p = prob_of_leak, observed = data_df.number_of_leaks_observed)\n    trace = pm.sample(draws = num_samples,chains=num_chains, tune=5_000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [prob_of_leak]\n\n\n\n    \n        \n      \n      100.00% [12000/12000 00:05<00:00 Sampling 2 chains, 1 divergences]\n    \n    \n\n\nSampling 2 chains for 5_000 tune and 1_000 draw iterations (10_000 + 2_000 draws total) took 10 seconds.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n\n\n\nstate_col = []\nprob_of_leak_col = []\nfor i in range(0,9):\n    state = states[i]\n    prob_of_leak = trace.prob_of_leak[:,i]\n    state_col += num_chains*num_samples*[state]\n    prob_of_leak_col += list(prob_of_leak)\n\ndata = {'State':state_col,'prob_of_leak':prob_of_leak_col}\ndf =  pd.DataFrame.from_dict(data)\n\n\nnew_order = df.groupby(by=[\"State\"])[\"prob_of_leak\"].median().sort_values().index\n\n#Visualise the data.\nsns.set_theme(style=\"ticks\")\nf, ax = plt.subplots(figsize=(15, 10))\nax.set_xscale(\"log\")\n\nsns.boxplot(x=\"prob_of_leak\", y=\"State\", data=df, order = new_order, whis=[5, 95], width=0.7,\n            palette=\"vlag\", flierprops = dict(markerfacecolor = '0.0', markersize = 0.1))\n\n\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nax.set(xlabel=\"Probability of failure per person processed\")\nax.set_xlim(1*10**-5, 2* 10**-4)\nax.set(title=\"Comparative probablity of failure by system\");"
  },
  {
    "objectID": "posts/2020-04-10-finding-vanishing-points.html",
    "href": "posts/2020-04-10-finding-vanishing-points.html",
    "title": "Finding Vanishing Points",
    "section": "",
    "text": "Why are we interested in finding vanishing points? Because they allow us to quickly and easily estimate key parameters about a camera. For example, its focal length, optical centre and rotation in 3D space.\nHowever, the first step is to identify the location of the vanishing points in an image.\nSometimes it can be easy to find the location of a vanishing point in an image, for example, when two objects in the real world are very long and quite close together, for example, train tracks. Often it’s a lot more challenging.\nLet’s use a semi-realistic image; I’ve chosen a rendered image from a video game (Counter-Strike). This means we can ignore other factors like Radial Distortion.\nLet’s start, as always, by importing what we will need later.\nNow let’s visualise the scene."
  },
  {
    "objectID": "posts/2020-04-10-finding-vanishing-points.html#maximum-likelihood-estimate",
    "href": "posts/2020-04-10-finding-vanishing-points.html#maximum-likelihood-estimate",
    "title": "Finding Vanishing Points",
    "section": "Maximum Likelihood Estimate",
    "text": "Maximum Likelihood Estimate\nAn ideal camera maps straight lines in 3D space to straight lines in 2D space. We can automate the detection of straight lines using algorithms like a hough transform. However, I’ve used an external program to manually annotate the image’s straight lines. I’ve annotated three sets of lines corresponding to one of 3 different vanishing points.\nWe often use a manhattan world assumption, assuming that there are three different sets of orthogonal parallel lines in the world.\nWe assume that two orthogonal lines form the ground plane. In an image, each set of parallel lines on this plane intersects at the horizon.\n\n\n\nAn annotated image\n\n\nNow, let’s use linear algebra and least mean squares (and the magic of Stack Overflow) to find one of the vanishing points.\n\nJSON = json.loads(open('data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.json','r').read())\n\n\ndef intersect_multiple_lines(P0,P1):\n    \"\"\"P0 and P1 are NxD arrays defining N lines.\n    D is the dimension of the space. This function \n    returns the least squares intersection of the N\n    lines from the system given by eq. 13 in \n    http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf.\n    \"\"\"\n    \n    # generate all line direction vectors \n    n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized\n\n    # generate the array of all projectors \n    projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis]  # I - n*n.T\n    \n    # generate R matrix and q vector\n    R = projs.sum(axis=0)\n    q = (projs @ P0[:,:,np.newaxis]).sum(axis=0)\n\n    # solve the least squares problem for the \n    # intersection point p: Rp = q\n    p = np.linalg.lstsq(R,q,rcond=None)[0]\n\n    return(p)\n\n\ndef load_line_data(point_name):\n    P0 = []\n    P1 = []\n    for shape in JSON['shapes']:\n        points = shape['points']\n        if shape['label'] == point_name:\n            P0.append(points[0])\n            P1.append(points[1])\n            \n    P0 = np.array(P0,dtype=np.float64)\n    P1 = np.array(P1,dtype=np.float64)\n    return(P0,P1)\n\ndef find_vanishing_point(point_name):\n    P0,P1 = load_line_data(point_name)\n    p = intersect_multiple_lines(P0,P1).ravel()\n    return(p)\n\np = find_vanishing_point(point_name='VP1')\n\nNow let’s visualise the location of vanishing point 1.\n\nplt.imshow(img)\nplt.scatter(p[0],p[1],color='r',label='Vanishing point #1')\nplt.legend()\nplt.xlim(0,2560)   \nplt.ylim(1600,0)\nplt.show()"
  },
  {
    "objectID": "posts/2020-04-10-finding-vanishing-points.html#monte-carlo-simulation",
    "href": "posts/2020-04-10-finding-vanishing-points.html#monte-carlo-simulation",
    "title": "Finding Vanishing Points",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nThis is a good start, but I’m interested in how certain we are about the vanishing point.\nFor example, when I annotated the lines, I most likely needed to correct the precise location of each point. We expect that shorter lines would significantly impact the vanishing point location more than longer lines.\nLet’s do a Monte Carlo simulation, to find the distribution of possible vanishing points.\n\ndef monte_carlo_simulation(point_name):\n    P0,P1 = load_line_data(point_name)\n    \n    point_error_magnitude = 1 \n    vanishing_points = []\n    for i in range(0,1000):\n        P0_stochastic = P0 + point_error_magnitude*np.random.randn(P0.shape[0],P0.shape[1])\n        P1_stochastic = P1 + point_error_magnitude*np.random.randn(P1.shape[0],P1.shape[1])\n        p = intersect_multiple_lines(P0_stochastic,P1_stochastic)\n        vanishing_points.append(p)\n    vanishing_points = np.asarray(vanishing_points)\n    return(vanishing_points)\n\n\npoint_name = 'VP1'\nvanishing_points = monte_carlo_simulation(point_name)\n\nNow let’s visualise the distribution of points:\n\nfor p in vanishing_points:\n    plt.scatter(p[0],p[1],color='k',alpha=0.1)\n\nplt.xlim(1350,1390)   \nplt.ylim(640,620)\nplt.grid()\nplt.show()\n\n\n\n\nFor completeness, we can compute the standard deviation of the points in both the x & y-axis.\n\nprint(vanishing_points.std(axis=0).ravel())\n\n[3.5399997  2.03944019]\n\n\nVoilà, we have a standard deviation of ±3.5 pixels in the x-direction and ±2 pixels in the y-direction."
  },
  {
    "objectID": "posts/2020-04-10-finding-vanishing-points.html#appendix",
    "href": "posts/2020-04-10-finding-vanishing-points.html#appendix",
    "title": "Finding Vanishing Points",
    "section": "Appendix",
    "text": "Appendix\nFor completeness, let’s compute the location of the other three vanishing points.\n\nvanishing_points = {}\nfor point_name in ['VP1','VP2','VP3']:\n    vanishing_points[point_name]= find_vanishing_point(point_name)\n\n\nplt.imshow(img)\nfor point_name,color in [('VP1','g'),('VP2','r'),('VP3','b')]:\n    vp = vanishing_points[point_name]\n    print(point_name,vp)\n    plt.scatter(vp[0],vp[1],color=color,label=point_name)\nplt.legend()\nplt.show()\n\nVP1 [1371.89171088  630.42051773]\nVP2 [-10651.53961582    536.68080631]\nVP3 [1272.22463298 7683.01978252]\n\n\n\n\n\n\nplt.imshow(img)\nfor point_name,color in [('VP1','g'),('VP2','r'),('VP3','b')]:\n    vanishing_points = monte_carlo_simulation(point_name)\n    print(point_name, vanishing_points.std(axis=0).ravel())\n    \n    for p in vanishing_points:\n        plt.scatter(p[0],p[1],color=color,alpha=0.1)\n\nplt.show()\n\nVP1 [3.29072559 2.01278422]\nVP2 [1706.16058495   32.11479688]\nVP3 [ 20.31361393 217.79742705]\n\n\n\n\n\nWe can see significant uncertainty in the x component (±1728) of the 2nd vanishing point. This is because of the lack of long parallel lines running left/right across the image."
  },
  {
    "objectID": "posts/2020-04-11-k-from-vanishing-points.html",
    "href": "posts/2020-04-11-k-from-vanishing-points.html",
    "title": "K From Vanishing points",
    "section": "",
    "text": "img = Image.open('data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg')\nplt.imshow(img)\nplt.show()\n\n\n\n\n\nvanishing_points = {'VP1': [1371.892,  630.421],\n                    'VP2': [-10651.54 ,    536.681],\n                    'VP3': [1272.225, 7683.02 ]}\n\n\nplt.imshow(img)\nfor point_name in ['VP1','VP2','VP3']:\n    vp = vanishing_points[point_name]\n    plt.scatter(vp[0],vp[1],label=point_name)\nplt.legend()\nplt.show()\n\n\n\n\nWe now have three different vanishing points. Using these let’s try and use them to gain some insights into the cameras’ relationship with the scene.\nHartley & Zisserman (H&Z) put it best, “Vanishing points are images of points at infinity, and provide orientation (attitude) information in a similar manner to that provided by the fixed stars.”\nH&Z also provide an algorithm (Example 8.27, page 226) to extract the camera calibration matrix K from 3 mutually orthogonal vanishing points.\nLet’s go and implement it in practice.\n\nSome algebra\nH&Z propose a matrix \\(\\omega\\) (omega), which captures the following relationship between the different vanishing points.\n\\(v^T_i \\omega v_j = 0\\)\nWhere:\n\\(\\omega = \\begin{bmatrix} w_1 & 0 & w_2\\\\ 0 & w_1 & w_3\\\\ w_2 & w_3 & w_4 \\end{bmatrix}\\)\nAnd:\n\\(v_j = \\begin{bmatrix} x_1\\\\ y_1\\\\ 1 \\end{bmatrix}\\)\n\\(v_i = \\begin{bmatrix} x_2 \\\\ y_2 \\\\ 1 \\end{bmatrix}\\)\nIf we can find this matrix \\(\\omega\\), then we can then find the camera calibration matrix if we make some assumptions:\n\nZero Skew\nSquare Pixels\n\nFrom H&Z, we have: “K is obtained from \\(\\omega\\) by Cholesky factorization of omega, followed by inversion.”\nFor good practice, we can also normalize the matrix so that the lower right value \\(K_{22}\\) is 1. In python:\nK = np.linalg.inv(np.linalg.cholesky(omega)).T\nK/=K[2,2]\nWorking backwards, we are faced with the task of finding \\(\\omega\\)\nMultiplying through, we find that:\n\\(v^T_i \\omega v_j = x_2(w_1 x_1 + w_2) + y_2(w_1 y_1 + w_3) + w_2 x_1 + w_3 y_1 + w_4\\)\nFactorising:\n\\(v^T_i \\omega v_j= w_1(x_2 x_1 + y_2 y_1) + w_2(x_2 + x_1) + w_3(y_2 + y_1) + w_4\\)\nWe can now find all the coefficients we need for our matrix from each pair of vanishing points.\nWe have three pairs of vanishing points:\n\n1 & 2\n2 & 3\n3 & 1\n\nFrom each pair, we can find a new set of values for \\(w_1\\) to \\(w_4\\).\nStacking them all on top of each other, we end up with the matrix \\(A\\).\n\\(A = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & w_{14}\\\\ w_{21} & w_{22} & w_{23} & w_{24}\\\\ w_{31} & w_{32} & w_{33} & w_{34} \\end{bmatrix}\\)\nIn Python:\ndef generate_A(vanishing_points):\n    A = []\n    for (point_name_1, point_name_2) in [('VP1','VP2'),('VP2','VP3'),('VP3','VP1')]:    \n        vp1 = vanishing_points[point_name_1]\n        vp2 = vanishing_points[point_name_2]\n\n        x1,y1 = vp1\n        x2,y2 = vp2\n\n        w1 = x2*x1 + y2*y1\n        w2 = x2 + x1 \n        w3 = y2 + y1\n        w4 = 1  \n        A.append([w1,w2,w3,w4])\n        \n    A = np.array(A)\n    return(A)\n\ndef generate_A(vanishing_points):\n    A = []\n    for (point_name_1, point_name_2) in [('VP1','VP2'),('VP2','VP3'),('VP3','VP1')]:    \n        vp1 = vanishing_points[point_name_1]\n        vp2 = vanishing_points[point_name_2]\n\n        x1,y1 = vp1\n        x2,y2 = vp2\n\n        w1 = x2*x1 + y2*y1\n        w2 = x2 + x1 \n        w3 = y2 + y1\n        w4 = 1  \n        A.append([w1,w2,w3,w4])\n        \n    A = np.array(A)\n    return(A)\n\ndef compute_K(A):\n    w = scipy.linalg.null_space(A).ravel()\n    \n    w1 = w[0]\n    w2 = w[1]\n    w3 = w[2]\n    w4 = w[3]\n\n    omega = np.array([[w1,0,w2],\n                      [0,w1,w3],\n                      [w2,w3,w4]])\n    K = np.linalg.inv(np.linalg.cholesky(omega)).T\n    K/=K[2,2]\n    return(K)\n\nA = generate_A(vanishing_points)\nK = compute_K(A)\n\n\nprint(K)\n\n[[7.276e+02 0.000e+00 1.327e+03]\n [6.236e-14 7.276e+02 7.060e+02]\n [1.218e-16 0.000e+00 1.000e+00]]\n\n\n\n\nThe Calibration Matrix\nSo now we have the calibration matrix \\(K\\), which gives us three separate pieces of information.\n\nThe Focal length in pixels: \\(K_{11}\\) or \\(K_{22}\\) (728)\nThe x coordinate of the camera optical centre: \\(K_{13}\\) (1327)\nThe y coordinate of the camera optical centre \\(K_{23}\\) (706)\n\n\\(K = \\begin{bmatrix} 728 & 0 & 1327\\\\ 0 & 728 & 706\\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\nplt.scatter(2560/2.0,1600/2.0,color='G')\nplt.scatter(K[0,2],K[1,2],color='R')\nplt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-30-training-data-from-openexr.html",
    "href": "posts/2020-10-30-training-data-from-openexr.html",
    "title": "Training Data From OpenEXR",
    "section": "",
    "text": "At the end of the previous post, I showed how to use Blender to generate depth maps and semantic segmentation maps.\nHowever, this information is in OpenEXR format, and we need to transform it into a form more suitable for training a computer vision model.\nWhile writing this post, I found this post by Tobias Weis to be beneficial.\n\nimport OpenEXR\nimport Imath\nfrom PIL import Image\nimport array\nimport numpy as np\nimport json\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nplt.rcParams['figure.figsize'] = [20,20]\n\n\n\nNext, we can use some boilerplate code to convert the exr file into a Numpy array.\n\ndef exr2numpy(exr_path, chanel_name):\n    '''\n    See:\n    https://excamera.com/articles/26/doc/intro.html\n    http://www.tobias-weis.de/groundtruth-data-for-computer-vision-with-blender/\n    '''\n    file = OpenEXR.InputFile(exr_path)\n    dw = file.header()['dataWindow']\n    size = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)\n    \n    Float_Type = Imath.PixelType(Imath.PixelType.FLOAT)\n    \n    channel_str = file.channel(chanel_name, Float_Type)\n    \n    channel = np.fromstring(channel_str, dtype = np.float32).reshape(size[1],-1)\n    \n    return(channel)\n\n\n\n\n\nexr_path = \"Data/2020-10-30-Training-Data-From-OpenEXR/Index/Image0001.exr\"\nsemantic_index = exr2numpy(exr_path, chanel_name= 'R')\nfig = plt.figure()\nplt.imshow(semantic_index)\nplt.colorbar()\nplt.show()\n\nDeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  channel = np.fromstring(channel_str, dtype = np.float32).reshape(size[1],-1)\n\n\n\n\n\n\n\n\n\nexr_path = \"Data/2020-10-30-Training-Data-From-OpenEXR/Depth/Image0001.exr\"\ndepth = exr2numpy(exr_path, chanel_name= 'R')\nfig = plt.figure()\nplt.imshow(depth)\nplt.colorbar()\nplt.show()\n\nDeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  channel = np.fromstring(channel_str, dtype = np.float32).reshape(size[1],-1)\n\n\n\n\n\n\n\n\n\ndef EncodeToSRGB(v):\n    return(np.where(v<=0.0031308,v * 12.92, 1.055*(v**(1.0/2.4)) - 0.055))\n\nchannels = []\nchannel_names = ['R','G','B']\nexr_path = \"Data/2020-10-30-Training-Data-From-OpenEXR/Image/Image0001.exr\"\nfor channel_name in channel_names:\n    channel = exr2numpy(exr_path, channel_name)\n    channels.append(EncodeToSRGB(channel))\n    \nRGB = np.dstack(channels)\nfig = plt.figure()\nplt.imshow(RGB)\nplt.colorbar()\nplt.show()\n\nDeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  channel = np.fromstring(channel_str, dtype = np.float32).reshape(size[1],-1)\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\nNow we can create bounding boxes for each object in the image. Depending on what we want to do next, we could generate annotations in COCO format.\n\n# Create figure and axes\nfig, ax = plt.subplots(1)\n# Display the image\nax.imshow(RGB)\n\nfor i in np.unique(semantic_index):\n    #index 0 is the background\n    if i!=0:\n        #Find the location of the object mask\n        yi, xi = np.where(semantic_index == i)\n        \n        # Create a Rectangle patch\n        rect = Rectangle((np.min(xi), np.min(yi)), np.max(xi) - np.min(xi), np.max(yi) - np.min(yi), linewidth=2, edgecolor='r', facecolor='none', alpha=0.8)\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nimg = Image.fromarray(np.uint8(RGB*255))\nimg.save('image.png')\n\n\n\n\n\nshapes = []\nfor i in np.unique(semantic_index):\n    #index 0 is the background\n    if i!=0:\n        #Find the location of the object mask\n        yi, xi = np.where(semantic_index == i)\n\n        #Print the index of the object, and it's bounding box\n        shapes.append({ \"label\": \"DRAGON\",\n                       \"points\": [[int(np.min(xi)), int(np.min(yi))], [int(np.max(xi)), int(np.max(yi))]],\n                       \"group_id\": None,\n                       \"shape_type\": \"rectangle\",\n                       \"flags\": {}})\n        \n        \nJSON = {\n    \"version\": \"4.5.6\",\n    \"flags\": {},\n    \"imagePath\": \"image.png\",\n    \"imageData\": None,\n    \"imageHeight\": 512,\n    \"imageWidth\": 512\n}\n\n\nJSON[\"shapes\"] = shapes\n\nJSON = json.dumps(JSON, sort_keys=True, indent=4)\n\nf = open('image.json','w')\nf.write(JSON)\nf.close()"
  },
  {
    "objectID": "posts/2020-12-18-A-Playground-In-Nuketown.html",
    "href": "posts/2020-12-18-A-Playground-In-Nuketown.html",
    "title": "A playground in Nuketown",
    "section": "",
    "text": "One exciting data source is video games, and I’ve settled on the fast, frantic, mega-franchise Call of Duty: Black Ops Cold War. Nuketown84 is one of the maps/levels, which presents a gritty, decaying ambience, perfect for learning more about how we can apply computer vision in practice. You can see some of the action here.\nIf our goal is to recognise where we are in the world based on what we can see, then our starting point is having some ground truth data (where we are). Capturing this ground truth data is an exciting challenge, which I will slowly build up over the next few posts.\nOne strategy is to infer the player’s position from the mini-map and the on-screen compass. Combining different measurements over time, we should understand where the player is in the world.\n\n\n\n_config.yml"
  },
  {
    "objectID": "posts/2020-10-16-Meshes-And-Materials-In-Blender.html",
    "href": "posts/2020-10-16-Meshes-And-Materials-In-Blender.html",
    "title": "Materials and meshes in Blender",
    "section": "",
    "text": "blender --background --python myscript.py\nLet’s walk through what myscript.py could look like:\nbpy.data.objects.remove(bpy.data.objects['Cube'], do_unlink = True)\nbpy.ops.mesh.primitive_plane_add(size=1000,location=(0, 0, 0), scale=(1, 1, 1))\n\n\ndef create_bunny():\n        # Load the mesh\n        bpy.ops.import_scene.obj(filepath=os.getcwd()+\"/stanford_bunny.obj\")\n        ob = bpy.data.objects[\"stanford_bunny\"]\n        ob.scale = (10,10,10)\n        ob.location = (1,0,-0.35)\n        ob.name = 'Bunny'\n        \n        #Perform subdivision\n        bevel_mod = ob.modifiers.new('Subsurf', 'SUBSURF')\n        bevel_mod.render_levels = 3\n\n\n\ndef create_material(object_name,material_name):\n        mat = bpy.data.materials.new(name=material_name)\n        bpy.data.objects[object_name].active_material = mat\n        \n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        return(nodes)\n\ndef create_ground_plane_material(object_name,material_name):\n        nodes = create_material(object_name,material_name)\n        \n        nodes[\"Principled BSDF\"].inputs['Base Color'].default_value = (0.7,0.7,0.7,1)\n        nodes[\"Principled BSDF\"].inputs['Specular'].default_value = 1\n        nodes[\"Principled BSDF\"].inputs['Roughness'].default_value = 0.1\n\ncreate_ground_plane_material(\"Plane\",\"Plane_material\")\ndef create_bunny_material(object_name,material_name):\n        nodes = create_material(object_name,material_name)\n        \n        nodes[\"Principled BSDF\"].inputs['Base Color'].default_value =  (0.603828, 1, 0.707399, 1)\n        nodes[\"Principled BSDF\"].inputs['Roughness'].default_value = 0.1\n        nodes[\"Principled BSDF\"].inputs['IOR'].default_value = 1.5\n        nodes[\"Principled BSDF\"].inputs['Transmission'].default_value = 1\n        nodes[\"Principled BSDF\"].inputs['Transmission Roughness'].default_value = 0.75\n\ncreate_bunny_material(\"Bunny\",\"Bunny_Material\")\n\n\n\ndef configure_light():\n        bpy.data.objects[\"Light\"].data.type = 'AREA'\n        bpy.data.objects[\"Light\"].scale = (10,10,1)\n        bpy.data.objects[\"Light\"].location = (0,0,6)\n        bpy.data.objects[\"Light\"].rotation_euler = (0,0,0)\n\ndef configure_camera():\n        bpy.data.objects[\"Camera\"].location = (0.7, -4, 3)\n        bpy.data.objects[\"Camera\"].rotation_euler = (np.radians(60),0,0)\n\ndef configure_render():\n        bpy.context.scene.render.engine = 'CYCLES'\n        bpy.context.scene.render.filepath = os.getcwd()+\"/render.png\"\n        bpy.context.scene.render.resolution_x = 1600\n        bpy.context.scene.render.resolution_y = 1200\n        bpy.context.scene.cycles.samples = 2560\n\n\nconfigure_light()\nconfigure_camera()\nconfigure_render()\n\nbpy.ops.render.render(write_still=True)"
  },
  {
    "objectID": "posts/2020-10-16-Meshes-And-Materials-In-Blender.html#the-results",
    "href": "posts/2020-10-16-Meshes-And-Materials-In-Blender.html#the-results",
    "title": "Materials and meshes in Blender",
    "section": "The Results",
    "text": "The Results\n\n\n\n_config.yml"
  },
  {
    "objectID": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html",
    "href": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html",
    "title": "Working with multiple coordinate systems from ArUco tags",
    "section": "",
    "text": "In a previous post, I’ve shown how to determine the position and orientation of a camera using an ArUco code.\nHowever, when we have multiple ArUco tags, we might want to find the relationships between all of them.\nThis will require us to work with different coordinate frames and transform coordinates from one frame to another.\nI’ve created a synthetic example, as below.\nIt consists of 2 tags, with the ArUco indexes of 0 and 1.\nTag 0 remains untransformed.\nTag 1 has undergone two transformations.\n\nTranslation by [3, 4, 0].\nRotation by 37 degrees around its Z-axis.\n\nThe camera itself is located at [5, -14, 10].\n\nLet’s now detect the ArUco tags in the image and then work out the position and orientation of the camera, according to each of the tag’s coordinate systems."
  },
  {
    "objectID": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#extracting-aruco-tags",
    "href": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#extracting-aruco-tags",
    "title": "Working with multiple coordinate systems from ArUco tags",
    "section": "Extracting ArUco tags",
    "text": "Extracting ArUco tags\n\nimport numpy as np\nimport cv2\nimport cv2.aruco as aruco\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as Rot\n\nplt.rcParams['figure.figsize'] = [10,10]\nnp.set_printoptions(precision=2, suppress=True)\n\nBecause we generated this image synthetically, we can quickly generate the Intrinsic matrix K. We also know that there is no distortion present.\n\nfocal_length_mm = 50\nwidth_pixels = 800.0\nheight_pixels = 600.0\nwidth_mm = 36.0\n\npixels_per_mm = width_pixels/width_mm\nfocal_length_pixels = pixels_per_mm * focal_length_mm\n\n\nK = np.array([[focal_length_pixels, 0.0 , width_pixels/2],\n              [0.0   , focal_length_pixels, height_pixels/2],\n              [0.0   , 0.0   ,    1.0]])\n\ndist = np.array([[0.0],[0.0],[0.0],[0.0],[0.0]])\n\nWe also know the dimensions of the target. Let’s set the origin of the target’s coordinate system to be in the centre of the target.\n\npoints_3d = np.array([[0.0,1.2,0.0],\n                      [1.2,1.2,0.0],\n                      [1.2,0.0,0.0],\n                      [0.0,0.0,0.0]])\n\npoints_3d[:,0:2] -= 0.6\n\narucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\narucoParams = cv2.aruco.DetectorParameters_create()\n\nNow we can detect the tags and visualise their coordinate systems.\n\nf_str = 'data/2021-03-25-Working-with-multiple-coordinate-systems-from-ArUco-tags/120.png'\nimg = cv2.imread(f_str)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Change grayscale\n(corners, ids, rejected) = cv2.aruco.detectMarkers(gray, arucoDict, parameters=arucoParams)\n\ntags = {}\nfor i in range(len(corners)):\n    tag_id = int(ids[i])\n    points_2d = corners[i].squeeze()\n\n    ret, rvec, tvec = cv2.solvePnP(objectPoints = points_3d, imagePoints = points_2d, cameraMatrix = K, distCoeffs = dist, flags = cv2.SOLVEPNP_IPPE_SQUARE)\n\n    r = Rot.from_rotvec(rvec[:,0])\n    R = r.as_matrix()\n    t = np.array(tvec)\n\n    #Find the camera's center, in the tag's coordinate system.\n    C = - np.dot(R.T,tvec)\n    \n    #Find the rotation of the camera, in the tag's coordinate system.\n    R = Rot.from_matrix(R.T).as_matrix()\n    \n    tags[tag_id] = (R,C)\n    \n    \n    # project 3D points to image plane\n    axis = np.float32([[0,0,0],[1,0,0], [0,1,0], [0,0,1]]).reshape(-1,3)\n    imgpts, jac = cv2.projectPoints(axis, rvec, tvec, K, dist)\n    imgpts = np.squeeze(imgpts)\n    \n    \n    plt.imshow(img[:,:,::-1])\n    plt.plot([imgpts[0,0],imgpts[1,0]],[imgpts[0,1], imgpts[1,1]], color='r', alpha=0.75, linewidth=5, label='X Axis')\n    plt.plot([imgpts[0,0],imgpts[2,0]],[imgpts[0,1], imgpts[2,1]], color='g', alpha=0.75, linewidth=5, label='Y Axis')\n    plt.plot([imgpts[0,0],imgpts[3,0]],[imgpts[0,1], imgpts[3,1]], color='b', alpha=0.75, linewidth=5, label='Z Axis')\n\n    plt.title(f'Tag {tag_id}')\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#changing-coordinate-frames",
    "href": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#changing-coordinate-frames",
    "title": "Working with multiple coordinate systems from ArUco tags",
    "section": "Changing Coordinate Frames",
    "text": "Changing Coordinate Frames\nFrom the above, we can see two different coordinate systems, one per tag.\nLet’s take a bird’s eye view of the situation:\n\nPreviously, we found the rotation and the camera’s position in the coordinate frame of each tag.\n\nR_0, C_0 = tags[0]\n\nprint(R_0)\nprint(C_0)\n\n[[ 1.   -0.   -0.  ]\n [ 0.   -0.49  0.87]\n [-0.   -0.87 -0.49]]\n[[  4.61]\n [-15.17]\n [ 10.25]]\n\n\n\nR_1, C_1 = tags[1]\n\nprint(R_1)\nprint(C_1)\n\n[[ 0.78  0.3  -0.54]\n [ 0.62 -0.37  0.69]\n [ 0.01 -0.88 -0.48]]\n[[ 13.2 ]\n [-14.46]\n [ 10.  ]]\n\n\n\nGiven the information we have, what if we want to find the position and orientation of Tag 1 with respect to Tag 0’s coordinate frame?\nWorking with all of our measurements in a single frame will provide us with a singular, coherent view of our world.\nFortunately, this is relatively easy.\nWe can first find the relative rotation between the two tags.\n\n#Rotatation of Tag 1 with respect to Tag 0\nR_delta = np.dot(R_0, R_1.T)\n\nOnce we have done this, we can transform C_1 into Tag 0’s coordinate system and then subtract it from C_0, forming the vector T_1\n\n#Position of the tag with respect to the origin\nT_1 = C_0 - np.dot(R_delta,C_1)\n\narray([[ 3.08],\n       [ 4.21],\n       [-0.02]])\n\n\nWe can see that Tag 1 is located at [3.08, 4.21, -0.02]. In reality, the tag is located at [3, 4, 0]. However, there is some noise inherent in the process.\nTag 1 was also rotated by an angle (37 degrees) around its Z-axis. We can measure this angle as follows:\n\nprint(np.degrees(np.arccos(R_delta[1,1])))\n\n38.19945877152531\n\n\nAs before, there is a small error between what we have measured, and reality."
  },
  {
    "objectID": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#conclusion",
    "href": "posts/2021-03-25-working-with-multiple-coordinate-systems-from-aruco-tags.html#conclusion",
    "title": "Working with multiple coordinate systems from ArUco tags",
    "section": "Conclusion",
    "text": "Conclusion\nWe can see how we can transform measurements from two different tags, into a single coordinate frame. In another post, I will look at what happens when we have more photos and end up with potentially contradictory measurements due to the measurement noise we will invariably encounter."
  },
  {
    "objectID": "posts/2020-12-19-fft-phase-correlation.html",
    "href": "posts/2020-12-19-fft-phase-correlation.html",
    "title": "FFT Phase Correlation",
    "section": "",
    "text": "Finding the geometric relationship between two images is a common problem in computer vision.\nIn our case, if we can find the relationship between the mini-map in one frame and the mini-map in another frame, we will understand how the player is moving around the map.\nOne method that can be highly effective, if only a translation exists between two images, is to perform a 2D cross-correlation. This is rarely the case when we try to compare the mini-map between two frames because the play is free to rotate. However, I will adapt this technique in later posts to solve the problem of rotation.\n\nWhen there is alignment between the two images, the correlation value will be maximum. While this is simple and effective, it has the potential to be computationally expensive, given the large number of operations required.\nHowever, it’s possible to solve this problem more efficiently using the Fast Fourier Transform (FFT)."
  },
  {
    "objectID": "posts/2020-12-19-fft-phase-correlation.html#the-algorithm",
    "href": "posts/2020-12-19-fft-phase-correlation.html#the-algorithm",
    "title": "FFT Phase Correlation",
    "section": "The Algorithm",
    "text": "The Algorithm\nIn particular, we can take advantage of convolution theorm. In particular, The Fourier transform of a convolution of two signals is the pointwise product of their Fourier transforms. In other words, convolution in the spatial domain is multiplication in the frequency domain.\nWikipedia has a great page that goes into more detail, but let’s step through implementation in Python.\n\nCreating test data\nFirst off, let’s load in a bunch of libraries.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport skimage.io\nfrom skimage import filters\nfrom skimage.color import rgb2gray\nfrom skimage.filters import window, difference_of_gaussians\n\nimport scipy \n\nplt.rcParams['figure.figsize'] = [10, 10]\n\nAs a test, I will be using an image from the popular video game Call of Duty: Black Ops Cold War.\n\nframe = skimage.io.imread('data/2020-12-19-FFT-Phase-Correlation/Frame.jpg')\n\nplt.imshow(frame)\nplt.show()\n\n\n\n\nIn particular, I will focus on the mini-map, which you can see in the top left of the image above.\n\nimg1 = rgb2gray(frame[50:165,55:170,::-1])\nplt.title('Mini Map')\nplt.imshow(img1,cmap='gray')\nplt.show()\n\n\n\n\nLet’s create some synthetic data. Using scipy.ndimage.fourier_shift, we can create a new image with a known shift.\n\nshift_x = 10\nshift_y = 20\n\n#Shift the img1 10 pixels to the right and 20 pixels down.\ntranslated = np.roll(img1, shift_y, axis=0)\nimg2 = np.roll(translated, shift_x, axis=1)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(img1)\nax[1].set_title(\"Image 2 \")\nax[1].imshow(img2)\nplt.show()\n\n\n\n\n\n\nFinding the shift\nFollowing the method described here:\nApply a window function (e.g., a Hamming window) on both images to reduce edge effects.\n\n# window images\nimg1_wimage = img1 * window('hann', img1.shape)\nimg2_wimage = img2 * window('hann', img2.shape)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1 windowed\")\nax[0].imshow(img1_wimage)\nax[1].set_title(\"Image 2 windowed\")\nax[1].imshow(img2_wimage)\nplt.show()\n\n\n\n\nCalculate the discrete 2D Fourier transform of both images.\n\n# Compute the 2D FFT of each image\nimg1_fs = np.fft.fft2(img1_wimage)\nimg2_fs = np.fft.fft2(img2_wimage)\n\nCalculate the cross-power spectrum by taking the complex conjugate of the second result, multiplying the Fourier transforms together elementwise, and normalizing this product elementwise.\n\n#Compute the corss power spectrum\ncross_power_spectrum = (img1_fs * img2_fs.conj()) / np.abs(img1_fs * img2_fs.conj())\n\nObtain the normalized cross-correlation by applying the inverse Fourier transform.\n\nr = np.abs(np.fft.ifft2(cross_power_spectrum))\n\nDetermine the location of the peak in r.\n\nr = np.fft.fftshift(r)\n\nplt.title('Cross Correlation Map')\nplt.imshow(r)\nplt.grid()\nplt.show()\n\n\n\n\nWe can see the cross-correlation peak at (47, 37); usually, the peak would not be so well defined.\n\n#Find the location of the peak\n[py,px] = np.argwhere(r==r.max())[0]\n\ncx,cy = 57,57\nshift_x = cx - px\nshift_y = cy - py\n\nprint(f'Shift measured X:{shift_x}, Y:{shift_y}')\n\nShift measured X:10, Y:20\n\n\nVoilà!\nWe can now measure the shift between two images with only a 2D translation between them.\nIn a future post, I’m going to look at what to do if there is a rotation and a translation."
  },
  {
    "objectID": "posts/2020-12-22-the-log-polar-transform-in-practice.html",
    "href": "posts/2020-12-22-the-log-polar-transform-in-practice.html",
    "title": "The Log Polar Transform In Practice",
    "section": "",
    "text": "This post aims to develop a Python pipeline to recover the rotation and translation between two images that have undergone a rigid euclidean transformation.\nWe will apply this to the problem of determining both the rotation and the translation between two different mini-maps.\n\nWhile writing this post, I borrowed heavily from this example, as well as this fantastic series of posts. Both are well worth a read and go into significantly more detail. Finally, the original paper, which first described the method, is also worth a read."
  },
  {
    "objectID": "posts/2020-12-22-the-log-polar-transform-in-practice.html#the-pipeline",
    "href": "posts/2020-12-22-the-log-polar-transform-in-practice.html#the-pipeline",
    "title": "The Log Polar Transform In Practice",
    "section": "The Pipeline",
    "text": "The Pipeline\nAt a high level, our pipeline is as follows:\nFirst, we find the rotation: * Preprocess Images:     * Apply Difference of Gaussians Filter (DOG)     * Window Function (WINDOW) * Compute Power Spectral Density of images (PSD) * Compute the Log Polar Transform of PSD (LPT) * Find translation in Log Polar Domain using Cross-Correlation (XCOR)\n\nOnce we have found the rotation, we can then find the translation: * Rotate one of the images (ROTATE) * Find translation using Cross-Correlation (XCOR)\n\nNow let’s go into the details of the implementation.\nLet’s start by importing the libraries we will need.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport skimage.io\nimport skimage.transform\n\nfrom skimage.color import rgb2gray\nimport skimage.filters\n\nplt.rcParams['figure.figsize'] = [10, 10]\n\nThis function compares two images/arrays and checks how well they match for any x/y shift. It’s a simplified version of a function from skimage (skimage.registration.phase_cross_correlation), and you can see the original source code here. In a previous post I explained how it works in more detail.\n\ndef phase_cross_correlation(reference_image, moving_image):\n    # Adapted from skimage.registration.phase_cross_correlation\n    src_freq = np.fft.fft2(reference_image)\n    target_freq = np.fft.fft2(moving_image)\n\n    # Whole-pixel shift - Compute cross-correlation by an IFFT\n    shape = src_freq.shape\n    image_product = src_freq * target_freq.conj()\n    cross_correlation = np.fft.ifft2(image_product)\n    \n    # Locate maximum\n    maxima = np.unravel_index(np.argmax(np.abs(cross_correlation)), cross_correlation.shape)\n    midpoints = np.array([np.fix(axis_size / 2) for axis_size in shape])\n\n    shifts = np.array(maxima, dtype=np.float64)\n    shifts[shifts > midpoints] -= np.array(shape)[shifts > midpoints]\n    \n    return np.abs(cross_correlation), shifts\n\nLet’s start by loading the image, converting it to grayscale, and then mean-centering it.\n\nframe = skimage.io.imread('data/2020-12-20-The-Log-Polar-Transform/Frame.jpg')\nimg1 = rgb2gray(frame[50:165,55:170])\nimg1 -= img1.mean()\n\nWe can now create a synthetic example by first translating then rotating the image we loaded. I’ve arbitrarily chosen to rotate the image by 30 degrees clockwise and shift it down and to the right by 10 and 20 pixels, respectively.\n\nangle_degrees = 30\nshift_y_pixels = 10\nshift_x_pixels = 20\n\ntranslated = np.roll(img1, shift_y_pixels, axis=0)\ntranslated = np.roll(translated, shift_x_pixels, axis=1)\n\nimg2 = skimage.transform.rotate(translated, angle_degrees)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(img1)\nax[1].set_title(\"Image 2 \")\nax[1].imshow(img2)\nplt.show()"
  },
  {
    "objectID": "posts/2020-12-22-the-log-polar-transform-in-practice.html#finding-the-rotation",
    "href": "posts/2020-12-22-the-log-polar-transform-in-practice.html#finding-the-rotation",
    "title": "The Log Polar Transform In Practice",
    "section": "Finding the Rotation",
    "text": "Finding the Rotation\nNow that we have two images, we can start finding the rotation between them.\nWe can start by performing Difference of Gaussians filtering as a form of feature engineering/preprocessing.\nNext, we window the images, which will help reduce the impact of the edges of the image on the frequency spectrum of the image.\n\n# First, band-pass filter both images\nimg1_dog = skimage.filters.difference_of_gaussians(img1, 1, 4)\nimg2_dog = skimage.filters.difference_of_gaussians(img2, 1, 4)\n\n# window images\nwimage_1 = img1_dog * skimage.filters.window('hann', img1.shape)\nwimage_2 = img2_dog * skimage.filters.window('hann', img2.shape)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(wimage_1)\nax[1].set_title(\"Image 2 \")\nax[1].imshow(wimage_2)\nplt.show()\n\n\n\n\nNow we compute each image’s Power Spectral Density (PSD).\nWhile it’s not immediately obvious, if we look carefully at the PSD of each image, we can notice that there is a slight (30 degree) rotation between the two.\nThe beauty of the PSD is that it is, to some degree, invariant to the translation of the two images.\n\n# work with shifted FFT magnitudes\nimage1_fs = np.abs(np.fft.fftshift(np.fft.fft2(wimage_1)))**2\nimage2_fs = np.abs(np.fft.fftshift(np.fft.fft2(wimage_2)))**2\n\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1 PSD\")\nax[0].imshow(np.log(image1_fs),vmin=0,vmax=np.log(image1_fs).max())\n\n\nax[1].set_title(\"Image 2 PSD\")\nax[1].imshow(np.log(image2_fs),vmin=0,vmax=np.log(image1_fs).max())\nplt.show()\n\n\n\n\nNow we can compute the Log-Polar Transform (LPT) of each PSD. This transform will allow us to measure the rotation between the two PSDs.\n\n# Create log-polar transformed FFT mag images and register\nradius = image1_fs.shape[0] // 8  # only take lower frequencies\nwarped_image1_fs = skimage.transform.warp_polar(image1_fs, radius=radius, output_shape=image1_fs.shape,\n                             scaling='log')\n\nwarped_image2_fs = skimage.transform.warp_polar(image2_fs, radius=radius, output_shape=image1_fs.shape,\n                           scaling='log')\n\nlabels =  np.arange(0,360,60)\ny = labels/(360/warped_image1_fs.shape[0])\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(warped_image1_fs)\nax[0].set_yticks(y, minor=False)\nax[0].set_yticklabels(labels)\nax[0].set_ylabel('Theta (Degrees)')\n\n\nax[1].set_title(\"Image 2\")\nax[1].imshow(warped_image2_fs)\nax[1].set_yticks(y, minor=False)\nax[1].set_yticklabels(labels)\nplt.show()\n\n\n\n\nNow we can compute the cross-correlation of the two transformed PSDs. This will allow us to find the translation between them that best matches, which will enable us to compute the best matching rotation.\n\nwarped_image1_fs = warped_image1_fs[:image1_fs.shape[0] // 2, :]  # only use half of FFT, because it's symetrical\nwarped_image2_fs = warped_image2_fs[:image1_fs.shape[0] // 2, :]\n\ncross_correlation, shifts = phase_cross_correlation(warped_image1_fs, warped_image2_fs)\n\nlabels =  [0, 30, 60, 90, -60, -30]\ny = np.arange(0,180,30) / (180 / cross_correlation.shape[0])\n\nplt.title (\"Rotation Cross Correlation Map\")\nplt.imshow(cross_correlation)\nplt.xlabel('Scale')\nplt.yticks(y, labels, rotation='vertical')\nplt.ylabel('Theta (Degrees)')\nplt.show()\n\n\n\n\nThe log-polar transform allows us to find both changes in rotation and scale, but we know that the scale between the two images is constant. This corresponds to the left-most column of the map, and if we plot this in 2D, we can see a peak at 30 degrees.\n\ncross_correlation_score = cross_correlation[:,0].ravel()\nplt.plot(cross_correlation_score)\n\n\nlabels =  [0, 30, 60, 90, -60, -30]\nx = np.arange(0,180,30) / (180 / cross_correlation.shape[0])\n\nplt.title('Angle vs Cross Correlation Score')\nplt.xticks(x, labels)\nplt.xlabel('Theta (Degrees)')\nplt.ylabel('Cross Correlation Score')\nplt.grid()\nplt.show()\n\n\n\n\n\nrecovered_angle_degrees = (360 / image1_fs.shape[0]) * np.argmax(cross_correlation_score)\n\nprint(recovered_angle_degrees)\nif recovered_angle_degrees > 90:\n    recovered_angle_degrees = recovered_angle_degrees - 180\n\nprint(f\"Computed rotation: {recovered_angle_degrees:.1f} degrees\")\n\n31.304347826086957\nComputed rotation: 31.3 degrees"
  },
  {
    "objectID": "posts/2020-12-22-the-log-polar-transform-in-practice.html#finding-the-translation",
    "href": "posts/2020-12-22-the-log-polar-transform-in-practice.html#finding-the-translation",
    "title": "The Log Polar Transform In Practice",
    "section": "Finding the Translation",
    "text": "Finding the Translation\nNow it’s time to find the rotation. Our first task is to rotate the img2, so that its orientation matches img1.\n\nimg2_rotated = skimage.transform.rotate(img2, -recovered_angle_degrees)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(img1)\nax[0].grid()\nax[1].set_title(\"Image 2 \")\nax[1].imshow(img2_rotated)\nax[1].grid()\nplt.show()\n\n\n\n\nNow that the orientations of both images match, we can now find the translation between the two images using cross-correlation. As before, we first window both images to reduce edge effects.\n\nwimage_1 = img1 * skimage.filters.window('hann', img1.shape)\nwimage_2_rotated = img2_rotated * skimage.filters.window('hann',img2.shape)\ncross_correlation, shift = phase_cross_correlation(wimage_1, wimage_2_rotated)\n\nplt.title (\"Translation Cross Correlation Map\")\nplt.imshow(cross_correlation)\nplt.xlabel('x shift (pixels)')\nplt.ylabel('y shift (pixels)')\n\nlabels =  [0,10,20,30,0,-10,-20,-30]\ntick_location = [0,10,20,30,115,115-10,115-20,115-30]\n\nplt.xticks(tick_location, labels)\nplt.yticks(tick_location, labels)\nplt.show()\n\n\n\n\nWe can see a peak in the bottom right corner of the cross-correlation map, which corresponds to a shift down and the right by 10 and 20 pixels, respectively, between the first and second image. We can now correct the second image to perfectly align with the first image.\n\nshift_x =  int(shift[1])\nshift_y =  int(shift[0])\n\ntranslated = np.roll(img2_rotated, shift_y, axis=0)\ntranslated = np.roll(translated, shift_x, axis=1)\n\nfig, axes = plt.subplots(1, 2)\nax = axes.ravel()\nax[0].set_title(\"Image 1\")\nax[0].imshow(img1)\nax[0].grid()\nax[1].set_title(\"Image 2 \")\nax[1].imshow(translated)\nax[1].grid()\nplt.show()"
  },
  {
    "objectID": "posts/2020-12-22-the-log-polar-transform-in-practice.html#conclusion",
    "href": "posts/2020-12-22-the-log-polar-transform-in-practice.html#conclusion",
    "title": "The Log Polar Transform In Practice",
    "section": "Conclusion",
    "text": "Conclusion\nWe can now find the rotation and translation between two different images. I want to emphasise that this is one of many ways to solve the problem of finding the rotation and translation between two images."
  },
  {
    "objectID": "posts/2020-03-13-rq-decomposition-in-practice.html",
    "href": "posts/2020-03-13-rq-decomposition-in-practice.html",
    "title": "RQ Decomposition In Practice",
    "section": "",
    "text": "Given a camera projection matrix, \\(P\\), we can decompose it into a \\(K\\) (Camera Matrix), \\(R\\) (Rotation Matrix) and \\(C\\) (Camera centroid location) matrix.\nIE, given we have \\(P = K[R|-RC]\\), We want to find \\(K\\), \\(R\\) and \\(C\\).\nThe method is described in Multiple View Geometry in Computer Vision (Second Edition), on page 163; however, let’s turn it into a practical Python implementation.\nLet’s follow along with an example from the book.\n\nimport numpy as np\nimport scipy.linalg\nnp.set_printoptions(precision=2)\n\n\nP = np.array([[3.53553e+2,  3.39645e+2, 2.77744e+2, -1.44946e+6],\n              [-1.03528e+2, 2.33212e+1, 4.59607e+2, -6.3252e+5],\n              [7.07107e-1, -3.53553e-1, 6.12372e-1, -9.18559e+2]])\n\nSo, we have: \\(P = [M | −MC]\\)\nM can be decomposed as \\(M=KR\\) using the RQ decomposition.\nM = P[0:3,0:3]\nK, R = linalg.rq(M)\nSo far, so good.\nNow things get a little more complex.\nWe want to find a Camera matrix with a positive diagonal, giving positive focal lengths.\nHowever, if this doesn’t happen, we can adjust the sign of each column for both the \\(K\\) and \\(R\\) matrix.\nT = np.diag(np.sign(np.diag(K)))\nif linalg.det(T) < 0:\n    T[1,1] *= -1\n\nK = np.dot(K,T)\nR = np.dot(T,R)\nFinally, we can find the Camera Center (\\(C\\)).\nWe have \\(P_4\\), the 4th column of \\(P\\).\n\\(P_4 = −MC\\)\nFrom this, we can find \\(C = {-M}^{-1} P_4\\)\n\ndef factorize(P):\n    M = P[:,0:3]\n    \n    K,R = scipy.linalg.rq(M)\n    \n    T = np.diag(np.sign(np.diag(K)))\n    \n    if scipy.linalg.det(T) < 0:\n        T[1,1] *= -1\n    \n    K = np.dot(K,T)\n    R = np.dot(T,R)\n    \n    C = np.dot(scipy.linalg.inv(-M),P[:,3])\n    return(K,R,C)\n    \nK,R,C = factorize(P)\n\nprint('K')\nprint(K)\n\nprint('R')\nprint(R)\n\nprint('C')\nprint(C)\n\nK\n[[468.16  91.23 300.  ]\n [  0.   427.2  200.  ]\n [  0.     0.     1.  ]]\nR\n[[ 0.41  0.91  0.05]\n [-0.57  0.22  0.79]\n [ 0.71 -0.35  0.61]]\nC\n[1000.01 2000.   1499.99]\n\n\nVoilà!\nThis presentation is a great read and provides a good overview of the RQ and QR decompositions."
  },
  {
    "objectID": "posts/2021-01-03-Number-Extraction-With-Tesseract.html",
    "href": "posts/2021-01-03-Number-Extraction-With-Tesseract.html",
    "title": "Number extraction with Tesseract",
    "section": "",
    "text": "While comparing two different mini-maps can tell us the change in angle/heading (\\(\\omega\\)) between them, we can determine the player’s heading (\\(\\theta\\)) via the compass. This can be seen at the top centre of the screen (107 degrees).\n\n\n\n_config.yml\n\n\nBy using Optical Character Recognition (OCR), we can read this heading, digitising the player’s current heading on a frame by frame basis. I’ve chosen to use Tesseract, a powerful open-source library for OCR. In Python, we can use Pytesseract, a wrapper around Tesseract.\nYou can find the video I’m digitising here.\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pytesseract\nimport re \n\nplt.rcParams['figure.figsize'] = [10, 10]"
  },
  {
    "objectID": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#pre-processing",
    "href": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#pre-processing",
    "title": "Number extraction with Tesseract",
    "section": "Pre-processing",
    "text": "Pre-processing\nLet’s load the first frame:\ndef load_frame(frame_number):\n    cap.set(1, frame_number-1)\n    res, frame = cap.read()\n    return(frame)\nNow let’s pre-process the frame by:\n\nCropping the image down so that it only contains the number.\nResize the image so that it’s larger.\nConverting the image to grayscale and then inverting.\n\nEach of these steps helps improve the accuracy of Tesseract.\n\ndef preprocess_frame(img):\n    img = img[10:50,610:670,::-1]\n    img = cv2.resize(img,(600,400))\n    img = 255 - cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return(img)\nOnce Tesseract has processed the image, we want to extract both the angle and how confident Tesseract was in its detection from the metadata. Often when Tesseract digitises a number, it will include other characters, which we need to strip out using “re.sub(”[^0-9^]“,”“,text)”.\ndef extract_angle_confidence(result_dict):\n    angle = np.NaN\n    confidence = np.NaN\n    for i in range(0,len(result_dict['text'])):\n        confidence = int(result_dict['conf'][i])\n        if confidence > 0:\n            text = result_dict['text'][i]\n            text = re.sub(\"[^0-9^]\", \"\",text)\n            if len(text)>0:\n                angle = int(text)\n                \n    return(angle, confidence)            \nLet’s now extract a single frame:\ncap = cv2.VideoCapture('../HighRes.mp4')\nimg = load_frame(0,cap)\nplt.imshow(img[:,:,::-1])\nplt.show()\n\n\n\n_config.yml\n\n\nAnd after preprocessing:\nimg = preprocess_frame(img)\nplt.imshow(img,cmap='gray')\nplt.show()\n\n\n\n_config.yml"
  },
  {
    "objectID": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#processing",
    "href": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#processing",
    "title": "Number extraction with Tesseract",
    "section": "Processing",
    "text": "Processing\nNow that we have pre-processed our images, it’s time to use Tesseract to digitise the text.\nWe have the opportunity to configure Tesseract; you can read more about the options available here.\ntesseract_config = r'--oem 3 --psm 13'\nresult_dict = pytesseract.image_to_data(img, config = tesseract_config, output_type = pytesseract.Output.DICT)\nLet’s now process the first 5,000 frames from the video:\nangles = []\nconfidences = []\nfor i in range(0,5_000):\n    img = load_frame(i,cap)\n    img = preprocess_frame(img)\n\n    tesseract_config = r'--oem 3 --psm 13'\n    result_dict = pytesseract.image_to_data(img, config = tesseract_config, output_type = pytesseract.Output.DICT)\n    \n    angle, confidence = extract_angle_confidence(result_dict)\n    \n    angles.append(angle)\n    confidences.append(confidence)\nWe can save both the angles and Tesseract’s level of confidence.\nangles = np.array(angles)\nconfidences = np.array(confidences)\n\nnp.save('angles.npy',angles)\nnp.save('confidences.npy',confidences)\nResults Analysis\n\nFinally, let’s do a quick analysis of the data extracted.\nBy examining the histogram, we find that Tesseract was often uncertain about its results.\nax = sns.histplot(confidences,bins = np.arange(0,100,10))\nax.set_xlabel('Confidence (%)')\n\n\n\n_config.yml\n\n\nWe can also chart the angles on a frame by frame basis. Note that the limits of the chart are set to the range of 0-360 (degrees).\nplt.plot(angles,alpha=0.5)\nplt.ylim(0,360)\nplt.xlabel('Sample Number')\nplt.ylabel('Angle (Degrees)')\nplt.show()\n\n\n\n_config.yml\n\n\nUsing:\nnp.count_nonzero(~np.isnan(angles))\nWe find that Tesseract managed to extract numbers in 4,049 out of 5,000 frames."
  },
  {
    "objectID": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#conclusion",
    "href": "posts/2021-01-03-Number-Extraction-With-Tesseract.html#conclusion",
    "title": "Number extraction with Tesseract",
    "section": "Conclusion",
    "text": "Conclusion\nOur next step is to take this heading data and integrate it with other data sources to form a more coherent view of the player’s position and heading. We can already see that our method will need to be robust to both missing and erroneous data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adventures with Numbers",
    "section": "",
    "text": "Blender\n\n\nBlog\n\n\n\n\nWhy do I blog?\n\n\n\n\n\n\nMay 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPyMC3\n\n\n\n\nCan we use bayesian analysis to compute the relative effectiveness of Hotel Quarantine programmes?\n\n\n\n\n\n\nMay 2, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPyMC3\n\n\n\n\nCan we use bayesian analysis to compute the relative effectiveness of Hotel Quarantine programmes?\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAR\n\n\nArUco\n\n\nComputer Vision\n\n\n\n\nHow can we find the geometric relationship between different ArUco tags?\n\n\n\n\n\n\nMar 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBlender\n\n\nComputer Vision\n\n\nPyTorch\n\n\n\n\nCan we train an object detection model on synthetically generated data?\n\n\n\n\n\n\nMar 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAR\n\n\nArUco\n\n\nComputer Vision\n\n\n\n\nWhat if we want more precision in our positioning from an ArUco tag?\n\n\n\n\n\n\nFeb 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAR\n\n\nArUco\n\n\nComputer Vision\n\n\n\n\nBuilding on the last post, let’s learn how to use ArUco tags to find a camera’s position and orientation.\n\n\n\n\n\n\nFeb 24, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAR\n\n\nComputer Vision\n\n\nBlender\n\n\nVisualisation\n\n\n\n\nLet’s look at the mathematics behind augmented reality.\n\n\n\n\n\n\nFeb 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPyMC3\n\n\n\n\nCan we use bayesian analysis to compute the relative effectiveness of Hotel Quarantine programmes?\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nTesseract\n\n\nNuketown84\n\n\n\n\nWe use Tesseract to capture numbers from an image.\n\n\n\n\n\n\nJan 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nNuketown84\n\n\n\n\nHow to use the Log Polar Transform to find the relative rotation between two images.\n\n\n\n\n\n\nDec 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nNuketown84\n\n\n\n\nHow to use the Log Polar Transform to find the relative rotation between two images.\n\n\n\n\n\n\nDec 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nNuketown84\n\n\n\n\nHow to use FFT Phase Correlation to find the relative translation between two images.\n\n\n\n\n\n\nDec 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nNuketown84\n\n\n\n\nAn overview of a new family of posts.\n\n\n\n\n\n\nDec 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nBlender\n\n\n\n\nConvert .exr outputs from Blender into a format useful for training computer vision models.\n\n\n\n\n\n\nOct 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nBlender\n\n\n\n\nLet’s learn how we can create depth and semantic maps for training machine learning models.\n\n\n\n\n\n\nOct 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nBlender\n\n\n\n\nI want to dig deeper into materials and import meshes in this post.\n\n\n\n\n\n\nOct 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nBlender\n\n\n\n\nLet’s learn how we can create synthetic imagery for training machine learning models.\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSympy\n\n\nFinance\n\n\nAfterpay\n\n\n\n\nHow do delays in recognising defaults impact the apparent profitability of Afterpay?\n\n\n\n\n\n\nOct 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nI want to take a step back and discuss what we have learned so far and what I think the next steps could be.\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nLet’s take a high-level view of how often Afterpay customers pay late, based on Afterpay’s FY2019 Annual report.\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nAfterpay has just released its 2020 annual report, and it includes two new and exciting pieces of information.\n\n\n\n\n\n\nAug 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nLet’s take a high-level view of how often Afterpay customers pay late, based on Afterpay’s FY2019 Annual report.\n\n\n\n\n\n\nAug 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nLet’s take a high-level view of how often Afterpay customers pay late, based on Afterpay’s FY2019 Annual report.\n\n\n\n\n\n\nAug 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nLet’s take a high-level view of how often Afterpay customers pay late, based on Afterpay’s FY2019 Annual report.\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nFinance\n\n\n\n\nLet’s step back and analyse Afterpay as a system and consider how it makes money.\n\n\n\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nFinance\n\n\nMonte Carlo Simulation\n\n\nAfterpay\n\n\n\n\nHow can we model the growth in Afterpay’s customer base?\n\n\n\n\n\n\nAug 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nComputer Vision\n\n\nPyMC3\n\n\nOptimisation\n\n\nLinear Algebra\n\n\n\n\nLet’s look and how we can use PyMC3 to help us improve our aim.\n\n\n\n\n\n\nJul 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nLinear Algebra\n\n\nMonte Carlo Simulation\n\n\n\n\nLet’s apply some of the techniques from previous points to a real-world problem.\n\n\n\n\n\n\nJul 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\nComputer Vision\n\n\n\n\nLet’s look and how we can use PyMC3 to help us improve our aim.\n\n\n\n\n\n\nJul 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDatashader\n\n\nVisualisation\n\n\nPUBG\n\n\n\n\nLet’s use Datashader to understand some of the gameplay mechanics of a hit video game while also making some abstract art.\n\n\n\n\n\n\nMay 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC3\n\n\n\n\nA Bayesian approach using PyMC3\n\n\n\n\n\n\nMay 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\n\n\nA Bayesian approach.\n\n\n\n\n\n\nApr 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nLinear Algebra\n\n\n\n\nLet’s use our vanishing points to estimate a camera rotation matrix (R).\n\n\n\n\n\n\nApr 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nLinear Algebra\n\n\n\n\nLet’s use our vanishing points to estimate a camera calibration matrix (K)\n\n\n\n\n\n\nApr 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nLinear Algebra\n\n\nMonte Carlo Simulation\n\n\n\n\nLet’s look at how to find a vanishing point mathematically.\n\n\n\n\n\n\nApr 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nOptimisation\n\n\nComputer Vision\n\n\n\n\nHow to find the inverse of a radial distortion function.\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nLinear Algebra\n\n\n\n\nHow to use RQ Decomposition to recover your camera’s K, R and C matrices.\n\n\n\n\n\n\nMar 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nComputer Vision\n\n\nOptimisation\n\n\nLinear Algebra\n\n\n\n\nLet’s learn how to use a set of known 2D and 3D point correspondences to calibrate a camera\n\n\n\n\n\n\nFeb 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nThree Interesting Papers\n\n\n\n\n\n\nJun 5, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\n\n\nKalman Filters\n\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\n\n\nVisualising Digital Elevation Maps\n\n\n\n\n\n\nNov 18, 2018\n\n\n\n\n\n\nNo matching items"
  }
]